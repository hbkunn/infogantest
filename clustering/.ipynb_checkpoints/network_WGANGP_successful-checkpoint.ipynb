{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        #main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        #nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            #nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            #nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G (\n",
      "  (main): Sequential (\n",
      "    (initial.100-256.convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "DCGAN_D (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "    (final.256-1.conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchSize = 64\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates).view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean().view(1)* lamda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][100/938][20] Loss_D: 120.555008 Loss_G: 16.957323 Loss_D_real: 137.171539 Loss_D_fake 16.616535\n",
      "[0/10000][200/938][40] Loss_D: 92.974213 Loss_G: 18.324451 Loss_D_real: 109.842697 Loss_D_fake 16.868484\n",
      "[0/10000][300/938][60] Loss_D: 95.627014 Loss_G: -15.291439 Loss_D_real: 83.460770 Loss_D_fake -12.166241\n",
      "[0/10000][400/938][80] Loss_D: 89.099083 Loss_G: -12.100209 Loss_D_real: 80.900665 Loss_D_fake -8.198415\n",
      "[0/10000][500/938][100] Loss_D: 80.669350 Loss_G: 3.513722 Loss_D_real: 83.960426 Loss_D_fake 3.291076\n",
      "[0/10000][600/938][120] Loss_D: 69.577332 Loss_G: 9.281697 Loss_D_real: 80.516281 Loss_D_fake 10.938950\n",
      "[0/10000][700/938][140] Loss_D: 56.077477 Loss_G: 19.412567 Loss_D_real: 75.619743 Loss_D_fake 19.542269\n",
      "[0/10000][800/938][160] Loss_D: 48.263042 Loss_G: 14.705771 Loss_D_real: 65.616600 Loss_D_fake 17.353558\n",
      "[0/10000][900/938][180] Loss_D: 48.431168 Loss_G: 14.901585 Loss_D_real: 63.900665 Loss_D_fake 15.469500\n",
      "[1/10000][60/938][200] Loss_D: 43.496532 Loss_G: 9.875791 Loss_D_real: 52.885208 Loss_D_fake 9.388675\n",
      "[1/10000][160/938][220] Loss_D: 41.580704 Loss_G: 9.134976 Loss_D_real: 50.615608 Loss_D_fake 9.034904\n",
      "[1/10000][260/938][240] Loss_D: 41.730797 Loss_G: 3.872589 Loss_D_real: 46.827667 Loss_D_fake 5.096870\n",
      "[1/10000][360/938][260] Loss_D: 37.937584 Loss_G: 6.776629 Loss_D_real: 46.070198 Loss_D_fake 8.132615\n",
      "[1/10000][460/938][280] Loss_D: 32.613117 Loss_G: 6.786242 Loss_D_real: 39.649483 Loss_D_fake 7.036366\n",
      "[1/10000][560/938][300] Loss_D: 27.892601 Loss_G: 11.443962 Loss_D_real: 38.448616 Loss_D_fake 10.556016\n",
      "[1/10000][660/938][320] Loss_D: 25.503189 Loss_G: 7.523812 Loss_D_real: 31.653170 Loss_D_fake 6.149981\n",
      "[1/10000][760/938][340] Loss_D: 21.353876 Loss_G: 1.870005 Loss_D_real: 24.271044 Loss_D_fake 2.917167\n",
      "[1/10000][860/938][360] Loss_D: 20.712721 Loss_G: 1.144663 Loss_D_real: 23.088451 Loss_D_fake 2.375731\n",
      "[2/10000][20/938][380] Loss_D: 18.454105 Loss_G: 0.352878 Loss_D_real: 20.389246 Loss_D_fake 1.935141\n",
      "[2/10000][120/938][400] Loss_D: 19.772451 Loss_G: -6.500606 Loss_D_real: 15.588432 Loss_D_fake -4.184019\n",
      "[2/10000][220/938][420] Loss_D: 20.864603 Loss_G: -5.980235 Loss_D_real: 15.295458 Loss_D_fake -5.569145\n",
      "[2/10000][320/938][440] Loss_D: 18.793709 Loss_G: -3.368786 Loss_D_real: 14.559097 Loss_D_fake -4.234612\n",
      "[2/10000][420/938][460] Loss_D: 17.419064 Loss_G: -2.340187 Loss_D_real: 15.604626 Loss_D_fake -1.814438\n",
      "[2/10000][520/938][480] Loss_D: 17.735624 Loss_G: -5.059662 Loss_D_real: 13.944354 Loss_D_fake -3.791270\n",
      "[2/10000][620/938][500] Loss_D: 15.893886 Loss_G: -3.626362 Loss_D_real: 12.741085 Loss_D_fake -3.152800\n",
      "[2/10000][720/938][520] Loss_D: 17.313181 Loss_G: -1.950556 Loss_D_real: 14.887851 Loss_D_fake -2.425331\n",
      "[2/10000][820/938][540] Loss_D: 15.673289 Loss_G: 2.094245 Loss_D_real: 16.824980 Loss_D_fake 1.151690\n",
      "[2/10000][920/938][560] Loss_D: 15.150770 Loss_G: 1.701416 Loss_D_real: 16.176472 Loss_D_fake 1.025702\n",
      "[3/10000][80/938][580] Loss_D: 14.023948 Loss_G: -1.179191 Loss_D_real: 15.020356 Loss_D_fake 0.996409\n",
      "[3/10000][180/938][600] Loss_D: 15.370772 Loss_G: -2.930364 Loss_D_real: 12.382906 Loss_D_fake -2.987866\n",
      "[3/10000][280/938][620] Loss_D: 14.460201 Loss_G: -1.162929 Loss_D_real: 12.870078 Loss_D_fake -1.590123\n",
      "[3/10000][380/938][640] Loss_D: 13.071047 Loss_G: 1.240070 Loss_D_real: 14.670239 Loss_D_fake 1.599192\n",
      "[3/10000][480/938][660] Loss_D: 14.042009 Loss_G: -1.683483 Loss_D_real: 12.815410 Loss_D_fake -1.226600\n",
      "[3/10000][580/938][680] Loss_D: 15.924603 Loss_G: -5.166853 Loss_D_real: 11.044543 Loss_D_fake -4.880059\n",
      "[3/10000][680/938][700] Loss_D: 13.194341 Loss_G: -1.472190 Loss_D_real: 11.956927 Loss_D_fake -1.237414\n",
      "[3/10000][780/938][720] Loss_D: 12.425574 Loss_G: 0.002836 Loss_D_real: 12.529880 Loss_D_fake 0.104305\n",
      "[3/10000][880/938][740] Loss_D: 13.966992 Loss_G: 1.694077 Loss_D_real: 15.452897 Loss_D_fake 1.485905\n",
      "[4/10000][40/938][760] Loss_D: 12.886589 Loss_G: 0.693373 Loss_D_real: 11.806554 Loss_D_fake -1.080035\n",
      "[4/10000][140/938][780] Loss_D: 13.551030 Loss_G: -2.983825 Loss_D_real: 11.443628 Loss_D_fake -2.107402\n"
     ]
    }
   ],
   "source": [
    "gen_iterations = 0\n",
    "critic_iters = 5\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(dataloader):\n",
    "                continue\n",
    "            real_cpu, _ = data_iter.next()\n",
    "            i +=1\n",
    "            batch_size = real_cpu.size(0)\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            \n",
    "            noise.resize_(batch_size, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True)\n",
    "            fake = netG(noisev).data\n",
    "            fake_data_v = Variable(fake)\n",
    "            \n",
    "            gradient_penalty = calc_gradient_penalty(netD, input, fake,lamda,batch_size)\n",
    "            real_data_v = Variable(input)\n",
    "            errD_real = netD(real_data_v).mean().view(1)\n",
    "            \n",
    "            inputv = Variable(netG(noisev).data)\n",
    "            errD_fake = netD(fake).mean().view(1)\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            D_cost.backward()\n",
    "            netD.zero_grad()\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD(real_data_v).mean().view(1)\n",
    "            errD_real.backward(mone)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(batch_size, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(fake).mean().view(1)\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data,lamda,batch_size)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            errD = errD_real - errD_fake \n",
    "            optimizerD.step()\n",
    "            \n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake).mean().view(1)\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'WGANGP.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'WGANGP_fake_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1,1],[32,32]],dtype='float32'))\n",
    "print(a.size())\n",
    "a.norm(2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputv.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'netG_try_kmeans_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD.state_dict(), 'netD_try_kmeans_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D_feature(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D_feature, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        #main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        #nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        #output = output.mean(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 1\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_train = X_train[0:1000,:,:,:]\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train_ = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train_,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=batchSize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_G = torch.load('netG_try_kmeans_epoch_%d.pth' % (1910))\n",
    "dict_D = torch.load('netD_try_kmeans_epoch_%d.pth' % (1910))\n",
    "del(dict_D['main.final.256-1.conv.weight'])\n",
    "\n",
    "feature_dict = {}\n",
    "netD_feature = DCGAN_D_feature()\n",
    "netD_feature.load_state_dict(dict_D)\n",
    "print(netD_feature)\n",
    "data_iter = iter(dataloader)\n",
    "for i,data in enumerate(data_iter):\n",
    "    feature_dict[i] = netD_feature(Variable(data[0])).data.numpy().reshape((1,256*4*4))\n",
    "    \n",
    "feature = np.zeros((max(feature_dict.keys()),4096),dtype=np.float32)\n",
    "for i,data in enumerate(feature):\n",
    "    data[:] = feature_dict[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "for i in range(0,10):\n",
    "    cluster = np.take(X_train, arg[i],axis=0).reshape(-1,3,32,32)\n",
    "    vutils.save_image(torch.from_numpy(cluster), 'cluster%d_%d.png'% (10,i),normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.take(X_train, [0],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "pic_list = []\n",
    "for i,index in enumerate(arg):\n",
    "    X_train[n,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
