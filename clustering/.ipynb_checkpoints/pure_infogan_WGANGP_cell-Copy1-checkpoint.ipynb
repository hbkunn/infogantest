{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6816, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-7:\n",
      "Process PoolWorker-2:\n",
      "Process PoolWorker-3:\n",
      "Process PoolWorker-6:\n",
      "Process PoolWorker-8:\n",
      "Process PoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-4:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    racquire()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    racquire()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    racquire()\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "KeyboardInterrupt\n",
      "    return recv()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "pool = ThreadPool(8) \n",
    "root_dir = '/disk1/cell_segment_save/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "#result = np.asarray(result,dtype=np.uint8)\n",
    "print(result.shape)\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.138-256.convt): ConvTranspose2d(138, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*10)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*10)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=False)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize):\n",
    "    rand_c = np.zeros((batchsize,10),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "        rand_c[i] = rand\n",
    "    \n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "def generate_fix_noise(dis=1, cont=4, rand=128):\n",
    "    \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "    map1 = np.concatenate((changing_dis,fixed_cont,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    single_cont = np.asarray([float(n-5)*2/5 for n in range(0,10)]*10,dtype = np.float32)\n",
    "    \n",
    "    fixed_dis = np.zeros((100,10),dtype=np.float32)\n",
    "    for t in range(0,5):\n",
    "        fixed_dis[t*20:t*20+20,t*2] = 1\n",
    "        \n",
    "    for t in range (0,4):\n",
    "        fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "        fixed_cont[:,t] = single_cont\n",
    "        map2 = np.concatenate((fixed_dis,fixed_cont,fixed_z),axis=1)\n",
    "        lst.append(map2.astype(np.float32))\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def fix_noise(dis=1, rand=128):\n",
    "        \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(100,138,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 37.6794471741 0.0654168128967\n",
      "1 40 34.584526062 0.575553894043\n",
      "2 60 32.6868934631 0.0304403305054\n",
      "3 80 32.8266181946 2.42160105705\n",
      "4 100 27.6151008606 0.0898790359497\n",
      "5 120 23.9720630646 1.09052562714\n",
      "6 140 23.9382400513 0.0778098106384\n",
      "7 160 21.483089447 0.44849395752\n",
      "8 180 18.537322998 5.91521835327\n",
      "9 200 18.6044521332 0.649379253387\n",
      "9 220 16.6380500793 3.06485104561\n",
      "10 240 17.3813095093 1.71171545982\n",
      "11 260 14.6354436874 0.00266695022583\n",
      "12 280 12.7983989716 0.373698711395\n",
      "13 300 13.3307790756 0.0189757347107\n",
      "14 320 12.1869411469 0.0320811271667\n",
      "15 340 12.0715360641 0.0220408439636\n",
      "16 360 11.6612186432 0.00663280487061\n",
      "17 380 11.0242557526 0.0759580135345\n",
      "18 400 9.93751144409 0.279325008392\n",
      "19 420 10.0870656967 0.01331615448\n",
      "19 440 9.87727451324 0.00810408592224\n",
      "20 460 9.8337392807 0.0507974624634\n",
      "21 480 9.50360774994 0.00444468855858\n",
      "22 500 9.09203243256 0.0187691152096\n",
      "23 520 8.40889453888 0.0263660997152\n",
      "24 540 8.58697509766 0.0106039457023\n",
      "25 560 7.75530338287 0.000283241271973\n",
      "26 580 7.42865133286 0.0100245149806\n",
      "27 600 8.37591457367 0.0293790102005\n",
      "28 620 7.8175163269 0.0293057858944\n",
      "29 640 6.74620866776 0.450800418854\n",
      "29 660 7.18001413345 0.000368118286133\n",
      "30 680 6.80920696259 0.119694858789\n",
      "31 700 6.87810993195 0.00217378139496\n",
      "32 720 6.96724510193 0.0025771856308\n",
      "33 740 6.74875688553 0.000967741012573\n",
      "34 760 6.46911382675 0.898541927338\n",
      "35 780 5.97911071777 0.39981096983\n",
      "36 800 6.19009113312 0.00285017490387\n",
      "37 820 5.96372175217 1.07540118694\n",
      "38 840 5.88408041 0.125447630882\n",
      "39 860 6.03115129471 0.453798294067\n",
      "39 880 5.02060365677 0.0128653049469\n",
      "40 900 5.45696258545 0.000414490699768\n",
      "41 920 5.32354640961 0.138444244862\n",
      "42 940 5.16084289551 0.00840377807617\n",
      "43 960 4.86906576157 2.43661141396\n",
      "44 980 5.35820913315 0.184813141823\n",
      "45 1000 5.30943536758 0.000627636909485\n",
      "46 1020 5.0203166008 0.00394141674042\n",
      "47 1040 5.14975309372 0.00609790533781\n",
      "48 1060 4.23985862732 0.0566338300705\n",
      "49 1080 4.64534521103 0.00631731748581\n",
      "49 1100 4.52428340912 0.00273382291198\n",
      "50 1120 4.6151304245 0.0107606649399\n",
      "51 1140 4.50115728378 9.6321105957e-05\n",
      "52 1160 4.48821783066 9.56058502197e-05\n",
      "53 1180 3.89380598068 0.675871253014\n",
      "54 1200 4.07219982147 0.0106244683266\n",
      "55 1220 3.89980053902 0.763892054558\n",
      "56 1240 4.03052663803 0.00168371200562\n",
      "57 1260 3.83871746063 0.00324440002441\n",
      "58 1280 3.610496521 0.0306574106216\n",
      "59 1300 3.77874279022 0.0518494844437\n",
      "59 1320 3.71820425987 0.309353113174\n",
      "60 1340 4.04853439331 2.8169798851\n",
      "61 1360 3.38709545135 0.43154668808\n",
      "62 1380 3.03445100784 0.00862538814545\n",
      "63 1400 3.47849845886 0.00137776136398\n",
      "64 1420 3.57308745384 0.427435040474\n",
      "65 1440 3.0544526577 1.77783989906\n",
      "66 1460 2.96333885193 0.401549816132\n",
      "67 1480 3.50850963593 0.00202333927155\n",
      "68 1500 3.07129049301 0.000504970550537\n",
      "69 1520 2.89627742767 0.00010299263522\n",
      "69 1540 2.98917484283 0.195572495461\n",
      "70 1560 3.0756752491 0.000441551208496\n",
      "71 1580 2.89745259285 0.000395059585571\n",
      "72 1600 2.74904370308 2.9444694519e-05\n",
      "73 1620 3.02904629707 0.0774954557419\n",
      "74 1640 2.93039178848 0.413838624954\n",
      "75 1660 3.07532191277 0.0932763516903\n",
      "76 1680 2.6415014267 0.00752079486847\n",
      "77 1700 2.79104542732 0.00291693210602\n",
      "78 1720 2.58580875397 0.096792191267\n",
      "79 1740 3.17058300972 0.462916374207\n",
      "79 1760 2.49562048912 0.00137037038803\n",
      "80 1780 2.67073345184 7.90566635132\n",
      "81 1800 2.52823257446 0.00353419780731\n",
      "82 1820 2.51517081261 0.0348256528378\n",
      "83 1840 2.57976484299 0.101743817329\n",
      "84 1860 2.57510876656 0.000738859176636\n",
      "85 1880 2.23408079147 0.000152707099915\n",
      "86 1900 2.40692949295 7.83590698242\n",
      "87 1920 2.45257329941 0.0512571334839\n",
      "88 1940 2.61941051483 0.083597779274\n",
      "89 1960 2.25123047829 0.000248432159424\n",
      "89 1980 2.41984367371 0.0162731409073\n",
      "90 2000 2.34845590591 0.00675968825817\n",
      "91 2020 2.28899407387 0.0193257331848\n",
      "92 2040 2.53885269165 0.000389099121094\n",
      "93 2060 2.43318414688 12.2043733597\n",
      "94 2080 2.5015463829 0.950297236443\n",
      "95 2100 2.19851756096 9.28276443481\n",
      "96 2120 2.24351787567 0.00760221481323\n",
      "97 2140 2.49542284012 4.68492507935e-05\n",
      "98 2160 2.05634427071 6.76088142395\n",
      "99 2180 2.66699075699 0.00123044848442\n",
      "99 2200 2.35872030258 1.90167784691\n",
      "100 2220 2.53062772751 0.00925588607788\n",
      "101 2240 2.32319259644 0.160403370857\n",
      "102 2260 2.66826629639 0.00391911715269\n",
      "103 2280 2.08044695854 0.000218868255615\n",
      "104 2300 2.05226159096 0.0515501499176\n",
      "105 2320 2.40043354034 0.000874280929565\n",
      "106 2340 2.52555918694 0.586347341537\n",
      "107 2360 2.39341711998 7.90357589722e-05\n",
      "108 2380 2.29231452942 7.39097595215e-06\n",
      "109 2400 2.14779734612 0.17214679718\n",
      "109 2420 2.23121929169 0.0204707384109\n",
      "110 2440 2.06234717369 0.000282645225525\n",
      "111 2460 1.96316730976 0.00694119930267\n",
      "112 2480 2.37432098389 0.0346443653107\n",
      "113 2500 1.95230865479 0.000523567199707\n",
      "114 2520 2.17895627022 5.33345270157\n",
      "115 2540 2.57945370674 0.0103746056557\n",
      "116 2560 2.19393801689 0.53816062212\n",
      "117 2580 1.99221396446 5.20291852951\n",
      "118 2600 2.23688936234 0.0459452867508\n",
      "119 2620 1.83351898193 0.000353395938873\n",
      "119 2640 2.17847061157 0.00641855597496\n",
      "120 2660 2.04200029373 0.0559642314911\n",
      "121 2680 2.28621792793 0.0390766859055\n",
      "122 2700 1.87896347046 0.000393867492676\n",
      "123 2720 1.78994774818 3.93390655518e-06\n",
      "124 2740 2.30685710907 0.00349187850952\n",
      "125 2760 1.69333255291 0.00385864078999\n",
      "126 2780 2.5050880909 0.00334048271179\n",
      "127 2800 2.03498959541 0.00716406106949\n",
      "128 2820 1.63012742996 7.11739444733\n",
      "129 2840 1.83609938622 4.07695770264e-05\n",
      "129 2860 1.91712296009 4.66835021973\n",
      "130 2880 1.80010187626 0.00929117202759\n",
      "131 2900 2.00266695023 6.27040863037e-05\n",
      "132 2920 1.77030873299 0.00566327571869\n",
      "133 2940 1.98468089104 0.511942863464\n",
      "134 2960 1.73489868641 0.0106147527695\n",
      "135 2980 2.13660740852 0.0810730457306\n",
      "136 3000 2.09033751488 0.000667214393616\n",
      "137 3020 1.83311760426 0.0224264860153\n",
      "138 3040 1.89848303795 0.277662396431\n",
      "139 3060 2.02895760536 0.00672340393066\n",
      "139 3080 1.84234535694 0.148380279541\n",
      "140 3100 1.91346418858 0.000517010688782\n",
      "141 3120 1.76070427895 0.000162601470947\n",
      "142 3140 2.05797171593 0.00192999839783\n",
      "143 3160 1.97773981094 0.0328960418701\n",
      "144 3180 1.60444366932 0.548608422279\n",
      "145 3200 1.92732310295 8.01086425781e-05\n",
      "146 3220 1.87922811508 0.00444841384888\n",
      "147 3240 1.58967649937 0.0231734514236\n",
      "148 3260 1.40996658802 12.0241174698\n",
      "149 3280 1.84372591972 4.05509710312\n",
      "149 3300 1.88158237934 0.00436105951667\n",
      "150 3320 1.8334813118 0.00010871887207\n",
      "151 3340 1.73057734966 0.00534003973007\n",
      "152 3360 1.64880871773 0.00143778324127\n",
      "153 3380 1.17906200886 2.95639038086e-05\n",
      "154 3400 1.287530303 0.0103315114975\n",
      "155 3420 1.94751966 0.905058383942\n",
      "156 3440 1.65644586086 9.17911529541e-06\n",
      "157 3460 1.40968418121 0.0417366027832\n",
      "158 3480 1.85183930397 10.1328058243\n",
      "159 3500 2.04731965065 0.000124573707581\n",
      "159 3520 1.75999164581 0.00365829467773\n",
      "160 3540 1.90622770786 5.96046447754e-07\n",
      "161 3560 1.62535834312 6.38961791992e-05\n",
      "162 3580 1.97080826759 0.726889610291\n",
      "163 3600 1.66600370407 0.165039658546\n",
      "164 3620 1.83426272869 0.378678798676\n",
      "165 3640 1.73954641819 0.000766754150391\n",
      "166 3660 1.7422003746 0.0492563247681\n",
      "167 3680 1.83653056622 0.271625995636\n",
      "168 3700 1.70637047291 2.45571136475e-05\n",
      "169 3720 1.84508526325 0.00254261493683\n",
      "169 3740 1.6022772789 0.00051736831665\n",
      "170 3760 1.90993356705 2.07583069801\n",
      "171 3780 1.20536541939 0.000142693519592\n",
      "172 3800 1.57599282265 0.000381588935852\n",
      "173 3820 1.58029043674 0.283236861229\n",
      "174 3840 1.90042889118 3.0517578125e-05\n",
      "175 3860 1.48149847984 1.14440917969e-05\n",
      "176 3880 1.59495139122 1.35898590088e-05\n",
      "177 3900 1.40290176868 0.000701904296875\n",
      "178 3920 1.5930352211 0.0382876396179\n",
      "179 3940 1.77808129787 0.267289996147\n",
      "179 3960 1.61423492432 4.31915187836\n",
      "180 3980 1.77484571934 5.65052032471e-05\n",
      "181 4000 1.47601270676 0.000539422035217\n",
      "182 4020 1.95015108585 0.00106501579285\n",
      "183 4040 1.65426838398 1.15880227089\n",
      "184 4060 1.54130780697 0.000143051147461\n",
      "185 4080 1.48068153858 0.0012332201004\n",
      "186 4100 1.73185288906 0.000236034393311\n",
      "187 4120 1.52394902706 4.47361183167\n",
      "188 4140 1.58691871166 0.00503873825073\n",
      "189 4160 1.58070099354 0.000240445137024\n",
      "189 4180 1.29776537418 0.00110054016113\n",
      "190 4200 1.44148206711 5.60391330719\n",
      "191 4220 1.28065502644 0.014918088913\n",
      "192 4240 1.61243462563 0.0\n",
      "193 4260 1.39334797859 1.33514404297e-05\n",
      "194 4280 1.65360069275 0.000423431396484\n",
      "195 4300 1.36135494709 10.2572393417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 4320 1.73476195335 0.0\n",
      "197 4340 1.58840560913 4.76837158203e-05\n",
      "198 4360 1.47295701504 10.9590826035\n",
      "199 4380 1.74318623543 0.0122556686401\n",
      "199 4400 1.41166067123 4.95910644531e-05\n",
      "200 4420 1.53502190113 0.000770568847656\n",
      "201 4440 1.57781934738 0.319969177246\n",
      "202 4460 1.09226834774 0.00759363174438\n",
      "203 4480 1.69297635555 0.0014271736145\n",
      "204 4500 1.63221919537 0.264080524445\n",
      "205 4520 1.31591713428 0.0007643699646\n",
      "206 4540 1.42089092731 3.31401824951e-05\n",
      "207 4560 1.38679838181 4.1127204895e-05\n",
      "208 4580 1.60056304932 0.315368175507\n",
      "209 4600 1.13224601746 0.00130617618561\n",
      "209 4620 1.28385567665 0.00256299972534\n",
      "210 4640 1.21616566181 2.86102294922e-06\n",
      "211 4660 1.41592013836 0.000436782836914\n",
      "212 4680 1.37905204296 0.00610941648483\n",
      "213 4700 1.28631019592 0.00479364395142\n",
      "214 4720 1.21356415749 1.52587890625e-05\n",
      "215 4740 1.32998561859 0.0363720655441\n",
      "216 4760 1.27686548233 0.0026570558548\n",
      "217 4780 1.52711701393 0.000923156738281\n",
      "218 4800 1.3530138731 0.00441384315491\n",
      "219 4820 1.1852850914 0.00545525550842\n",
      "219 4840 1.40603864193 4.57394218445\n",
      "220 4860 1.23964345455 4.76837158203e-06\n",
      "221 4880 1.48620474339 0.000160217285156\n",
      "222 4900 1.39123761654 0.0025155544281\n",
      "223 4920 1.04218447208 0.222784519196\n",
      "224 4940 1.25703144073 0.0296230316162\n",
      "225 4960 1.58430361748 2.62260437012e-06\n",
      "226 4980 1.2359303236 0.00062084197998\n",
      "227 5000 1.50459527969 8.34465026855e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+10*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, 10)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+10*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, 10)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_segmentate.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_samples_kmeans.png',nrow=10,normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
