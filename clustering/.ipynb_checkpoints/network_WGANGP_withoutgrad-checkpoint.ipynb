{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        return output\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        #main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        #nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            #nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            #nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G (\n",
      "  (main): Sequential (\n",
      "    (initial.100-256.convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "DCGAN_D (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "    (final.256-1.conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchSize = 64\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\\n    #print real_data.size()\\n    alpha = torch.rand(batch_size,1,1,1)\\n    #print (real_data.size())\\n    alpha = alpha.expand(real_data.size())\\n    alpha = alpha.cuda()\\n\\n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\\n    interpolates = interpolates.cuda()\\n    interpolates = Variable(interpolates, requires_grad=True)\\n\\n    disc_interpolates = netD(interpolates)\\n\\n    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\\n                              create_graph=True)\\n    \\n    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\\n                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\\n                              #create_graph=True, retain_graph=True, only_inputs=True)\\n    #gradients*gradients\\n    \\n    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean().view(1)* lamda\\n    return gradient_penalty\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    alpha_1 = torch.rand(batch_size,1,1,1).expand(real_data.size()).cuda()\n",
    "    alpha_2 = torch.rand(batch_size,1,1,1).expand(real_data.size()).cuda()\n",
    "    interpolates_1 = alpha_1 * real_data + ((1 - alpha_1) * fake_data)\n",
    "    interpolates_2 = alpha_2 * real_data + ((1 - alpha_2) * fake_data)\n",
    "    interpolates_1 = Variable(interpolates_1, requires_grad=True)\n",
    "    interpolates_2 = Variable(interpolates_2, requires_grad=True)\n",
    "    disc_interpolates_1 = netD(interpolates_1).view(batch_size,-1)\n",
    "    disc_interpolates_2 = netD(interpolates_2).view(batch_size,-1)\n",
    "    gradient_penalty = lamda*((disc_interpolates_2 - disc_interpolates_1).norm(1, dim=1)/(interpolates_1.view(batch_size,-1)-interpolates_2.view(batch_size,-1)).norm(2, dim=1)-1)**2\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean().view(1)* lamda\n",
    "    return gradient_penalty\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][100/938][20] Loss_D: 157.201614 Loss_G: 34.641701 Loss_D_real: 192.667374 Loss_D_fake 35.465759\n",
      "[0/10000][200/938][40] Loss_D: 114.777359 Loss_G: 43.424484 Loss_D_real: 160.440002 Loss_D_fake 45.662647\n",
      "[0/10000][300/938][60] Loss_D: 125.514709 Loss_G: 23.186188 Loss_D_real: 150.139557 Loss_D_fake 24.624851\n",
      "[0/10000][400/938][80] Loss_D: 109.868286 Loss_G: 43.990837 Loss_D_real: 147.647278 Loss_D_fake 37.778996\n",
      "[0/10000][500/938][100] Loss_D: 102.109276 Loss_G: 74.989090 Loss_D_real: 180.435303 Loss_D_fake 78.326027\n",
      "[0/10000][600/938][120] Loss_D: 105.582039 Loss_G: 95.860771 Loss_D_real: 194.214020 Loss_D_fake 88.631981\n",
      "[0/10000][700/938][140] Loss_D: 97.907967 Loss_G: 82.020920 Loss_D_real: 202.778503 Loss_D_fake 104.870537\n",
      "[0/10000][800/938][160] Loss_D: 104.850861 Loss_G: 127.611526 Loss_D_real: 237.260468 Loss_D_fake 132.409607\n",
      "[0/10000][900/938][180] Loss_D: 101.088303 Loss_G: 121.259399 Loss_D_real: 232.335648 Loss_D_fake 131.247345\n",
      "[1/10000][60/938][200] Loss_D: 106.989471 Loss_G: 152.185120 Loss_D_real: 245.725922 Loss_D_fake 138.736450\n",
      "[1/10000][160/938][220] Loss_D: 121.535652 Loss_G: 139.978363 Loss_D_real: 228.404709 Loss_D_fake 106.869057\n",
      "[1/10000][260/938][240] Loss_D: 107.615685 Loss_G: 108.525887 Loss_D_real: 235.377213 Loss_D_fake 127.761528\n",
      "[1/10000][360/938][260] Loss_D: 102.668015 Loss_G: 130.713516 Loss_D_real: 233.464722 Loss_D_fake 130.796707\n",
      "[1/10000][460/938][280] Loss_D: 103.432983 Loss_G: 114.254738 Loss_D_real: 229.871506 Loss_D_fake 126.438522\n",
      "[1/10000][560/938][300] Loss_D: 107.061066 Loss_G: 125.182953 Loss_D_real: 245.684158 Loss_D_fake 138.623093\n",
      "[1/10000][660/938][320] Loss_D: 99.636551 Loss_G: 147.824173 Loss_D_real: 248.975342 Loss_D_fake 149.338791\n",
      "[1/10000][760/938][340] Loss_D: 121.651161 Loss_G: 120.032829 Loss_D_real: 217.950485 Loss_D_fake 96.299324\n",
      "[1/10000][860/938][360] Loss_D: 107.157959 Loss_G: 96.399536 Loss_D_real: 238.553818 Loss_D_fake 131.395859\n",
      "[2/10000][20/938][380] Loss_D: 114.963295 Loss_G: 119.126602 Loss_D_real: 228.137573 Loss_D_fake 113.174278\n",
      "[2/10000][120/938][400] Loss_D: 117.271286 Loss_G: 113.577972 Loss_D_real: 211.152130 Loss_D_fake 93.880844\n",
      "[2/10000][220/938][420] Loss_D: 107.808365 Loss_G: 102.652885 Loss_D_real: 224.416763 Loss_D_fake 116.608398\n",
      "[2/10000][320/938][440] Loss_D: 118.258842 Loss_G: 88.883377 Loss_D_real: 197.738831 Loss_D_fake 79.479988\n",
      "[2/10000][420/938][460] Loss_D: 117.891838 Loss_G: 88.536385 Loss_D_real: 188.835693 Loss_D_fake 70.943855\n",
      "[2/10000][520/938][480] Loss_D: 97.944275 Loss_G: 81.975853 Loss_D_real: 239.289551 Loss_D_fake 141.345276\n",
      "[2/10000][620/938][500] Loss_D: 106.950859 Loss_G: 53.969212 Loss_D_real: 180.985931 Loss_D_fake 74.035072\n",
      "[2/10000][720/938][520] Loss_D: 95.266113 Loss_G: 57.937103 Loss_D_real: 225.639420 Loss_D_fake 130.373306\n",
      "[2/10000][820/938][540] Loss_D: 102.974663 Loss_G: 73.255531 Loss_D_real: 194.999496 Loss_D_fake 92.024834\n",
      "[2/10000][920/938][560] Loss_D: 105.878395 Loss_G: 43.514271 Loss_D_real: 191.798737 Loss_D_fake 85.920341\n",
      "[3/10000][80/938][580] Loss_D: 103.378151 Loss_G: 52.821663 Loss_D_real: 176.466782 Loss_D_fake 73.088631\n",
      "[3/10000][180/938][600] Loss_D: 127.648117 Loss_G: 87.468010 Loss_D_real: 157.641510 Loss_D_fake 29.993395\n",
      "[3/10000][280/938][620] Loss_D: 98.936142 Loss_G: 31.479549 Loss_D_real: 191.173859 Loss_D_fake 92.237717\n",
      "[3/10000][380/938][640] Loss_D: 107.475662 Loss_G: 14.290529 Loss_D_real: 159.307449 Loss_D_fake 51.831791\n",
      "[3/10000][480/938][660] Loss_D: 116.416550 Loss_G: 34.506523 Loss_D_real: 140.913193 Loss_D_fake 24.496639\n",
      "[3/10000][580/938][680] Loss_D: 88.887169 Loss_G: 5.899571 Loss_D_real: 181.337112 Loss_D_fake 92.449944\n",
      "[3/10000][680/938][700] Loss_D: 120.263336 Loss_G: 26.020443 Loss_D_real: 125.388268 Loss_D_fake 5.124929\n",
      "[3/10000][780/938][720] Loss_D: 121.239105 Loss_G: 15.984618 Loss_D_real: 120.482033 Loss_D_fake -0.757074\n",
      "[3/10000][880/938][740] Loss_D: 113.807236 Loss_G: 28.765152 Loss_D_real: 156.104874 Loss_D_fake 42.297642\n",
      "[4/10000][40/938][760] Loss_D: 117.800011 Loss_G: 6.023198 Loss_D_real: 125.079399 Loss_D_fake 7.279389\n",
      "[4/10000][140/938][780] Loss_D: 105.716064 Loss_G: 24.469786 Loss_D_real: 122.229424 Loss_D_fake 16.513359\n",
      "[4/10000][240/938][800] Loss_D: 110.274979 Loss_G: -27.014774 Loss_D_real: 139.889496 Loss_D_fake 29.614517\n",
      "[4/10000][340/938][820] Loss_D: 119.570137 Loss_G: 0.618235 Loss_D_real: 104.762581 Loss_D_fake -14.807555\n",
      "[4/10000][440/938][840] Loss_D: 110.636475 Loss_G: -1.366649 Loss_D_real: 123.226196 Loss_D_fake 12.589723\n",
      "[4/10000][540/938][860] Loss_D: 112.976570 Loss_G: -20.763876 Loss_D_real: 94.895699 Loss_D_fake -18.080872\n",
      "[4/10000][640/938][880] Loss_D: 96.308281 Loss_G: -50.245323 Loss_D_real: 114.700150 Loss_D_fake 18.391870\n",
      "[4/10000][740/938][900] Loss_D: 120.197113 Loss_G: -30.503422 Loss_D_real: 96.859238 Loss_D_fake -23.337872\n",
      "[4/10000][840/938][920] Loss_D: 118.308296 Loss_G: -56.895744 Loss_D_real: 91.857719 Loss_D_fake -26.450579\n",
      "[4/10000][938/938][940] Loss_D: 117.601517 Loss_G: -60.765900 Loss_D_real: 64.439049 Loss_D_fake -53.162468\n",
      "[5/10000][100/938][960] Loss_D: 111.187157 Loss_G: -26.836409 Loss_D_real: 87.598167 Loss_D_fake -23.588991\n",
      "[5/10000][200/938][980] Loss_D: 104.168533 Loss_G: -50.623501 Loss_D_real: 79.873672 Loss_D_fake -24.294859\n",
      "[5/10000][300/938][1000] Loss_D: 114.809235 Loss_G: -27.842775 Loss_D_real: 50.191448 Loss_D_fake -64.617790\n",
      "[5/10000][400/938][1020] Loss_D: 106.638130 Loss_G: -61.146515 Loss_D_real: 82.792793 Loss_D_fake -23.845335\n",
      "[5/10000][500/938][1040] Loss_D: 118.808014 Loss_G: -29.669127 Loss_D_real: 62.562309 Loss_D_fake -56.245701\n",
      "[5/10000][600/938][1060] Loss_D: 95.018417 Loss_G: -77.436516 Loss_D_real: 74.597603 Loss_D_fake -20.420815\n",
      "[5/10000][700/938][1080] Loss_D: 118.005928 Loss_G: -77.056885 Loss_D_real: 36.815735 Loss_D_fake -81.190193\n",
      "[5/10000][800/938][1100] Loss_D: 120.992531 Loss_G: -88.809059 Loss_D_real: 6.173187 Loss_D_fake -114.819344\n",
      "[5/10000][900/938][1120] Loss_D: 117.628548 Loss_G: -58.954376 Loss_D_real: 20.399567 Loss_D_fake -97.228981\n",
      "[6/10000][60/938][1140] Loss_D: 116.901123 Loss_G: -49.495766 Loss_D_real: 25.023769 Loss_D_fake -91.877357\n",
      "[6/10000][160/938][1160] Loss_D: 111.655205 Loss_G: -110.714790 Loss_D_real: 19.816515 Loss_D_fake -91.838692\n",
      "[6/10000][260/938][1180] Loss_D: 121.340698 Loss_G: -125.595406 Loss_D_real: 17.127569 Loss_D_fake -104.213127\n",
      "[6/10000][360/938][1200] Loss_D: 107.870735 Loss_G: -93.192307 Loss_D_real: 2.557510 Loss_D_fake -105.313225\n",
      "[6/10000][460/938][1220] Loss_D: 96.040123 Loss_G: -133.757141 Loss_D_real: 2.573985 Loss_D_fake -93.466141\n",
      "[6/10000][560/938][1240] Loss_D: 133.917191 Loss_G: -133.640564 Loss_D_real: -4.823478 Loss_D_fake -138.740662\n",
      "[6/10000][660/938][1260] Loss_D: 96.515953 Loss_G: -97.253181 Loss_D_real: 26.510344 Loss_D_fake -70.005608\n",
      "[6/10000][760/938][1280] Loss_D: 107.508377 Loss_G: -128.602524 Loss_D_real: 21.807293 Loss_D_fake -85.701088\n",
      "[6/10000][860/938][1300] Loss_D: 118.012947 Loss_G: -126.186325 Loss_D_real: -10.035411 Loss_D_fake -128.048355\n",
      "[7/10000][20/938][1320] Loss_D: 108.994873 Loss_G: -106.038391 Loss_D_real: -2.891830 Loss_D_fake -111.886703\n",
      "[7/10000][120/938][1340] Loss_D: 91.022583 Loss_G: -95.710861 Loss_D_real: 16.088154 Loss_D_fake -74.934425\n",
      "[7/10000][220/938][1360] Loss_D: 121.176315 Loss_G: -90.855980 Loss_D_real: -4.919326 Loss_D_fake -126.095642\n",
      "[7/10000][320/938][1380] Loss_D: 123.266373 Loss_G: -108.932304 Loss_D_real: -10.554411 Loss_D_fake -133.820786\n",
      "[7/10000][420/938][1400] Loss_D: 126.860260 Loss_G: -154.424271 Loss_D_real: -11.187087 Loss_D_fake -138.047348\n",
      "[7/10000][520/938][1420] Loss_D: 110.783653 Loss_G: -137.442108 Loss_D_real: 14.568261 Loss_D_fake -96.215393\n",
      "[7/10000][620/938][1440] Loss_D: 133.668640 Loss_G: -127.776863 Loss_D_real: -20.299776 Loss_D_fake -153.968414\n",
      "[7/10000][720/938][1460] Loss_D: 118.025650 Loss_G: -136.114243 Loss_D_real: -27.943789 Loss_D_fake -145.969437\n",
      "[7/10000][820/938][1480] Loss_D: 100.508339 Loss_G: -160.006165 Loss_D_real: -22.606468 Loss_D_fake -123.114807\n",
      "[7/10000][920/938][1500] Loss_D: 98.036911 Loss_G: -182.688004 Loss_D_real: -25.889908 Loss_D_fake -123.926819\n",
      "[8/10000][80/938][1520] Loss_D: 126.399208 Loss_G: -100.501144 Loss_D_real: -35.726234 Loss_D_fake -162.125443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10000][180/938][1540] Loss_D: 114.969391 Loss_G: -87.296211 Loss_D_real: -14.884798 Loss_D_fake -129.854187\n",
      "[8/10000][280/938][1560] Loss_D: 107.734879 Loss_G: -151.915863 Loss_D_real: -18.334713 Loss_D_fake -126.069588\n",
      "[8/10000][380/938][1580] Loss_D: 99.769836 Loss_G: -181.887344 Loss_D_real: -25.105713 Loss_D_fake -124.875549\n",
      "[8/10000][480/938][1600] Loss_D: 112.684494 Loss_G: -193.305023 Loss_D_real: -51.356533 Loss_D_fake -164.041031\n",
      "[8/10000][580/938][1620] Loss_D: 118.803925 Loss_G: -150.884781 Loss_D_real: -49.254345 Loss_D_fake -168.058273\n",
      "[8/10000][680/938][1640] Loss_D: 123.356415 Loss_G: -183.752609 Loss_D_real: -45.049297 Loss_D_fake -168.405716\n",
      "[8/10000][780/938][1660] Loss_D: 109.669586 Loss_G: -172.763535 Loss_D_real: -42.989582 Loss_D_fake -152.659164\n",
      "[8/10000][880/938][1680] Loss_D: 112.787804 Loss_G: -191.313461 Loss_D_real: -88.075462 Loss_D_fake -200.863266\n",
      "[9/10000][40/938][1700] Loss_D: 125.823250 Loss_G: -161.495102 Loss_D_real: -44.815376 Loss_D_fake -170.638626\n",
      "[9/10000][140/938][1720] Loss_D: 109.134636 Loss_G: -196.500122 Loss_D_real: -72.172844 Loss_D_fake -181.307480\n",
      "[9/10000][240/938][1740] Loss_D: 115.645630 Loss_G: -174.162949 Loss_D_real: -46.833714 Loss_D_fake -162.479340\n",
      "[9/10000][340/938][1760] Loss_D: 126.534241 Loss_G: -194.606461 Loss_D_real: -79.525467 Loss_D_fake -206.059708\n",
      "[9/10000][440/938][1780] Loss_D: 120.658859 Loss_G: -170.499695 Loss_D_real: -78.930344 Loss_D_fake -199.589203\n",
      "[9/10000][540/938][1800] Loss_D: 111.635696 Loss_G: -178.859421 Loss_D_real: -71.255463 Loss_D_fake -182.891159\n",
      "[9/10000][640/938][1820] Loss_D: 106.024895 Loss_G: -197.676407 Loss_D_real: -85.716652 Loss_D_fake -191.741547\n",
      "[9/10000][740/938][1840] Loss_D: 115.924400 Loss_G: -122.627502 Loss_D_real: -52.125359 Loss_D_fake -168.049759\n",
      "[9/10000][840/938][1860] Loss_D: 116.666435 Loss_G: -217.725693 Loss_D_real: -86.637123 Loss_D_fake -203.303558\n",
      "[9/10000][938/938][1880] Loss_D: 115.583542 Loss_G: -238.751984 Loss_D_real: -84.873367 Loss_D_fake -200.456909\n",
      "[10/10000][100/938][1900] Loss_D: 110.063530 Loss_G: -198.255417 Loss_D_real: -72.319298 Loss_D_fake -182.382828\n",
      "[10/10000][200/938][1920] Loss_D: 105.961823 Loss_G: -183.674789 Loss_D_real: -67.495255 Loss_D_fake -173.457077\n",
      "[10/10000][300/938][1940] Loss_D: 123.912231 Loss_G: -202.631989 Loss_D_real: -105.135635 Loss_D_fake -229.047867\n",
      "[10/10000][400/938][1960] Loss_D: 104.028030 Loss_G: -158.749817 Loss_D_real: -62.640259 Loss_D_fake -166.668289\n",
      "[10/10000][500/938][1980] Loss_D: 110.437355 Loss_G: -171.615860 Loss_D_real: -73.693489 Loss_D_fake -184.130844\n",
      "[10/10000][600/938][2000] Loss_D: 110.009789 Loss_G: -219.995712 Loss_D_real: -91.494759 Loss_D_fake -201.504547\n",
      "[10/10000][700/938][2020] Loss_D: 104.651497 Loss_G: -197.647308 Loss_D_real: -92.960686 Loss_D_fake -197.612183\n",
      "[10/10000][800/938][2040] Loss_D: 114.660370 Loss_G: -185.104935 Loss_D_real: -68.776657 Loss_D_fake -183.437027\n",
      "[10/10000][900/938][2060] Loss_D: 120.535980 Loss_G: -229.808578 Loss_D_real: -146.376984 Loss_D_fake -266.912964\n",
      "[11/10000][60/938][2080] Loss_D: 133.085480 Loss_G: -206.222046 Loss_D_real: -124.787682 Loss_D_fake -257.873169\n",
      "[11/10000][160/938][2100] Loss_D: 114.009361 Loss_G: -210.070938 Loss_D_real: -102.695763 Loss_D_fake -216.705124\n",
      "[11/10000][260/938][2120] Loss_D: 104.494637 Loss_G: -227.557739 Loss_D_real: -86.400520 Loss_D_fake -190.895157\n",
      "[11/10000][360/938][2140] Loss_D: 124.233116 Loss_G: -207.428726 Loss_D_real: -113.610756 Loss_D_fake -237.843872\n",
      "[11/10000][460/938][2160] Loss_D: 132.735870 Loss_G: -205.940567 Loss_D_real: -114.081146 Loss_D_fake -246.817017\n",
      "[11/10000][560/938][2180] Loss_D: 116.869057 Loss_G: -235.301636 Loss_D_real: -123.442833 Loss_D_fake -240.311890\n",
      "[11/10000][660/938][2200] Loss_D: 119.429855 Loss_G: -218.692429 Loss_D_real: -117.163437 Loss_D_fake -236.593292\n",
      "[11/10000][760/938][2220] Loss_D: 123.874794 Loss_G: -233.749527 Loss_D_real: -124.501701 Loss_D_fake -248.376495\n",
      "[11/10000][860/938][2240] Loss_D: 123.950943 Loss_G: -226.488785 Loss_D_real: -125.617920 Loss_D_fake -249.568863\n",
      "[12/10000][20/938][2260] Loss_D: 112.368652 Loss_G: -254.593140 Loss_D_real: -151.524780 Loss_D_fake -263.893433\n",
      "[12/10000][120/938][2280] Loss_D: 112.071030 Loss_G: -277.402374 Loss_D_real: -139.299606 Loss_D_fake -251.370636\n",
      "[12/10000][220/938][2300] Loss_D: 117.750015 Loss_G: -240.827515 Loss_D_real: -137.243607 Loss_D_fake -254.993622\n",
      "[12/10000][320/938][2320] Loss_D: 116.553375 Loss_G: -259.501068 Loss_D_real: -134.515488 Loss_D_fake -251.068863\n",
      "[12/10000][420/938][2340] Loss_D: 125.515060 Loss_G: -260.382294 Loss_D_real: -150.674423 Loss_D_fake -276.189484\n",
      "[12/10000][520/938][2360] Loss_D: 125.885437 Loss_G: -287.181061 Loss_D_real: -176.978485 Loss_D_fake -302.863922\n",
      "[12/10000][620/938][2380] Loss_D: 103.767746 Loss_G: -258.901978 Loss_D_real: -158.524277 Loss_D_fake -262.292023\n",
      "[12/10000][720/938][2400] Loss_D: 113.949875 Loss_G: -272.778839 Loss_D_real: -169.083298 Loss_D_fake -283.033173\n",
      "[12/10000][820/938][2420] Loss_D: 120.241287 Loss_G: -274.398315 Loss_D_real: -162.204453 Loss_D_fake -282.445740\n",
      "[12/10000][920/938][2440] Loss_D: 113.110672 Loss_G: -286.785187 Loss_D_real: -171.531326 Loss_D_fake -284.641998\n",
      "[13/10000][80/938][2460] Loss_D: 119.222855 Loss_G: -239.607407 Loss_D_real: -126.227859 Loss_D_fake -245.450714\n",
      "[13/10000][180/938][2480] Loss_D: 120.135300 Loss_G: -245.672729 Loss_D_real: -168.695358 Loss_D_fake -288.830658\n",
      "[13/10000][280/938][2500] Loss_D: 115.721130 Loss_G: -251.042587 Loss_D_real: -181.773590 Loss_D_fake -297.494720\n",
      "[13/10000][380/938][2520] Loss_D: 120.292969 Loss_G: -274.409302 Loss_D_real: -163.171844 Loss_D_fake -283.464813\n",
      "[13/10000][480/938][2540] Loss_D: 116.653946 Loss_G: -252.072388 Loss_D_real: -156.107315 Loss_D_fake -272.761261\n",
      "[13/10000][580/938][2560] Loss_D: 131.916290 Loss_G: -253.062012 Loss_D_real: -169.591919 Loss_D_fake -301.508209\n",
      "[13/10000][680/938][2580] Loss_D: 105.098511 Loss_G: -258.837860 Loss_D_real: -142.884277 Loss_D_fake -247.982788\n",
      "[13/10000][780/938][2600] Loss_D: 118.098419 Loss_G: -306.034149 Loss_D_real: -129.597412 Loss_D_fake -247.695831\n",
      "[13/10000][880/938][2620] Loss_D: 123.340500 Loss_G: -240.576187 Loss_D_real: -162.008469 Loss_D_fake -285.348969\n",
      "[14/10000][40/938][2640] Loss_D: 99.844208 Loss_G: -265.699585 Loss_D_real: -128.967209 Loss_D_fake -228.811417\n",
      "[14/10000][140/938][2660] Loss_D: 117.728806 Loss_G: -225.585480 Loss_D_real: -150.854721 Loss_D_fake -268.583527\n",
      "[14/10000][240/938][2680] Loss_D: 109.396576 Loss_G: -331.346985 Loss_D_real: -192.315338 Loss_D_fake -301.711914\n",
      "[14/10000][340/938][2700] Loss_D: 122.746292 Loss_G: -348.823273 Loss_D_real: -229.649490 Loss_D_fake -352.395782\n",
      "[14/10000][440/938][2720] Loss_D: 122.318970 Loss_G: -273.083130 Loss_D_real: -188.473602 Loss_D_fake -310.792572\n",
      "[14/10000][540/938][2740] Loss_D: 97.802307 Loss_G: -303.483490 Loss_D_real: -160.946259 Loss_D_fake -258.748566\n",
      "[14/10000][640/938][2760] Loss_D: 126.459564 Loss_G: -283.361755 Loss_D_real: -191.060333 Loss_D_fake -317.519897\n",
      "[14/10000][740/938][2780] Loss_D: 127.748886 Loss_G: -323.416046 Loss_D_real: -182.341934 Loss_D_fake -310.090820\n",
      "[14/10000][840/938][2800] Loss_D: 124.634949 Loss_G: -306.517670 Loss_D_real: -157.890350 Loss_D_fake -282.525299\n",
      "[14/10000][938/938][2820] Loss_D: 121.634293 Loss_G: -333.008820 Loss_D_real: -172.028885 Loss_D_fake -293.663177\n",
      "[15/10000][100/938][2840] Loss_D: 101.408768 Loss_G: -316.224701 Loss_D_real: -161.760147 Loss_D_fake -263.168915\n",
      "[15/10000][200/938][2860] Loss_D: 122.393677 Loss_G: -297.885162 Loss_D_real: -176.517426 Loss_D_fake -298.911102\n",
      "[15/10000][300/938][2880] Loss_D: 124.440887 Loss_G: -322.911438 Loss_D_real: -195.178345 Loss_D_fake -319.619232\n",
      "[15/10000][400/938][2900] Loss_D: 123.545364 Loss_G: -321.788757 Loss_D_real: -217.870682 Loss_D_fake -341.416046\n",
      "[15/10000][500/938][2920] Loss_D: 117.479279 Loss_G: -352.879974 Loss_D_real: -228.491089 Loss_D_fake -345.970367\n",
      "[15/10000][600/938][2940] Loss_D: 138.105774 Loss_G: -325.365204 Loss_D_real: -254.384430 Loss_D_fake -392.490204\n",
      "[15/10000][700/938][2960] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[15/10000][800/938][2980] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/10000][900/938][3000] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][60/938][3020] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][160/938][3040] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][260/938][3060] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][360/938][3080] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][460/938][3100] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][560/938][3120] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][660/938][3140] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][760/938][3160] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[16/10000][860/938][3180] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][20/938][3200] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][120/938][3220] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][220/938][3240] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][320/938][3260] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][420/938][3280] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][520/938][3300] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n",
      "[17/10000][620/938][3320] Loss_D: nan Loss_G: nan Loss_D_real: nan Loss_D_fake nan\n"
     ]
    }
   ],
   "source": [
    "gen_iterations = 0\n",
    "critic_iters = 5\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(dataloader):\n",
    "                continue\n",
    "            real_cpu, _ = data_iter.next()\n",
    "            i +=1\n",
    "            batch_size = real_cpu.size(0)\n",
    "            real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "            netD.zero_grad()\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD(inputv).mean().view(1)\n",
    "            errD_real.backward(mone)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(batch_size, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            #inputv = fake\n",
    "            errD_fake = netD(fake).mean().view(1)\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, inputv.data, fake.data,lamda,batch_size).mean().view(1)\n",
    "            gradient_penalty.backward(one)\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            errD = errD_real - errD_fake \n",
    "            optimizerD.step()\n",
    "            \n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake).mean().view(1)\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'WGANGP.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'WGANGP_fake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1,1],[32,32]],dtype='float32'))\n",
    "print(a.size())\n",
    "a.norm(2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputv.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'netG_try_kmeans_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD.state_dict(), 'netD_try_kmeans_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D_feature(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D_feature, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        #main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        #nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        #output = output.mean(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 1\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_train = X_train[0:1000,:,:,:]\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train_ = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train_,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=batchSize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_G = torch.load('netG_try_kmeans_epoch_%d.pth' % (1910))\n",
    "dict_D = torch.load('netD_try_kmeans_epoch_%d.pth' % (1910))\n",
    "del(dict_D['main.final.256-1.conv.weight'])\n",
    "\n",
    "feature_dict = {}\n",
    "netD_feature = DCGAN_D_feature()\n",
    "netD_feature.load_state_dict(dict_D)\n",
    "print(netD_feature)\n",
    "data_iter = iter(dataloader)\n",
    "for i,data in enumerate(data_iter):\n",
    "    feature_dict[i] = netD_feature(Variable(data[0])).data.numpy().reshape((1,256*4*4))\n",
    "    \n",
    "feature = np.zeros((max(feature_dict.keys()),4096),dtype=np.float32)\n",
    "for i,data in enumerate(feature):\n",
    "    data[:] = feature_dict[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "for i in range(0,10):\n",
    "    cluster = np.take(X_train, arg[i],axis=0).reshape(-1,3,32,32)\n",
    "    vutils.save_image(torch.from_numpy(cluster), 'cluster%d_%d.png'% (10,i),normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.take(X_train, [0],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "pic_list = []\n",
    "for i,index in enumerate(arg):\n",
    "    X_train[n,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
