{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        #main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        #nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            #nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            #nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G (\n",
      "  (main): Sequential (\n",
      "    (initial.100-256.convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "DCGAN_D (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "    (final.256-1.conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchSize = 64\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates).view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean().view(1)* lamda\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][100/938][20] Loss_D: 120.555008 Loss_G: 16.957323 Loss_D_real: 137.171539 Loss_D_fake 16.616535\n",
      "[0/10000][200/938][40] Loss_D: 92.974213 Loss_G: 18.324451 Loss_D_real: 109.842697 Loss_D_fake 16.868484\n",
      "[0/10000][300/938][60] Loss_D: 95.627014 Loss_G: -15.291439 Loss_D_real: 83.460770 Loss_D_fake -12.166241\n",
      "[0/10000][400/938][80] Loss_D: 89.099083 Loss_G: -12.100209 Loss_D_real: 80.900665 Loss_D_fake -8.198415\n",
      "[0/10000][500/938][100] Loss_D: 80.669350 Loss_G: 3.513722 Loss_D_real: 83.960426 Loss_D_fake 3.291076\n",
      "[0/10000][600/938][120] Loss_D: 69.577332 Loss_G: 9.281697 Loss_D_real: 80.516281 Loss_D_fake 10.938950\n",
      "[0/10000][700/938][140] Loss_D: 56.077477 Loss_G: 19.412567 Loss_D_real: 75.619743 Loss_D_fake 19.542269\n",
      "[0/10000][800/938][160] Loss_D: 48.263042 Loss_G: 14.705771 Loss_D_real: 65.616600 Loss_D_fake 17.353558\n",
      "[0/10000][900/938][180] Loss_D: 48.431168 Loss_G: 14.901585 Loss_D_real: 63.900665 Loss_D_fake 15.469500\n",
      "[1/10000][60/938][200] Loss_D: 43.496532 Loss_G: 9.875791 Loss_D_real: 52.885208 Loss_D_fake 9.388675\n",
      "[1/10000][160/938][220] Loss_D: 41.580704 Loss_G: 9.134976 Loss_D_real: 50.615608 Loss_D_fake 9.034904\n",
      "[1/10000][260/938][240] Loss_D: 41.730797 Loss_G: 3.872589 Loss_D_real: 46.827667 Loss_D_fake 5.096870\n",
      "[1/10000][360/938][260] Loss_D: 37.937584 Loss_G: 6.776629 Loss_D_real: 46.070198 Loss_D_fake 8.132615\n",
      "[1/10000][460/938][280] Loss_D: 32.613117 Loss_G: 6.786242 Loss_D_real: 39.649483 Loss_D_fake 7.036366\n",
      "[1/10000][560/938][300] Loss_D: 27.892601 Loss_G: 11.443962 Loss_D_real: 38.448616 Loss_D_fake 10.556016\n",
      "[1/10000][660/938][320] Loss_D: 25.503189 Loss_G: 7.523812 Loss_D_real: 31.653170 Loss_D_fake 6.149981\n",
      "[1/10000][760/938][340] Loss_D: 21.353876 Loss_G: 1.870005 Loss_D_real: 24.271044 Loss_D_fake 2.917167\n",
      "[1/10000][860/938][360] Loss_D: 20.712721 Loss_G: 1.144663 Loss_D_real: 23.088451 Loss_D_fake 2.375731\n",
      "[2/10000][20/938][380] Loss_D: 18.454105 Loss_G: 0.352878 Loss_D_real: 20.389246 Loss_D_fake 1.935141\n",
      "[2/10000][120/938][400] Loss_D: 19.772451 Loss_G: -6.500606 Loss_D_real: 15.588432 Loss_D_fake -4.184019\n",
      "[2/10000][220/938][420] Loss_D: 20.864603 Loss_G: -5.980235 Loss_D_real: 15.295458 Loss_D_fake -5.569145\n",
      "[2/10000][320/938][440] Loss_D: 18.793709 Loss_G: -3.368786 Loss_D_real: 14.559097 Loss_D_fake -4.234612\n",
      "[2/10000][420/938][460] Loss_D: 17.419064 Loss_G: -2.340187 Loss_D_real: 15.604626 Loss_D_fake -1.814438\n",
      "[2/10000][520/938][480] Loss_D: 17.735624 Loss_G: -5.059662 Loss_D_real: 13.944354 Loss_D_fake -3.791270\n",
      "[2/10000][620/938][500] Loss_D: 15.893886 Loss_G: -3.626362 Loss_D_real: 12.741085 Loss_D_fake -3.152800\n",
      "[2/10000][720/938][520] Loss_D: 17.313181 Loss_G: -1.950556 Loss_D_real: 14.887851 Loss_D_fake -2.425331\n",
      "[2/10000][820/938][540] Loss_D: 15.673289 Loss_G: 2.094245 Loss_D_real: 16.824980 Loss_D_fake 1.151690\n",
      "[2/10000][920/938][560] Loss_D: 15.150770 Loss_G: 1.701416 Loss_D_real: 16.176472 Loss_D_fake 1.025702\n",
      "[3/10000][80/938][580] Loss_D: 14.023948 Loss_G: -1.179191 Loss_D_real: 15.020356 Loss_D_fake 0.996409\n",
      "[3/10000][180/938][600] Loss_D: 15.370772 Loss_G: -2.930364 Loss_D_real: 12.382906 Loss_D_fake -2.987866\n",
      "[3/10000][280/938][620] Loss_D: 14.460201 Loss_G: -1.162929 Loss_D_real: 12.870078 Loss_D_fake -1.590123\n",
      "[3/10000][380/938][640] Loss_D: 13.071047 Loss_G: 1.240070 Loss_D_real: 14.670239 Loss_D_fake 1.599192\n",
      "[3/10000][480/938][660] Loss_D: 14.042009 Loss_G: -1.683483 Loss_D_real: 12.815410 Loss_D_fake -1.226600\n",
      "[3/10000][580/938][680] Loss_D: 15.924603 Loss_G: -5.166853 Loss_D_real: 11.044543 Loss_D_fake -4.880059\n",
      "[3/10000][680/938][700] Loss_D: 13.194341 Loss_G: -1.472190 Loss_D_real: 11.956927 Loss_D_fake -1.237414\n",
      "[3/10000][780/938][720] Loss_D: 12.425574 Loss_G: 0.002836 Loss_D_real: 12.529880 Loss_D_fake 0.104305\n",
      "[3/10000][880/938][740] Loss_D: 13.966992 Loss_G: 1.694077 Loss_D_real: 15.452897 Loss_D_fake 1.485905\n",
      "[4/10000][40/938][760] Loss_D: 12.886589 Loss_G: 0.693373 Loss_D_real: 11.806554 Loss_D_fake -1.080035\n",
      "[4/10000][140/938][780] Loss_D: 13.551030 Loss_G: -2.983825 Loss_D_real: 11.443628 Loss_D_fake -2.107402\n",
      "[4/10000][240/938][800] Loss_D: 13.898411 Loss_G: -3.036117 Loss_D_real: 12.565357 Loss_D_fake -1.333053\n",
      "[4/10000][340/938][820] Loss_D: 12.638707 Loss_G: -1.358550 Loss_D_real: 10.346711 Loss_D_fake -2.291996\n",
      "[4/10000][440/938][840] Loss_D: 11.978828 Loss_G: -0.039255 Loss_D_real: 10.855624 Loss_D_fake -1.123204\n",
      "[4/10000][540/938][860] Loss_D: 12.256896 Loss_G: -3.098748 Loss_D_real: 9.013121 Loss_D_fake -3.243776\n",
      "[4/10000][640/938][880] Loss_D: 12.165973 Loss_G: -1.526034 Loss_D_real: 9.757592 Loss_D_fake -2.408381\n",
      "[4/10000][740/938][900] Loss_D: 11.376314 Loss_G: 0.184484 Loss_D_real: 10.915606 Loss_D_fake -0.460708\n",
      "[4/10000][840/938][920] Loss_D: 11.793909 Loss_G: -0.837050 Loss_D_real: 11.212777 Loss_D_fake -0.581132\n",
      "[4/10000][938/938][940] Loss_D: 11.184161 Loss_G: -2.846512 Loss_D_real: 8.722029 Loss_D_fake -2.462132\n",
      "[5/10000][100/938][960] Loss_D: 12.140281 Loss_G: -0.266979 Loss_D_real: 11.588159 Loss_D_fake -0.552122\n",
      "[5/10000][200/938][980] Loss_D: 12.208844 Loss_G: -1.498655 Loss_D_real: 9.997971 Loss_D_fake -2.210873\n",
      "[5/10000][300/938][1000] Loss_D: 12.389414 Loss_G: -1.564532 Loss_D_real: 11.213591 Loss_D_fake -1.175823\n",
      "[5/10000][400/938][1020] Loss_D: 12.548224 Loss_G: -1.486666 Loss_D_real: 10.882129 Loss_D_fake -1.666095\n",
      "[5/10000][500/938][1040] Loss_D: 11.984157 Loss_G: -1.030806 Loss_D_real: 10.961769 Loss_D_fake -1.022388\n",
      "[5/10000][600/938][1060] Loss_D: 11.626833 Loss_G: -3.742479 Loss_D_real: 7.964628 Loss_D_fake -3.662205\n",
      "[5/10000][700/938][1080] Loss_D: 11.884521 Loss_G: -1.890560 Loss_D_real: 10.074504 Loss_D_fake -1.810016\n",
      "[5/10000][800/938][1100] Loss_D: 10.859212 Loss_G: -0.056403 Loss_D_real: 10.420946 Loss_D_fake -0.438266\n",
      "[5/10000][900/938][1120] Loss_D: 9.583812 Loss_G: -1.613330 Loss_D_real: 7.612459 Loss_D_fake -1.971353\n",
      "[6/10000][60/938][1140] Loss_D: 11.255816 Loss_G: -1.831430 Loss_D_real: 9.488348 Loss_D_fake -1.767468\n",
      "[6/10000][160/938][1160] Loss_D: 9.770487 Loss_G: -3.293394 Loss_D_real: 7.635025 Loss_D_fake -2.135463\n",
      "[6/10000][260/938][1180] Loss_D: 9.637660 Loss_G: -0.455057 Loss_D_real: 8.619488 Loss_D_fake -1.018173\n",
      "[6/10000][360/938][1200] Loss_D: 11.621366 Loss_G: -2.076714 Loss_D_real: 9.357613 Loss_D_fake -2.263753\n",
      "[6/10000][460/938][1220] Loss_D: 11.849913 Loss_G: -1.056521 Loss_D_real: 10.353607 Loss_D_fake -1.496306\n",
      "[6/10000][560/938][1240] Loss_D: 11.273471 Loss_G: -1.525938 Loss_D_real: 8.245378 Loss_D_fake -3.028093\n",
      "[6/10000][660/938][1260] Loss_D: 10.485284 Loss_G: -1.827882 Loss_D_real: 8.974049 Loss_D_fake -1.511236\n",
      "[6/10000][760/938][1280] Loss_D: 10.794978 Loss_G: -2.238134 Loss_D_real: 8.542143 Loss_D_fake -2.252836\n",
      "[6/10000][860/938][1300] Loss_D: 10.187525 Loss_G: -0.927270 Loss_D_real: 9.183469 Loss_D_fake -1.004056\n",
      "[7/10000][20/938][1320] Loss_D: 10.627708 Loss_G: 0.427381 Loss_D_real: 10.497397 Loss_D_fake -0.130311\n",
      "[7/10000][120/938][1340] Loss_D: 10.683967 Loss_G: -0.507037 Loss_D_real: 10.639854 Loss_D_fake -0.044112\n",
      "[7/10000][220/938][1360] Loss_D: 10.137079 Loss_G: -2.901145 Loss_D_real: 9.274713 Loss_D_fake -0.862367\n",
      "[7/10000][320/938][1380] Loss_D: 10.527920 Loss_G: 0.595874 Loss_D_real: 10.668867 Loss_D_fake 0.140947\n",
      "[7/10000][420/938][1400] Loss_D: 9.919514 Loss_G: 1.787956 Loss_D_real: 10.867497 Loss_D_fake 0.947984\n",
      "[7/10000][520/938][1420] Loss_D: 10.628956 Loss_G: -1.178877 Loss_D_real: 10.103601 Loss_D_fake -0.525354\n",
      "[7/10000][620/938][1440] Loss_D: 10.430683 Loss_G: -1.810565 Loss_D_real: 8.463880 Loss_D_fake -1.966803\n",
      "[7/10000][720/938][1460] Loss_D: 10.725327 Loss_G: -0.091336 Loss_D_real: 9.286327 Loss_D_fake -1.438999\n",
      "[7/10000][820/938][1480] Loss_D: 9.933325 Loss_G: -1.235576 Loss_D_real: 9.201656 Loss_D_fake -0.731668\n",
      "[7/10000][920/938][1500] Loss_D: 9.615213 Loss_G: -3.773256 Loss_D_real: 6.219183 Loss_D_fake -3.396030\n",
      "[8/10000][80/938][1520] Loss_D: 9.421593 Loss_G: 0.547004 Loss_D_real: 9.841839 Loss_D_fake 0.420247\n",
      "[8/10000][180/938][1540] Loss_D: 9.457906 Loss_G: 0.638795 Loss_D_real: 9.794537 Loss_D_fake 0.336631\n",
      "[8/10000][280/938][1560] Loss_D: 9.684274 Loss_G: -0.645525 Loss_D_real: 8.585620 Loss_D_fake -1.098654\n",
      "[8/10000][380/938][1580] Loss_D: 10.728751 Loss_G: -0.818971 Loss_D_real: 8.595727 Loss_D_fake -2.133024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10000][480/938][1600] Loss_D: 9.895820 Loss_G: 0.307696 Loss_D_real: 9.840294 Loss_D_fake -0.055526\n",
      "[8/10000][580/938][1620] Loss_D: 8.071090 Loss_G: -2.597000 Loss_D_real: 7.400770 Loss_D_fake -0.670320\n",
      "[8/10000][680/938][1640] Loss_D: 8.958690 Loss_G: 0.522664 Loss_D_real: 9.156104 Loss_D_fake 0.197415\n",
      "[8/10000][780/938][1660] Loss_D: 9.447925 Loss_G: -0.190498 Loss_D_real: 8.742894 Loss_D_fake -0.705030\n",
      "[8/10000][880/938][1680] Loss_D: 7.344112 Loss_G: -1.364069 Loss_D_real: 7.566219 Loss_D_fake 0.222107\n",
      "[9/10000][40/938][1700] Loss_D: 10.111905 Loss_G: -2.407081 Loss_D_real: 7.390326 Loss_D_fake -2.721579\n",
      "[9/10000][140/938][1720] Loss_D: 9.573758 Loss_G: -0.518639 Loss_D_real: 9.451454 Loss_D_fake -0.122304\n",
      "[9/10000][240/938][1740] Loss_D: 10.090281 Loss_G: 0.428007 Loss_D_real: 10.487998 Loss_D_fake 0.397718\n",
      "[9/10000][340/938][1760] Loss_D: 9.409363 Loss_G: -0.840538 Loss_D_real: 9.487056 Loss_D_fake 0.077693\n",
      "[9/10000][440/938][1780] Loss_D: 8.031617 Loss_G: 0.134326 Loss_D_real: 8.795334 Loss_D_fake 0.763716\n",
      "[9/10000][540/938][1800] Loss_D: 9.135946 Loss_G: 0.265368 Loss_D_real: 9.961440 Loss_D_fake 0.825494\n",
      "[9/10000][640/938][1820] Loss_D: 7.927846 Loss_G: 0.582281 Loss_D_real: 8.752073 Loss_D_fake 0.824227\n",
      "[9/10000][740/938][1840] Loss_D: 9.319124 Loss_G: 1.072089 Loss_D_real: 8.703881 Loss_D_fake -0.615242\n",
      "[10/10000][200/938][1920] Loss_D: 8.618643 Loss_G: 0.236277 Loss_D_real: 9.314916 Loss_D_fake 0.696273\n"
     ]
    }
   ],
   "source": [
    "gen_iterations = 0\n",
    "critic_iters = 5\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(dataloader):\n",
    "                continue\n",
    "            real_cpu, _ = data_iter.next()\n",
    "            i +=1\n",
    "            batch_size = real_cpu.size(0)\n",
    "            real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            real_data_v = Variable(input)\n",
    "            netD.zero_grad()\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD(real_data_v).mean().view(1)\n",
    "            errD_real.backward(mone)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(batch_size, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(fake).mean().view(1)\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data,lamda,batch_size)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            errD = errD_real - errD_fake \n",
    "            optimizerD.step()\n",
    "            \n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake).mean().view(1)\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'WGANGP.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'WGANGP_fake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1,1],[32,32]],dtype='float32'))\n",
    "print(a.size())\n",
    "a.norm(2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputv.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'netG_try_kmeans_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD.state_dict(), 'netD_try_kmeans_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D_feature(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D_feature, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        #main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        #nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        #output = output.mean(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 1\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_train = X_train[0:1000,:,:,:]\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train_ = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train_,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=batchSize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_G = torch.load('netG_try_kmeans_epoch_%d.pth' % (1910))\n",
    "dict_D = torch.load('netD_try_kmeans_epoch_%d.pth' % (1910))\n",
    "del(dict_D['main.final.256-1.conv.weight'])\n",
    "\n",
    "feature_dict = {}\n",
    "netD_feature = DCGAN_D_feature()\n",
    "netD_feature.load_state_dict(dict_D)\n",
    "print(netD_feature)\n",
    "data_iter = iter(dataloader)\n",
    "for i,data in enumerate(data_iter):\n",
    "    feature_dict[i] = netD_feature(Variable(data[0])).data.numpy().reshape((1,256*4*4))\n",
    "    \n",
    "feature = np.zeros((max(feature_dict.keys()),4096),dtype=np.float32)\n",
    "for i,data in enumerate(feature):\n",
    "    data[:] = feature_dict[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "for i in range(0,10):\n",
    "    cluster = np.take(X_train, arg[i],axis=0).reshape(-1,3,32,32)\n",
    "    vutils.save_image(torch.from_numpy(cluster), 'cluster%d_%d.png'% (10,i),normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.take(X_train, [0],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "pic_list = []\n",
    "for i,index in enumerate(arg):\n",
    "    X_train[n,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
