{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19725, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk1/cell_segment_save/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(0.5), \n",
    "    iaa.Fliplr(0.5), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "seq2 = iaa.Sequential([\n",
    "    iaa.Flipud(0.5), \n",
    "    iaa.Fliplr(0.5), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.138-256.convt): ConvTranspose2d(138, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*10)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*10)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=False)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize):\n",
    "    rand_c = np.zeros((batchsize,10),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "        rand_c[i] = rand\n",
    "    \n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "def generate_fix_noise(dis=1, cont=4, rand=128):\n",
    "    \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "    map1 = np.concatenate((changing_dis,fixed_cont,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    single_cont = np.asarray([float(n-5)*2/5 for n in range(0,10)]*10,dtype = np.float32)\n",
    "    \n",
    "    fixed_dis = np.zeros((100,10),dtype=np.float32)\n",
    "    for t in range(0,5):\n",
    "        fixed_dis[t*20:t*20+20,t*2] = 1\n",
    "        \n",
    "    for t in range (0,4):\n",
    "        fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "        fixed_cont[:,t] = single_cont\n",
    "        map2 = np.concatenate((fixed_dis,fixed_cont,fixed_z),axis=1)\n",
    "        lst.append(map2.astype(np.float32))\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def fix_noise(dis=1, rand=128):\n",
    "        \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(100,138,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 73.146774292 14.6396598816\n",
      "0 40 72.7760009766 3.2018532753\n",
      "0 60 74.6105804443 10.4632329941\n",
      "1 80 69.6628417969 0.352785110474\n",
      "1 100 63.0455551147 0.716050386429\n",
      "1 120 65.6076583862 0.0167212486267\n",
      "2 140 54.5525856018 2.70273852348\n",
      "2 160 58.244468689 0.530369758606\n",
      "2 180 53.9449729919 0.242077827454\n",
      "3 200 46.7189788818 0.0575428009033\n",
      "3 220 43.8084449768 0.806046247482\n",
      "3 240 36.2058639526 0.594546794891\n",
      "4 260 36.3666381836 0.519401550293\n",
      "4 280 39.5933074951 0.42516207695\n",
      "4 300 34.917930603 0.0470192432404\n",
      "5 320 28.8092041016 0.288006067276\n",
      "5 340 26.1269283295 7.69433546066\n",
      "5 360 23.1816978455 0.964841961861\n",
      "6 380 19.1886978149 4.76312875748\n",
      "6 400 19.2713546753 0.108652830124\n",
      "6 420 14.2629823685 0.00463151931763\n",
      "7 440 12.2027921677 0.0908648967743\n",
      "7 460 8.95395278931 0.0014979839325\n",
      "7 480 7.67117881775 3.87652826309\n",
      "8 500 6.78877830505 6.26172876358\n",
      "8 520 6.66382026672 0.0816030353308\n",
      "8 540 4.7885684967 0.137192487717\n",
      "9 560 4.35271120071 6.10926151276\n",
      "9 580 4.10038757324 0.014509677887\n",
      "9 600 4.21042871475 0.149608492851\n",
      "9 620 3.79881668091 2.60077404976\n",
      "10 640 3.94926023483 0.00849226862192\n",
      "10 660 3.16127252579 0.84023976326\n",
      "10 680 3.297062397 0.00273656845093\n",
      "11 700 2.41290259361 0.0273617375642\n",
      "11 720 2.7717962265 0.0209601819515\n",
      "11 740 2.81282567978 0.00118851661682\n",
      "12 760 2.90600538254 0.00283217430115\n",
      "12 780 3.01997733116 0.196887493134\n",
      "12 800 2.68039464951 0.0969252288342\n",
      "13 820 2.73203372955 0.00340247154236\n",
      "13 840 2.46533441544 0.145117163658\n",
      "13 860 2.25130367279 0.000183343887329\n",
      "14 880 2.27978515625 0.000701159238815\n",
      "14 900 2.19095826149 0.00772872567177\n",
      "14 920 2.34670996666 1.24188315868\n",
      "15 940 2.63469076157 0.0726380646229\n",
      "15 960 1.95283377171 0.0159468054771\n",
      "15 980 2.35677814484 1.6025967598\n",
      "16 1000 2.34612941742 0.250482916832\n",
      "16 1020 2.30207633972 0.224294543266\n",
      "16 1040 2.0775578022 0.12963449955\n",
      "17 1060 1.95556437969 0.0270565748215\n",
      "17 1080 2.03025388718 0.00688755512238\n",
      "17 1100 1.74496042728 0.00751566886902\n",
      "18 1120 1.9239500761 0.0653548240662\n",
      "18 1140 1.9548933506 1.39967858791\n",
      "18 1160 1.94782721996 3.36063146591\n",
      "19 1180 1.47503817081 0.0386157035828\n",
      "19 1200 2.04091954231 0.0007164478302\n",
      "19 1220 1.94825124741 5.0665564537\n",
      "19 1240 2.17915129662 1.18827593327\n",
      "20 1260 1.66461837292 0.000464797019958\n",
      "20 1280 1.86616444588 0.0155282020569\n",
      "20 1300 1.66255986691 0.00102138519287\n",
      "21 1320 1.62710368633 0.000112295150757\n",
      "21 1340 1.80323553085 0.0170134305954\n",
      "21 1360 0.982567369938 0.911770939827\n",
      "22 1380 1.70135307312 0.0054167509079\n",
      "22 1400 1.53668999672 0.443870902061\n",
      "22 1420 2.06315994263 0.000178694725037\n",
      "23 1440 1.72891438007 0.000219821929932\n",
      "23 1460 2.10077285767 0.000259637832642\n",
      "23 1480 2.13312935829 0.00891947746277\n",
      "24 1500 1.52801036835 0.193052887917\n",
      "24 1520 1.53598868847 0.0749569237232\n",
      "24 1540 1.35823106766 0.0038970708847\n",
      "25 1560 1.75444340706 0.0004021525383\n",
      "25 1580 1.46570265293 0.00323939323425\n",
      "25 1600 1.64585328102 6.55651092529e-06\n",
      "26 1620 1.58441007137 0.00156199932098\n",
      "26 1640 1.70464766026 5.05909347534\n",
      "26 1660 1.58243978024 0.000908970832825\n",
      "27 1680 0.999033987522 0.00991189479828\n",
      "27 1700 1.95617175102 5.98975610733\n",
      "27 1720 1.73000001907 0.523870229721\n",
      "28 1740 2.1643550396 0.0136367678642\n",
      "28 1760 1.51014375687 0.0034157037735\n",
      "28 1780 1.6205432415 8.34465026855e-06\n",
      "29 1800 1.81936526299 0.000159025192261\n",
      "29 1820 1.17921149731 0.0150835514069\n",
      "29 1840 1.51213955879 0.4173579216\n",
      "29 1860 1.94666445255 0.0846086740494\n",
      "30 1880 1.80174326897 0.000684499740601\n",
      "30 1900 1.95374393463 2.87294387817e-05\n",
      "30 1920 1.78673350811 0.0477347373962\n",
      "31 1940 1.37091672421 0.00346028059721\n",
      "31 1960 1.41727161407 0.0211502313614\n",
      "31 1980 1.90514802933 0.000462055206299\n",
      "32 2000 1.37000882626 0.00419652462006\n",
      "32 2020 1.87906110287 0.00735354423523\n",
      "32 2040 1.71845960617 2.43186950684e-05\n",
      "33 2060 1.48367214203 0.00127351284027\n",
      "33 2080 1.38644742966 0.000111222267151\n",
      "33 2100 1.15987503529 0.96077144146\n",
      "34 2120 1.61373198032 2.16960906982e-05\n",
      "34 2140 1.6586316824 0.440136194229\n",
      "34 2160 1.62906062603 0.125653624535\n",
      "35 2180 1.58653509617 0.00809192657471\n",
      "35 2200 1.46045136452 0.000457048416138\n",
      "35 2220 1.5838009119 0.000186562538147\n",
      "36 2240 1.45788884163 0.000565648078918\n",
      "36 2260 1.42136979103 7.91549682617e-05\n",
      "36 2280 1.05563020706 0.00316393375397\n",
      "37 2300 1.65633010864 5.66929197311\n",
      "37 2320 2.01785087585 0.000610113143921\n",
      "37 2340 1.59214806557 0.0246665477753\n",
      "38 2360 1.40236032009 0.00024151802063\n",
      "38 2380 1.82611751556 2.28881835938e-05\n",
      "38 2400 1.43478107452 0.00454318523407\n",
      "39 2420 1.61667299271 0.283376455307\n",
      "39 2440 1.51431262493 0.823269963264\n",
      "39 2460 1.43570685387 0.000297904014587\n",
      "39 2480 1.29863798618 1.12010383606\n",
      "40 2500 1.93027591705 2.31266021729e-05\n",
      "40 2520 1.36115419865 0.426080942154\n",
      "40 2540 1.45277655125 1.56164169312e-05\n",
      "41 2560 1.44170153141 0.556444764137\n",
      "41 2580 1.65894722939 0.000236988067627\n",
      "41 2600 1.43445670605 0.00466108322144\n",
      "42 2620 1.5818631649 0.00245904922485\n",
      "42 2640 1.29586398602 0.0266752243042\n",
      "42 2660 1.35593318939 0.000602453947067\n",
      "43 2680 1.61065769196 0.0597810745239\n",
      "43 2700 1.47932350636 0.00408339500427\n",
      "43 2720 1.63262534142 0.015145778656\n",
      "44 2740 1.68018865585 0.000655770301819\n",
      "44 2760 1.37116909027 8.11815261841e-05\n",
      "44 2780 1.37893509865 0.0203495025635\n",
      "45 2800 1.28709852695 0.00136482715607\n",
      "45 2820 1.75264084339 0.000710487365723\n",
      "45 2840 1.56298303604 4.58058357239\n",
      "46 2860 1.7767649889 0.0163955688477\n",
      "46 2880 1.41476523876 0.00277996063232\n",
      "46 2900 1.41373646259 9.64403152466e-05\n",
      "47 2920 0.996001005173 0.00065279006958\n",
      "47 2940 1.34903502464 0.0303106307983\n",
      "47 2960 1.67597675323 5.54323196411e-05\n",
      "48 2980 1.42885971069 0.00364315509796\n",
      "48 3000 1.5063675642 0.000697612762451\n",
      "48 3020 1.57249164581 3.66003847122\n",
      "49 3040 1.47393238544 0.0244069099426\n",
      "49 3060 1.60461401939 0.373692274094\n",
      "49 3080 1.53784990311 0.0493865013123\n",
      "49 3100 1.27252531052 0.215947628021\n",
      "50 3120 1.40581285954 0.805812001228\n",
      "50 3140 1.44632458687 0.00504946708679\n",
      "50 3160 1.69455945492 1.41037464142\n",
      "51 3180 1.333766222 0.063663482666\n",
      "51 3200 1.54339206219 0.00122880935669\n",
      "51 3220 1.38412880898 0.00581789016724\n",
      "52 3240 1.5130405426 6.92185544968\n",
      "52 3260 1.24855232239 2.98445272446\n",
      "52 3280 1.24104857445 9.03689002991\n",
      "53 3300 1.28607976437 0.000406265258789\n",
      "53 3320 1.57520544529 0.000123977661133\n",
      "53 3340 1.61351668835 6.17504119873e-05\n",
      "54 3360 1.49222564697 5.57899475098e-05\n",
      "54 3380 1.41326224804 4.76837158203e-06\n",
      "54 3400 1.38011848927 0.00743007659912\n",
      "55 3420 1.27848911285 6.48498535156e-05\n",
      "55 3440 1.45423364639 0.011022567749\n",
      "55 3460 1.51868510246 9.53674316406e-06\n",
      "56 3480 1.49511754513 0.151934623718\n",
      "56 3500 1.41100406647 3.24249267578e-05\n",
      "56 3520 1.30379533768 4.86373901367e-05\n",
      "57 3540 1.47882139683 0.000114440917969\n",
      "57 3560 1.62504911423 0.00192975997925\n",
      "57 3580 1.62718760967 1.90734863281e-06\n",
      "58 3600 1.08016169071 0.0447511672974\n",
      "58 3620 1.52276206017 1.47819519043e-05\n",
      "58 3640 1.35733687878 0.00174808502197\n",
      "59 3660 1.63207924366 0.000839233398438\n",
      "59 3680 1.60126042366 0.000706672668457\n",
      "59 3700 1.64762282372 0.133990287781\n",
      "59 3720 0.857861757278 0.00580286979675\n",
      "60 3740 1.53463256359 2.09808349609e-05\n",
      "60 3760 1.46198689938 0.0349221229553\n",
      "60 3780 1.43554675579 0.000425338745117\n",
      "61 3800 1.27439081669 5.25712966919e-05\n",
      "61 3820 1.05485701561 0.000852584838867\n",
      "61 3840 1.0837675333 8.20159912109e-05\n",
      "62 3860 1.43860960007 0.00126075744629\n",
      "62 3880 1.38781666756 1.98996448517\n",
      "62 3900 1.59594905376 0.218614578247\n",
      "63 3920 1.27251017094 0.118469238281\n",
      "63 3940 1.25654578209 7.62939453125e-06\n",
      "63 3960 1.58049571514 4.19616699219e-05\n",
      "64 3980 1.39184558392 0.000762939453125\n",
      "64 4000 1.42853200436 0.000317573547363\n",
      "64 4020 1.22103750706 0.0\n",
      "65 4040 1.15406906605 0.117359161377\n",
      "65 4060 1.28495037556 0.0\n",
      "65 4080 1.33640420437 2.86733055115\n",
      "66 4100 1.32445263863 0.128391265869\n",
      "66 4120 1.27250611782 0.00388336181641\n",
      "66 4140 1.76269960403 2.86102294922e-06\n",
      "67 4160 1.49829614162 0.0194931030273\n",
      "67 4180 1.48924183846 0.0\n",
      "67 4200 1.33867752552 8.39233398438e-05\n",
      "68 4220 1.15336966515 0.173777580261\n",
      "68 4240 1.4810655117 5.72204589844e-06\n",
      "68 4260 1.45079112053 0.0838069915771\n",
      "69 4280 1.27226185799 0.000598907470703\n",
      "69 4300 1.24017333984 0.00234889984131\n",
      "69 4320 1.1439538002 0.000251770019531\n",
      "69 4340 1.37053632736 3.81469726562e-05\n",
      "70 4360 1.93592405319 0.159805297852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 4380 1.25679910183 0.0011043548584\n",
      "70 4400 1.35548222065 3.81469726562e-06\n",
      "71 4420 1.43884658813 0.0242834091187\n",
      "71 4440 1.03555393219 6.52955245972\n",
      "71 4460 1.70411908627 0.000123977661133\n",
      "72 4480 1.15480387211 0.233791351318\n",
      "72 4500 1.434232831 4.38690185547e-05\n",
      "72 4520 1.53755795956 9.53674316406e-05\n",
      "73 4540 1.61052167416 3.24249267578e-05\n",
      "73 4560 1.26720571518 0.0106563568115\n",
      "73 4580 1.52695930004 4.95910644531e-05\n",
      "74 4600 1.3091622591 1.90734863281e-06\n",
      "74 4620 1.58327627182 1.14440917969e-05\n",
      "74 4640 1.13187265396 6.76357650757\n",
      "75 4660 1.22146892548 2.28881835938e-05\n",
      "75 4680 1.386469841 9.34600830078e-05\n",
      "75 4700 0.818281292915 0.000478744506836\n",
      "76 4720 1.09092748165 0.0013484954834\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+10*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, 10)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+10*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, 10)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_seg.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_samples_seg.png',normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
