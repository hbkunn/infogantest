{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "X_train = functions.bonemarrow_cell()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.138-256.convt): ConvTranspose2d(138, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*10)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*10)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize):\n",
    "    rand_c = np.zeros((batchsize,10),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "        rand_c[i] = rand\n",
    "    \n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "def generate_fix_noise(dis=1, cont=4, rand=128):\n",
    "    \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "    map1 = np.concatenate((changing_dis,fixed_cont,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    single_cont = np.asarray([float(n-5)*2/5 for n in range(0,10)]*10,dtype = np.float32)\n",
    "    \n",
    "    fixed_dis = np.zeros((100,10),dtype=np.float32)\n",
    "    for t in range(0,5):\n",
    "        fixed_dis[t*20:t*20+20,t*2] = 1\n",
    "        \n",
    "    for t in range (0,4):\n",
    "        fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "        fixed_cont[:,t] = single_cont\n",
    "        map2 = np.concatenate((fixed_dis,fixed_cont,fixed_z),axis=1)\n",
    "        lst.append(map2.astype(np.float32))\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def fix_noise(dis=1, rand=128):\n",
    "        \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(100,138,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20 8.35248279572 0.000232604332268\n",
      "3 40 7.25627851486 0.00568279623985\n",
      "4 60 6.74875640869 3.22125852108e-05\n",
      "6 80 5.8663482666 0.0014715497382\n",
      "7 100 5.6945977211 0.00132467038929\n",
      "9 120 5.96909952164 0.0291027277708\n",
      "10 140 5.19147205353 8.23475420475e-05\n",
      "12 160 6.34396791458 0.000300081446767\n",
      "13 180 4.54677724838 0.000263751484454\n",
      "15 200 5.72915220261 0.00091986451298\n",
      "16 220 5.21468687057 1.33911380544e-05\n",
      "18 240 5.01160573959 0.00291320122778\n",
      "19 260 1.93066036701 0.00401925062761\n",
      "21 280 4.23214960098 0.000524769537151\n",
      "23 300 3.35070109367 2.80662206933e-05\n",
      "24 320 3.78325533867 0.000827711541206\n",
      "26 340 3.462474823 0.00016435328871\n",
      "27 360 3.88543987274 0.000359685975127\n",
      "29 380 3.95492196083 7.61919654906e-05\n",
      "30 400 4.4666018486 0.00199199980125\n",
      "32 420 5.03944969177 0.000562960747629\n",
      "33 440 4.07128620148 6.22128136456e-05\n",
      "35 460 4.0890879631 0.000214081257582\n",
      "36 480 3.72262740135 0.0359534956515\n",
      "38 500 3.79451155663 2.36947089434e-05\n",
      "39 520 3.08243894577 0.00250709801912\n",
      "41 540 3.79602622986 1.3830140233e-05\n",
      "43 560 3.89304804802 0.00145417638123\n",
      "44 580 4.11398696899 0.0120826680213\n",
      "46 600 3.21892976761 0.00456326641142\n",
      "47 620 3.46746444702 0.0432185046375\n",
      "49 640 3.72001576424 0.0754025131464\n",
      "50 660 3.86874246597 0.00389690697193\n",
      "52 680 2.9387524128 4.63239848614e-06\n",
      "53 700 3.61095738411 0.000185009092093\n",
      "55 720 2.70546102524 0.000239919871092\n",
      "56 740 3.23624181747 0.00913419947028\n",
      "58 760 3.35667181015 0.000125925987959\n",
      "59 780 4.61862611771 4.51542437077e-05\n",
      "61 800 3.67126655579 0.000694517046213\n",
      "63 820 2.29996466637 0.00181196816266\n",
      "64 840 3.24514079094 0.0013495888561\n",
      "66 860 2.63922643661 0.000212304294109\n",
      "67 880 2.78148698807 2.71201133728e-06\n",
      "69 900 3.31463360786 0.000759705901146\n",
      "70 920 2.38691878319 0.00823467038572\n",
      "72 940 2.80897307396 0.000121258199215\n",
      "73 960 3.54433012009 1.16042792797e-06\n",
      "75 980 3.34486413002 2.6036053896e-05\n",
      "76 1000 2.62378239632 0.000969769433141\n",
      "78 1020 2.75377106667 0.00040266290307\n",
      "79 1040 3.84246563911 0.00567115284503\n",
      "81 1060 2.52805709839 4.61935997009e-07\n",
      "83 1080 2.87878394127 0.000233536586165\n",
      "84 1100 2.63310670853 4.93228435516e-05\n",
      "86 1120 2.83108711243 1.2144446373e-06\n",
      "87 1140 2.43645167351 0.0667859315872\n",
      "89 1160 2.55143785477 0.0491417497396\n",
      "90 1180 2.29978084564 6.15864992142e-05\n",
      "92 1200 3.09608030319 0.000958161428571\n",
      "93 1220 3.4985127449 8.7171792984e-07\n",
      "95 1240 3.12831187248 4.20212745667e-06\n",
      "96 1260 2.46299600601 0.0987405627966\n",
      "98 1280 2.63103961945 0.000542245805264\n",
      "99 1300 -2.17065787315 0.000880360603333\n",
      "101 1320 2.58505606651 5.56278973818e-05\n",
      "103 1340 1.45015859604 1.7836689949e-05\n",
      "104 1360 2.22341942787 4.01139259338e-05\n",
      "106 1380 1.91400372982 7.31181353331e-05\n",
      "107 1400 2.42904829979 0.00703890807927\n",
      "109 1420 2.04243946075 1.74660235643e-05\n",
      "110 1440 2.50779604912 2.86400318146e-05\n",
      "112 1460 2.93984937668 3.76999378204e-06\n",
      "113 1480 2.0975420475 3.57627868652e-07\n",
      "115 1500 3.49161887169 1.05053186417e-06\n",
      "116 1520 2.28727340698 5.81815838814e-05\n",
      "118 1540 4.08704185486 3.44067811966e-05\n",
      "119 1560 0.970512747765 2.53915786743e-05\n",
      "121 1580 2.74855256081 3.4436583519e-05\n",
      "123 1600 2.15902638435 0.00837258994579\n",
      "124 1620 2.92838048935 0.0122857801616\n",
      "126 1640 3.29151058197 1.64732336998e-05\n",
      "127 1660 2.89171791077 2.92435288429e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+10*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, 10)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+10*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, 10)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_2.png',nrow=10,normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'netG_cell_1_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD.state_dict(), 'netD_cell_1_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_D.state_dict(), 'netD_D_cell_1_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_Q.state_dict(), 'netD_Q_cell_1_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 1\n",
    "\n",
    "X_train = functions.bonemarrow_cell()\n",
    "#X_train = X_train[0:1000,:,:,:]\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train_ = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train_,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=batchSize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "data_iter = iter(dataloader)\n",
    "for i,data in enumerate(data_iter):\n",
    "    feature_dict[i] = np.argmax ( netD_Q(netD(Variable(data[0].cuda()))).data.cpu().numpy() )\n",
    "    \n",
    "#feature = np.zeros((max(feature_dict.keys()),4096),dtype=np.float32)\n",
    "#for i,data in enumerate(feature):\n",
    "    #data[:] = feature_dict[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 2, ..., 9, 9, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(feature_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(np.array(feature_dict.values())==n) for n in range(0,10)]\n",
    "for i in range(0,10):\n",
    "    cluster = np.take(X_train, arg[i],axis=0).reshape(-1,3,32,32)\n",
    "    vutils.save_image(torch.from_numpy(cluster), 'cluster%d_%d.png'% (10,i),normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
