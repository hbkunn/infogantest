{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20388, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "pool = ThreadPool(8) \n",
    "root_dir = '/disk1/cell_segmentate_decon/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(0.5), \n",
    "    iaa.Fliplr(0.5), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "seq2 = iaa.Sequential([\n",
    "    iaa.Flipud(0.5), \n",
    "    iaa.Fliplr(0.5), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.138-256.convt): ConvTranspose2d(138, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*10)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*10)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=False)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize):\n",
    "    rand_c = np.zeros((batchsize,10),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "        rand_c[i] = rand\n",
    "    \n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "def generate_fix_noise(dis=1, cont=4, rand=128):\n",
    "    \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "    map1 = np.concatenate((changing_dis,fixed_cont,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    single_cont = np.asarray([float(n-5)*2/5 for n in range(0,10)]*10,dtype = np.float32)\n",
    "    \n",
    "    fixed_dis = np.zeros((100,10),dtype=np.float32)\n",
    "    for t in range(0,5):\n",
    "        fixed_dis[t*20:t*20+20,t*2] = 1\n",
    "        \n",
    "    for t in range (0,4):\n",
    "        fixed_cont = np.zeros((100,cont),dtype = np.float32)\n",
    "        fixed_cont[:,t] = single_cont\n",
    "        map2 = np.concatenate((fixed_dis,fixed_cont,fixed_z),axis=1)\n",
    "        lst.append(map2.astype(np.float32))\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def fix_noise(dis=1, rand=128):\n",
    "        \n",
    "    fixed_z = np.random.randn(10,rand).repeat(10,axis=0)\n",
    "    changing_dis = np.zeros((100,10),dtype = np.float32)\n",
    "    list = [n for n in range(0,10)]*10\n",
    "    for i in range(0,100):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    \n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(100,138,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 66.8973999023 11.3951969147\n",
      "0 40 65.746131897 3.71481323242\n",
      "0 60 63.4511642456 1.72807300091\n",
      "1 80 53.9659194946 6.17868614197\n",
      "1 100 47.9996337891 0.24796462059\n",
      "1 120 46.0099334717 1.33385968208\n",
      "2 140 43.8622665405 6.95227146149\n",
      "2 160 42.3021507263 0.683262228966\n",
      "2 180 36.0889205933 0.302974522114\n",
      "3 200 32.5330581665 0.620330810547\n",
      "3 220 29.7361984253 0.331152439117\n",
      "3 240 25.067489624 0.328016221523\n",
      "4 260 21.8514633179 0.020895242691\n",
      "4 280 22.1823215485 0.159882545471\n",
      "4 300 16.9360275269 3.47094535828\n",
      "4 320 15.7543964386 0.0444298386574\n",
      "5 340 13.6158819199 2.5314540863\n",
      "5 360 11.5642175674 0.0234216451645\n",
      "5 380 11.8638067245 0.201372861862\n",
      "6 400 8.89529037476 0.00457519292831\n",
      "6 420 8.85514640808 0.12627530098\n",
      "6 440 6.7172794342 0.0466945171356\n",
      "7 460 7.90608215332 0.119418621063\n",
      "7 480 6.78898715973 0.0276929140091\n",
      "7 500 5.73141050339 0.235630631447\n",
      "8 520 4.67201900482 0.220414638519\n",
      "8 540 4.05008506775 0.191542506218\n",
      "8 560 3.77863073349 0.00110816955566\n",
      "9 580 3.71256446838 0.000109076499939\n",
      "9 600 3.45507335663 0.177721083164\n",
      "9 620 3.04274320602 0.000761389732361\n",
      "9 640 2.89554452896 0.159017562866\n",
      "10 660 3.40210151672 0.0254970788956\n",
      "10 680 2.7041862011 0.0611557364464\n",
      "10 700 3.17706608772 0.262410521507\n",
      "11 720 2.69477748871 0.00158774852753\n",
      "11 740 2.49202203751 0.0166337490082\n",
      "11 760 2.37653255463 0.396141767502\n",
      "12 780 2.58553981781 4.37105751038\n",
      "12 800 2.24084877968 0.0100072622299\n",
      "12 820 2.35686588287 0.00353562831879\n",
      "13 840 2.58366036415 0.0104178190231\n",
      "13 860 2.10075187683 0.00338326394558\n",
      "13 880 2.09001064301 0.00597155094147\n",
      "14 900 2.22214651108 0.205511569977\n",
      "14 920 2.21249365807 1.47116231918\n",
      "14 940 1.8268456459 0.0035297870636\n",
      "14 960 1.83760094643 2.37659978867\n",
      "15 980 2.38820171356 0.314319849014\n",
      "15 1000 1.93420934677 3.60012054443e-05\n",
      "15 1020 1.81481146812 9.48905944824e-05\n",
      "16 1040 1.9672254324 0.00122725963593\n",
      "16 1060 1.78480947018 0.120443820953\n",
      "16 1080 1.90894389153 0.0908776447177\n",
      "17 1100 2.06614208221 0.0356928110123\n",
      "17 1120 1.53121376038 0.0495219230652\n",
      "17 1140 1.5020891428 0.125238001347\n",
      "18 1160 1.74947392941 0.00173437595367\n",
      "18 1180 1.6054264307 0.0689480304718\n",
      "18 1200 1.63147640228 0.112633317709\n",
      "19 1220 1.39206790924 0.000648617744446\n",
      "19 1240 1.50113534927 0.00106310844421\n",
      "19 1260 1.34799826145 0.0657188892365\n",
      "19 1280 1.41326963902 0.765309929848\n",
      "20 1300 1.81627845764 0.0160731077194\n",
      "20 1320 1.70728588104 0.0020877122879\n",
      "20 1340 1.81539821625 0.0088232755661\n",
      "21 1360 1.3202368021 0.00955253839493\n",
      "21 1380 1.20184159279 0.133826613426\n",
      "21 1400 1.91185975075 1.26969563961\n",
      "22 1420 1.71477591991 0.694007515907\n",
      "22 1440 1.38608002663 6.66379928589e-05\n",
      "22 1460 1.52088522911 0.161990970373\n",
      "23 1480 1.55791401863 3.83790636063\n",
      "23 1500 1.18136405945 0.848060369492\n",
      "23 1520 1.29760575294 0.0155382156372\n",
      "24 1540 1.27896642685 5.5193901062e-05\n",
      "24 1560 1.51833510399 0.0525777339935\n",
      "24 1580 1.10174322128 0.000194072723389\n",
      "24 1600 1.21577501297 0.000360190868378\n",
      "25 1620 2.35655379295 0.747773170471\n",
      "25 1640 1.47031116486 0.00223660469055\n",
      "25 1660 1.48698568344 0.000256657600403\n",
      "26 1680 1.72633910179 0.000476360321045\n",
      "26 1700 1.53342235088 0.329166889191\n",
      "26 1720 1.94053399563 0.00464963912964\n",
      "27 1740 1.38133621216 0.00032651424408\n",
      "27 1760 1.56921195984 0.000271081924438\n",
      "27 1780 1.77944195271 0.03251516819\n",
      "28 1800 1.75407195091 6.74724578857e-05\n",
      "28 1820 1.48838365078 0.393423706293\n",
      "28 1840 1.17209672928 0.000987648963928\n",
      "29 1860 1.21194529533 7.54937934875\n",
      "29 1880 1.84542930126 0.000198602676392\n",
      "29 1900 1.77004408836 0.00224041938782\n",
      "29 1920 1.75549411774 0.0708014965057\n",
      "30 1940 1.43444013596 0.00254189968109\n",
      "30 1960 1.67894232273 0.0313552618027\n",
      "30 1980 1.71801352501 0.144366383553\n",
      "31 2000 1.57250416279 0.000622987747192\n",
      "31 2020 1.5922768116 0.0355036258698\n",
      "31 2040 1.62128555775 0.00734078884125\n",
      "32 2060 1.70301699638 0.0950874090195\n",
      "32 2080 1.32415294647 0.0005863904953\n",
      "32 2100 1.5815321207 4.40734434128\n",
      "33 2120 1.17061066628 0.00090765953064\n",
      "33 2140 1.38717615604 1.78308796883\n",
      "33 2160 1.11479485035 0.113417863846\n",
      "34 2180 1.21925425529 0.00258123874664\n",
      "34 2200 1.52227544785 0.000451564788818\n",
      "34 2220 1.17688310146 0.0178018808365\n",
      "34 2240 1.92369616032 0.000481963157654\n",
      "35 2260 1.49959850311 0.00101208686829\n",
      "35 2280 1.2216129303 0.15180516243\n",
      "35 2300 1.86247825623 0.00482988357544\n",
      "36 2320 1.6985462904 1.90734863281e-06\n",
      "36 2340 1.86467504501 0.00337761640549\n",
      "36 2360 1.64132249355 0.587646722794\n",
      "37 2380 1.51315557957 0.00153636932373\n",
      "37 2400 1.42578446865 0.00039803981781\n",
      "37 2420 1.21937382221 0.0136083364487\n",
      "38 2440 1.32887935638 0.000972621142864\n",
      "38 2460 1.23339629173 0.000128269195557\n",
      "38 2480 1.42613625526 0.000370621681213\n",
      "39 2500 1.74767315388 0.000231981277466\n",
      "39 2520 1.89128947258 0.00982260704041\n",
      "39 2540 1.34732723236 0.000409603118896\n",
      "39 2560 1.25172543526 0.00132215023041\n",
      "40 2580 1.47129023075 4.92353963852\n",
      "40 2600 1.37341475487 0.00158905982971\n",
      "40 2620 1.66717529297 0.0047755241394\n",
      "41 2640 1.50171375275 0.181813001633\n",
      "41 2660 1.46769118309 0.00278949737549\n",
      "41 2680 1.29742801189 0.0225480794907\n",
      "42 2700 1.18641161919 3.38508176804\n",
      "42 2720 1.99962580204 0.206397652626\n",
      "42 2740 1.6010992527 0.00463247299194\n",
      "43 2760 1.51020693779 1.47819519043e-05\n",
      "43 2780 1.0861017704 0.0382649898529\n",
      "43 2800 1.37448394299 0.0014169216156\n",
      "44 2820 1.26603782177 0.468737363815\n",
      "44 2840 1.38591623306 0.000329852104187\n",
      "44 2860 1.59679281712 0.0036506652832\n",
      "44 2880 1.41360843182 0.00764298439026\n",
      "45 2900 1.08184659481 0.00739061832428\n",
      "45 2920 1.09654486179 0.179129600525\n",
      "45 2940 0.935085356236 0.743884801865\n",
      "46 2960 1.40574836731 0.00179243087769\n",
      "46 2980 1.14289236069 2.36842131615\n",
      "46 3000 1.1162352562 0.000824451446533\n",
      "47 3020 0.886350393295 0.00146341323853\n",
      "47 3040 1.43104410172 0.00091540813446\n",
      "47 3060 1.21145319939 0.0165529251099\n",
      "48 3080 1.05403184891 0.101862072945\n",
      "48 3100 1.05657470226 0.0147417783737\n",
      "48 3120 1.4686871767 0.00499677658081\n",
      "49 3140 1.38482654095 0.000431299209595\n",
      "49 3160 1.56053721905 1.35898590088e-05\n",
      "49 3180 1.26876354218 2.1038441658\n",
      "49 3200 1.03155899048 0.0646669864655\n",
      "50 3220 2.02094721794 0.00676906108856\n",
      "50 3240 1.12742495537 0.83334171772\n",
      "50 3260 1.39605355263 0.000815033912659\n",
      "51 3280 1.29520583153 0.0021231174469\n",
      "51 3300 1.24105775356 0.00767135620117\n",
      "51 3320 1.12545192242 0.143409609795\n",
      "52 3340 0.824963092804 0.0463817119598\n",
      "52 3360 1.41172528267 0.0877398252487\n",
      "52 3380 1.44303417206 8.38806056976\n",
      "53 3400 1.5355386734 1.04904174805e-05\n",
      "53 3420 1.60150527954 0.0794599056244\n",
      "53 3440 0.999698400497 0.00126218795776\n",
      "54 3460 1.51134419441 3.43904924393\n",
      "54 3480 1.4487401247 0.000330924987793\n",
      "54 3500 1.28828334808 0.0023467540741\n",
      "54 3520 1.22893440723 0.000999450683594\n",
      "55 3540 1.33433842659 0.00425219535828\n",
      "55 3560 1.52438390255 6.5803527832e-05\n",
      "55 3580 1.36077284813 0.00150513648987\n",
      "56 3600 1.04123365879 2.96870136261\n",
      "56 3620 1.6557996273 0.000264167785645\n",
      "56 3640 1.12099862099 0.734440803528\n",
      "57 3660 1.44037365913 9.34600830078e-05\n",
      "57 3680 1.28987443447 0.00042986869812\n",
      "57 3700 1.02120828629 0.00362586975098\n",
      "58 3720 1.33849000931 3.00407409668e-05\n",
      "58 3740 1.39135146141 0.0479201674461\n",
      "58 3760 1.19544363022 0.00587272644043\n",
      "59 3780 1.15869760513 0.0553231239319\n",
      "59 3800 1.24307870865 5.91278076172e-05\n",
      "59 3820 1.18660235405 0.0519285202026\n",
      "59 3840 1.69767284393 4.48735427856\n",
      "60 3860 1.10922110081 0.248891830444\n",
      "60 3880 1.36826574802 0.167507171631\n",
      "60 3900 1.58701074123 0.299373626709\n",
      "61 3920 1.24136400223 1.14440917969e-05\n",
      "61 3940 0.968692183495 0.0541543960571\n",
      "61 3960 1.41101408005 0.00809574127197\n",
      "62 3980 1.15523338318 0.000125169754028\n",
      "62 4000 1.27540755272 0.00842952728271\n",
      "62 4020 0.838389158249 0.182981491089\n",
      "63 4040 1.0494440794 3.24249267578e-05\n",
      "63 4060 1.02421808243 0.00485038757324\n",
      "63 4080 1.52023637295 0.0281791687012\n",
      "64 4100 0.964263260365 0.00960636138916\n",
      "64 4120 1.69095051289 0.00162029266357\n",
      "64 4140 1.22857546806 0.000148773193359\n",
      "64 4160 1.23156714439 4.44407653809\n",
      "65 4180 1.18125879765 3.64153289795\n",
      "65 4200 1.36625778675 0.00659370422363\n",
      "65 4220 1.24936759472 0.000144958496094\n",
      "66 4240 1.30466127396 0.000105857849121\n",
      "66 4260 1.14104390144 0.013720035553\n",
      "66 4280 0.879443764687 0.00255966186523\n",
      "67 4300 1.24683129787 0.0018196105957\n",
      "67 4320 0.979345142841 0.0466461181641\n",
      "67 4340 1.15386581421 0.031397819519\n",
      "68 4360 1.49976170063 0.0150279998779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 4380 1.17619132996 0.00506401062012\n",
      "68 4400 1.52335810661 0.0591616630554\n",
      "69 4420 1.23678457737 1.33514404297e-05\n",
      "69 4440 1.15312802792 0.712071418762\n",
      "69 4460 1.30706739426 0.00210952758789\n",
      "69 4480 1.34009075165 0.0654258728027\n",
      "70 4500 1.2277725935 8.83490276337\n",
      "70 4520 1.25795102119 1.14440917969e-05\n",
      "70 4540 1.5133061409 0.570677757263\n",
      "71 4560 1.20624375343 0.00126457214355\n",
      "71 4580 1.40605008602 0.000520706176758\n",
      "71 4600 1.41255700588 0.00763416290283\n",
      "72 4620 1.23524868488 0.00115966796875\n",
      "72 4640 0.951492071152 0.000316619873047\n",
      "72 4660 1.29663407803 0.000204086303711\n",
      "73 4680 1.01833677292 0.000782012939453\n",
      "73 4700 1.52845454216 0.00306510925293\n",
      "73 4720 1.67752671242 0.56290435791\n",
      "74 4740 1.33942222595 0.0644855499268\n",
      "74 4760 1.1322132349 0.000131607055664\n",
      "74 4780 1.21183395386 1.33514404297e-05\n",
      "74 4800 1.07246589661 0.442764282227\n",
      "75 4820 1.31430852413 0.00984573364258\n",
      "75 4840 1.43590760231 6.67572021484e-05\n",
      "75 4860 1.54597318172 1.90734863281e-05\n",
      "76 4880 1.37241530418 0.000984191894531\n",
      "76 4900 1.49437880516 0.0255661010742\n",
      "76 4920 1.30835950375 0.0188035964966\n",
      "77 4940 1.14744830132 0.00424957275391\n",
      "77 4960 1.52363610268 0.000333786010742\n",
      "77 4980 1.21729207039 0.012336730957\n",
      "78 5000 1.35328650475 0.00010871887207\n",
      "78 5020 1.38972914219 0.000213623046875\n",
      "78 5040 1.77642023563 0.0764312744141\n",
      "79 5060 1.33133172989 0.000171661376953\n",
      "79 5080 1.24978208542 0.0030689239502\n",
      "79 5100 1.42188096046 0.000822067260742\n",
      "79 5120 1.27889704704 0.000467300415039\n",
      "80 5140 1.02367424965 0.0494232177734\n",
      "80 5160 1.47056102753 0.0428256988525\n",
      "80 5180 1.19198894501 0.000244140625\n",
      "81 5200 1.02533435822 0.000322341918945\n",
      "81 5220 1.07638943195 0.804986953735\n",
      "81 5240 1.17568671703 0.000110626220703\n",
      "82 5260 1.27872228622 1.29672431946\n",
      "82 5280 0.711830258369 0.0632934570312\n",
      "82 5300 1.24838292599 0.0317974090576\n",
      "83 5320 1.15665793419 0.000259399414062\n",
      "83 5340 1.26542806625 0.000198364257812\n",
      "83 5360 1.5509147644 0.000114440917969\n",
      "84 5380 1.42300772667 3.81469726562e-06\n",
      "84 5400 1.13574147224 3.0517578125e-05\n",
      "84 5420 1.4154061079 0.000162124633789\n",
      "84 5440 1.23270869255 0.00013542175293\n",
      "85 5460 1.38585352898 0.000221252441406\n",
      "85 5480 1.01283788681 0.00199031829834\n",
      "85 5500 1.2585593462 0.0247993469238\n",
      "86 5520 1.27867603302 0.0113162994385\n",
      "86 5540 1.43863761425 0.00584030151367\n",
      "86 5560 1.45120954514 0.0658512115479\n",
      "87 5580 0.995190680027 7.62939453125e-06\n",
      "87 5600 1.45031380653 0.0163993835449\n",
      "87 5620 1.29279983044 0.00391006469727\n",
      "88 5640 1.19524204731 0.000185012817383\n",
      "88 5660 0.986220061779 0.000556945800781\n",
      "88 5680 1.08978390694 0.000137329101562\n",
      "89 5700 0.992887973785 7.62939453125e-06\n",
      "89 5720 1.39059686661 0.000846862792969\n",
      "89 5740 1.16440272331 2.28881835938e-05\n",
      "89 5760 1.25989544392 0.00827598571777\n",
      "90 5780 1.39353680611 0.00105476379395\n",
      "90 5800 0.743610739708 0.000165939331055\n",
      "90 5820 1.29529178143 8.01086425781e-05\n",
      "91 5840 1.26177585125 0.00666046142578\n",
      "91 5860 0.89289522171 0.000856399536133\n",
      "91 5880 1.31381249428 0.000879287719727\n",
      "92 5900 1.31741249561 0.00184631347656\n",
      "92 5920 1.35010957718 0.00114250183105\n",
      "92 5940 1.22000575066 3.43322753906e-05\n",
      "93 5960 0.981545746326 12.1474266052\n",
      "93 5980 0.946767628193 8.01086425781e-05\n",
      "93 6000 0.994106411934 0.202276229858\n",
      "94 6020 1.34035754204 6.86645507812e-05\n",
      "94 6040 0.93899834156 0.0120983123779\n",
      "94 6060 1.17658209801 0.0217361450195\n",
      "94 6080 1.24224412441 0.00550079345703\n",
      "95 6100 0.818334162235 0.0926399230957\n",
      "95 6120 1.06278586388 1.52587890625e-05\n",
      "95 6140 0.598899006844 9.82284545898e-05\n",
      "96 6160 0.993431866169 0.0284080505371\n",
      "96 6180 1.38801562786 0.000289916992188\n",
      "96 6200 1.15405392647 0.0\n",
      "97 6220 1.10084366798 5.72204589844e-06\n",
      "97 6240 1.33585548401 3.81469726562e-06\n",
      "97 6260 1.33781242371 0.000537872314453\n",
      "98 6280 0.894709825516 0.0101013183594\n",
      "98 6300 1.20407819748 0.0119781494141\n",
      "98 6320 1.35635566711 0.000385284423828\n",
      "99 6340 1.29173123837 0.452476501465\n",
      "99 6360 1.52727675438 0.266820907593\n",
      "99 6380 1.18130934238 0.0717544555664\n",
      "99 6400 1.04991543293 0.0\n",
      "100 6420 1.0608907938 0.000400543212891\n",
      "100 6440 1.01847088337 0.000959396362305\n",
      "100 6460 1.24289572239 0.00202178955078\n",
      "101 6480 0.918015778065 0.00788497924805\n",
      "101 6500 1.08928585052 0.00440979003906\n",
      "101 6520 1.24184596539 1.14440917969e-05\n",
      "102 6540 1.1200492382 0.000148773193359\n",
      "102 6560 1.10431230068 0.0535640716553\n",
      "102 6580 1.09681415558 0.00114822387695\n",
      "103 6600 1.09975755215 9.53674316406e-05\n",
      "103 6620 1.1865555048 0.00444030761719\n",
      "103 6640 1.10606050491 0.000209808349609\n",
      "104 6660 1.12924647331 0.00831985473633\n",
      "104 6680 1.17487633228 0.0008544921875\n",
      "104 6700 0.971437990665 0.12398147583\n",
      "104 6720 0.700604438782 0.0108299255371\n",
      "105 6740 0.981350421906 0.204074859619\n",
      "105 6760 1.36682534218 0.00122833251953\n",
      "105 6780 1.13118946552 4.95910644531e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+10*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, 10)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+10*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, 10)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_seg.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_samples_seg.png',nrow=10,normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
