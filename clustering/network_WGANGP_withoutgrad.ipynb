{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        return output\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        #main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        #nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            #nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            #nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchSize = 64\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    alpha_1 = torch.rand(batch_size,1,1,1).expand(real_data.size()).cuda()\n",
    "    alpha_2 = torch.rand(batch_size,1,1,1).expand(real_data.size()).cuda()\n",
    "    interpolates_1 = alpha_1 * real_data + ((1 - alpha_1) * fake_data)\n",
    "    interpolates_2 = alpha_2 * real_data + ((1 - alpha_2) * fake_data)\n",
    "    interpolates_1 = Variable(interpolates_1, requires_grad=True)\n",
    "    interpolates_2 = Variable(interpolates_2, requires_grad=True)\n",
    "    disc_interpolates_1 = netD(interpolates_1).view(batch_size,-1)\n",
    "    disc_interpolates_2 = netD(interpolates_2).view(batch_size,-1)\n",
    "    gradient_penalty = (((disc_interpolates_2 - disc_interpolates_1)/(interpolates_1.view(batch_size,-1)-interpolates_2.view(batch_size,-1)))\\\n",
    "                              .norm(2, dim=1)-1)**2*lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def calc_gradient_penalty(netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean().view(1)* lamda\n",
    "    return gradient_penalty\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_iterations = 0\n",
    "critic_iters = 5\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(dataloader):\n",
    "                continue\n",
    "            real_cpu, _ = data_iter.next()\n",
    "            i +=1\n",
    "            batch_size = real_cpu.size(0)\n",
    "            real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "            netD.zero_grad()\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD(inputv).mean().view(1)\n",
    "            errD_real.backward(mone)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(batch_size, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            #inputv = fake\n",
    "            errD_fake = netD(fake).mean().view(1)\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD, inputv.data, fake.data,lamda,batch_size).mean().view(1)\n",
    "            gradient_penalty.backward(one)\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            errD = errD_real - errD_fake \n",
    "            optimizerD.step()\n",
    "            \n",
    "            \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake).mean().view(1)\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'WGANGP.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'WGANGP_fake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1,1],[32,32]],dtype='float32'))\n",
    "print(a.size())\n",
    "a.norm(2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputv.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'netG_try_kmeans_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD.state_dict(), 'netD_try_kmeans_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN_D_feature(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D_feature, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        #main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        #nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        #output = output.mean(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 1\n",
    "\n",
    "X_train = functions.get_mnist()\n",
    "X_train = X_train[0:1000,:,:,:]\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train_ = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train_,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=batchSize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_G = torch.load('netG_try_kmeans_epoch_%d.pth' % (1910))\n",
    "dict_D = torch.load('netD_try_kmeans_epoch_%d.pth' % (1910))\n",
    "del(dict_D['main.final.256-1.conv.weight'])\n",
    "\n",
    "feature_dict = {}\n",
    "netD_feature = DCGAN_D_feature()\n",
    "netD_feature.load_state_dict(dict_D)\n",
    "print(netD_feature)\n",
    "data_iter = iter(dataloader)\n",
    "for i,data in enumerate(data_iter):\n",
    "    feature_dict[i] = netD_feature(Variable(data[0])).data.numpy().reshape((1,256*4*4))\n",
    "    \n",
    "feature = np.zeros((max(feature_dict.keys()),4096),dtype=np.float32)\n",
    "for i,data in enumerate(feature):\n",
    "    data[:] = feature_dict[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "for i in range(0,10):\n",
    "    cluster = np.take(X_train, arg[i],axis=0).reshape(-1,3,32,32)\n",
    "    vutils.save_image(torch.from_numpy(cluster), 'cluster%d_%d.png'% (10,i),normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.take(X_train, [0],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg = [np.argwhere(kmeans==n) for n in range(0,10)]\n",
    "pic_list = []\n",
    "for i,index in enumerate(arg):\n",
    "    X_train[n,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
