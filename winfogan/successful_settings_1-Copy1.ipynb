{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device_ids = [0,1]\n",
    "batchsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_img_resize():\n",
    "    X_pics = np.zeros((3124,3,64,64),dtype=np.float32)\n",
    "    root_dir = '/home/hubo/315/intestinal/'\n",
    "    imageList = os.listdir(root_dir)\n",
    "    q = 0\n",
    "    \n",
    "    for n in imageList:\n",
    "        n = root_dir + n\n",
    "       # print (n)\n",
    "        img = cv2.imread(n, 3)\n",
    "        if img is None:\n",
    "            continue\n",
    "        resize = img[:,:,::-1].transpose((2,0,1))\n",
    "        resize = resize.astype(np.float32)/(255.0/2) - 1.0\n",
    "        resize = resize.reshape((1, 3, 64, 64))\n",
    "        X_pics[q] = resize\n",
    "        q+=1\n",
    "\n",
    "    print (q)    \n",
    "    assert q == X_pics.shape[0]\n",
    "        \n",
    "    np.random.seed(1234) # set seed for deterministic ordering\n",
    "   # p = np.random.permutation(X_train.shape[0])\n",
    "   # X_train = X_train[p]\n",
    "    return X_pics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bonemarrow_cell():\n",
    "    X = np.load(\"/home/msragpu/cellwork/data/data.npy\")\n",
    "    X = np.asarray([cv2.resize(x, (64,64)) for x in X])\n",
    "    X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "    X = X[:,:,:,::-1].transpose((0, 3,1,2))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    mnist = fetch_mldata('MNIST original',data_home=\"/home/msragpu/cellwork/test_dataset/\")\n",
    "    np.random.seed(1234) # set seed for deterministic ordering\n",
    "   #p = np.datacom.permutation(mnist.data.shape[0])\n",
    "   #X = mnist.data[p]\n",
    "    X = mnist.data.reshape((70000, 28, 28))\n",
    "\n",
    "    X = np.asarray([cv2.resize(x, (64,64)) for x in X])\n",
    "\n",
    "    X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "    X = X.reshape((70000, 1, 64, 64)) \n",
    "    X = np.tile(X, (1, 3, 1, 1))\n",
    "    p = np.random.permutation(70000)\n",
    "    X = X[p]\n",
    "    X_train = X[:60000]\n",
    "    X_test = X[60000:70000]\n",
    "    \n",
    "    return X_train.reshape(60000,3,64,64)\n",
    "\n",
    "def skin_lesion():\n",
    "    X_pics = np.zeros((1125,3,64,64),dtype=np.float32)\n",
    "    root_dir =\"/home/msragpu/cellwork/test_dataset/\"\n",
    "    imageList = os.listdir(root_dir)\n",
    "    q = 0\n",
    "    \n",
    "    for n in imageList:\n",
    "        n = root_dir + n\n",
    "        print (n)\n",
    "        img = cv2.imread(n, 3)\n",
    "        if img is None:\n",
    "            continue\n",
    "        hight, width = img.shape[0], img.shape[1]\n",
    "        if (hight/2-300<=0) or (width/2-300<=0):\n",
    "            continue\n",
    "        resize = cv2.resize(img[hight/2-300:hight/2+300,width/2-300:width/2+300,:],(64,64))\n",
    "        resize = resize[:,:,::-1].transpose((2,0,1))\n",
    "        resize = resize.astype(np.float32)/(255.0/2) - 1.0\n",
    "        resize = resize.reshape((1, 3, 64, 64))\n",
    "        X_pics[q] = resize\n",
    "        q+=1\n",
    "\n",
    "    print (q)    \n",
    "    assert q == X_pics.shape[0]\n",
    "        \n",
    "    np.random.seed(1234) # set seed for deterministic ordering\n",
    "   # p = np.random.permutation(X_train.shape[0])\n",
    "   # X_train = X_train[p]\n",
    "    return X_pics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3, 64, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = get_mnist()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "\n",
    "def visual(X):\n",
    "    assert len(X.shape) == 4\n",
    "    X = X.transpose((0, 2, 3, 1))\n",
    "    X = (X+1.0)*(255.0/2.0)\n",
    "    X = X.reshape(X.shape[1],X.shape[2],X.shape[3])\n",
    " #   X = X[:,:,::-1]\n",
    "    return np.uint8(X) #  cv2.waitKey(1)\n",
    "\n",
    "def fill_buf(buf, i, img, shape):\n",
    "    n = buf.shape[0]/shape[1]\n",
    "    m = buf.shape[1]/shape[0]\n",
    "\n",
    "    sx = (i%m)*shape[0]\n",
    "    sy = (i/m)*shape[1]\n",
    "    buf[sy:sy+shape[1], sx:sx+shape[0], :] = img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 32\n",
    "Z_dim = 16\n",
    "X_dim = 64\n",
    "y_dim = 64\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.149-512.convt): ConvTranspose2d(149, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.512.relu): ReLU (inplace)\n",
      "    (pyramid.512-256.convt): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD_D (\n",
      "  (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n",
      "_netD_Q (\n",
      "  (conv): Conv2d(512, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (softmax): Softmax ()\n",
      ")\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "Conv2d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "import torch.nn.parallel\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, ngpu, isize = 64, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        return self.main(input)\n",
    "    \n",
    "#noise = Variable(torch.randn(1, 74))\n",
    "netG = _netG(6)\n",
    "print (netG)\n",
    "#netG(noise)\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 64, nz = 149, nc = 3, ndf = 64, ngpu = 6, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD()\n",
    "#print (netD)\n",
    "\n",
    "generate = Variable(torch.zeros(1,3,64,64))\n",
    "#print netD(generate)\n",
    "\n",
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        x = self.softmax(x)\n",
    "     #   x = x.view(1,-1,1,1)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_2, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        x = self.softmax(x)\n",
    "     #   x = x.view(1,-1,1,1)\n",
    "        return x\n",
    "\n",
    "class _netD_Q_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_3, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()\n",
    "netD_Q_2 = _netD_Q_2()\n",
    "netD_Q_3 = _netD_Q_3()\n",
    "\n",
    "\n",
    "print (netD_D)\n",
    "print (netD_Q)\n",
    "#output = netD(x,'D')\n",
    "#output\n",
    "generate = Variable(torch.zeros(1,3,64,64))\n",
    "netD_Q(netD(generate)).size()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "   # print (classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        print (classname)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        print (classname)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_Q_2.apply(weights_init)\n",
    "netD_Q_3.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "\"\"\" ====================== OPTIMISER ========================== \"\"\"\n",
    "\n",
    "optimizerD = optim.RMSprop([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 0.00005)\n",
    "\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.00005)\n",
    "\n",
    "optimizerQ = optim.RMSprop([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "                {'params': netD_Q_2.parameters()},\n",
    "                {'params': netD_Q_3.parameters()}\n",
    "            ], 0.00005)\n",
    "\n",
    "input = torch.FloatTensor(1, 3, 64, 64)\n",
    "noise = torch.FloatTensor(1, 149,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c2 = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c3 = torch.FloatTensor(np.random.uniform(-1,1,(1,1)))\n",
    "z = torch.randn(1, 128,1,1)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "netD = netD.cuda()\n",
    "netG = netG.cuda()\n",
    "netD_D = netD_D.cuda()\n",
    "netD_Q = netD_Q.cuda()\n",
    "netD_Q_2 = netD_Q_2.cuda()\n",
    "netD_Q_3 = netD_Q_3.cuda()\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c, c2, c3 = z.cuda(), c.cuda(), c2.cuda(), c3.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "c = Variable(c)\n",
    "c2 = Variable(c2)\n",
    "c3 = Variable(c3)\n",
    "z = Variable(z)\n",
    "\n",
    "\"\"\" ======================TRAIN========================== \"\"\"\n",
    "\n",
    "def sample_c():\n",
    "    rand_c = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_Q_2.zero_grad()\n",
    "    netD_Q_3.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "    \n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1\n",
    "mone = mone.cuda()\n",
    "gen_iterations = 0\n",
    "\n",
    "for epoch in range(100000):\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 1\n",
    "        else:\n",
    "            Diters = 1\n",
    "        \n",
    "        j = 0\n",
    "        while j < Diters and i < len(train_loader):\n",
    "            j += 1\n",
    "            image_, _ = dataiter.next()\n",
    "            i +=1\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "            for p in netD_D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "    #train on D\n",
    "    #sending real data \n",
    "            zero_grad()\n",
    "            input.data.copy_(image_)\n",
    "            label.data.resize_(1).fill_(real_label)\n",
    "            D_real =netD_D(netD(input))\n",
    "            #D_loss_real = criterion(D_real, label)\n",
    "            D_real.backward(one)\n",
    "\n",
    "    #sending noise\n",
    "            z.data.normal_(0, 1)\n",
    "            c.data.copy_(sample_c())\n",
    "            c2.data.copy_(sample_c())\n",
    "            c3.data.uniform_(-1,1)\n",
    "            noise = torch.cat([c,c2,c3,z],1)\n",
    "        \n",
    "            G_sample = netG(noise)\n",
    "            D_fake = netD_D(netD(G_sample))\n",
    "            label.data.resize_(1).fill_(fake_label)\n",
    "           # D_loss_fake = criterion(D_fake, label)\n",
    "            D_fake.backward(mone)\n",
    "        \n",
    "    # update D\n",
    "            optimizerD.step()\n",
    "    \n",
    "\n",
    "    # update G  \n",
    "        zero_grad()\n",
    "        G_sample = netG(noise)\n",
    "        D_fake = netD_D(netD(G_sample))\n",
    "        label.data.resize_(1).fill_(real_label)\n",
    "       # G_loss = criterion(D_fake, label)\n",
    "        D_fake.backward(one)\n",
    "        optimizerG.step()\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "    # update Q\n",
    "        zero_grad()\n",
    "        G_sample = netG(noise)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample))\n",
    "        Q_c_given_x_2 = netD_Q_2(netD(G_sample))\n",
    "        Q_c_given_x_3 = netD_Q_3(netD(G_sample))\n",
    "        \n",
    "        crossent_loss = torch.mean(-torch.sum(c * torch.log(Q_c_given_x + 1e-8), dim=1))\n",
    "       # print (Q_c_given_x)\n",
    "        crossent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(Q_c_given_x_2 + 1e-8), dim=1))\n",
    "        crossent_loss_3 = criterion_mse(Q_c_given_x_3, c3) \n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "\n",
    "        mi_loss = crossent_loss + crossent_loss_2 + 0.1*crossent_loss_3\n",
    "\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            vutils.save_image(G_sample.data, 'fake_samples.png')\n",
    "            \n",
    "            storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "            z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "            _c1_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "            _c2_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "            _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "            \n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "                for q in range(0,10):\n",
    "                    _c2 = np.zeros((1,10),dtype = np.float32)\n",
    "                    _c2[0,q] = 1\n",
    "                    _c2 = Variable(torch.Tensor(_c2).cuda())\n",
    "                    noise = torch.cat([_c1_fix,_c2,_c3_fix,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'dis_1.png',nrow=10)\n",
    "            \n",
    "\n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "                for q in range(0,10):\n",
    "                    _c1 = np.zeros((1,10),dtype = np.float32)\n",
    "                    _c1[0,q] = 1\n",
    "                    _c1 = Variable(torch.Tensor(_c1).cuda())\n",
    "                    noise = torch.cat([_c1,_c2_fix,_c3_fix,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'dis_2.png',nrow=10)\n",
    "            #vutils.save_image(torch.FloatTensor(storage), '{0}_info_{1}.png'.format(epoch, gen_iterations),nrow=10)\n",
    "            \n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c1_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "                _c2_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "                for q in range(0,10):\n",
    "                    _c3 = Variable(torch.Tensor(np.asarray([float(q-5)*0.4],dtype=np.float32).reshape(1,1)).cuda())\n",
    "                    noise = torch.cat([_c1_fix,_c2_fix,_c3,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'cont_1.png',nrow=10)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.149-512.convt): ConvTranspose2d(149, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.512.relu): ReLU (inplace)\n",
      "    (pyramid.512-256.convt): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "ConvTranspose2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "Conv2d\n",
      "Conv2d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "import torch.nn.parallel\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, ngpu, isize = 64, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        return self.main(input)\n",
    "    \n",
    "#noise = Variable(torch.randn(1, 74))\n",
    "netG = _netG(0)\n",
    "print (netG)\n",
    "#netG(noise)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 64, nz = 149, nc = 3, ndf = 64, ngpu = 0, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        gpu_ids = None\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            gpu_ids = range(self.ngpu)\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD()\n",
    "#print (netD)\n",
    "\n",
    "generate = Variable(torch.zeros(1,3,64,64))\n",
    "#print netD(generate)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        x = self.softmax(x)\n",
    "     #   x = x.view(1,-1,1,1)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_2, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        x = self.softmax(x)\n",
    "     #   x = x.view(1,-1,1,1)\n",
    "        return x\n",
    "\n",
    "class _netD_Q_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_3, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(1,-1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()\n",
    "netD_Q_2 = _netD_Q_2()\n",
    "netD_Q_3 = _netD_Q_3()\n",
    "\n",
    "generate = Variable(torch.zeros(1,3,64,64))\n",
    "netD_Q(netD(generate)).size()\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \n",
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc2(x)\n",
    "      #  x = x.sigmoid()\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.fc4 = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc4(x)\n",
    "      #  x = x.sigmoid()\n",
    "        return x\n",
    "    \n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()\n",
    "\n",
    "print (netD_D)\n",
    "print (netD_Q)\n",
    "#output = netD(x,'D')\n",
    "#output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "   # print (classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        print (classname)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        print (classname)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_Q_2.apply(weights_init)\n",
    "netD_Q_3.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\"\"\" ====================== OPTIMISER ========================== \"\"\"\n",
    "\n",
    "optimizerD = optim.RMSprop([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 0.00005)\n",
    "\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.00005)\n",
    "\n",
    "optimizerQ = optim.RMSprop([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "                {'params': netD_Q_2.parameters()},\n",
    "                {'params': netD_Q_3.parameters()}\n",
    "            ], 0.00005)\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "input = torch.FloatTensor(1, 3, 64, 64)\n",
    "noise = torch.FloatTensor(1, 149,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c2 = torch.FloatTensor(np.random.multinomial(1, 10*[0.1], size=1))\n",
    "c3 = torch.FloatTensor(np.random.uniform(-1,1,(1,1)))\n",
    "z = torch.randn(1, 128,1,1)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "c3.uniform_(-1,1)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "netD = netD.cuda()\n",
    "netG = netG.cuda()\n",
    "netD_D = netD_D.cuda()\n",
    "netD_Q = netD_Q.cuda()\n",
    "netD_Q_2 = netD_Q_2.cuda()\n",
    "netD_Q_3 = netD_Q_3.cuda()\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c, c2, c3 = z.cuda(), c.cuda(), c2.cuda(), c3.cuda()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "c = Variable(c)\n",
    "c2 = Variable(c2)\n",
    "c3 = Variable(c3)\n",
    "z = Variable(z)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\"\"\" ======================TRAIN========================== \"\"\"\n",
    "\n",
    "def sample_c():\n",
    "    rand_c = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_Q_2.zero_grad()\n",
    "    netD_Q_3.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import time\n",
    "\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1\n",
    "mone = mone.cuda()\n",
    "gen_iterations = 0\n",
    "\n",
    "for epoch in range(100000):\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 1\n",
    "        else:\n",
    "            Diters = 1\n",
    "        \n",
    "        j = 0\n",
    "        while j < Diters and i < len(train_loader):\n",
    "            j += 1\n",
    "            image_, _ = dataiter.next()\n",
    "            i +=1\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "            for p in netD_D.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "    #train on D\n",
    "    #sending real data \n",
    "            zero_grad()\n",
    "            input.data.copy_(image_)\n",
    "            label.data.resize_(1).fill_(real_label)\n",
    "            D_real =netD_D(netD(input))\n",
    "            #D_loss_real = criterion(D_real, label)\n",
    "            D_real.backward(one)\n",
    "\n",
    "    #sending noise\n",
    "            z.data.normal_(0, 1)\n",
    "            c.data.copy_(sample_c())\n",
    "            c2.data.copy_(sample_c())\n",
    "            c3.data.uniform_(-1,1)\n",
    "            noise = torch.cat([c,c2,c3,z],1)\n",
    "        \n",
    "            G_sample = netG(noise)\n",
    "            D_fake = netD_D(netD(G_sample))\n",
    "            label.data.resize_(1).fill_(fake_label)\n",
    "           # D_loss_fake = criterion(D_fake, label)\n",
    "            D_fake.backward(mone)\n",
    "        \n",
    "    # update D\n",
    "            optimizerD.step()\n",
    "    \n",
    "\n",
    "    # update G  \n",
    "        zero_grad()\n",
    "        G_sample = netG(noise)\n",
    "        D_fake = netD_D(netD(G_sample))\n",
    "        label.data.resize_(1).fill_(real_label)\n",
    "       # G_loss = criterion(D_fake, label)\n",
    "        D_fake.backward(one)\n",
    "        optimizerG.step()\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "    # update Q\n",
    "        zero_grad()\n",
    "        G_sample = netG(noise)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample))\n",
    "        Q_c_given_x_2 = netD_Q_2(netD(G_sample))\n",
    "        Q_c_given_x_3 = netD_Q_3(netD(G_sample))\n",
    "        \n",
    "        crossent_loss = torch.mean(-torch.sum(c * torch.log(Q_c_given_x + 1e-8), dim=1))\n",
    "       # print (Q_c_given_x)\n",
    "        crossent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(Q_c_given_x_2 + 1e-8), dim=1))\n",
    "        crossent_loss_3 = criterion_mse(Q_c_given_x_3, c3) \n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "\n",
    "        mi_loss = crossent_loss + crossent_loss_2 + 0.2*crossent_loss_3\n",
    "\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            vutils.save_image(G_sample.data, 'fake_samples.png')\n",
    "            \n",
    "            storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "            z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "            _c1_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "            _c2_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "            _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "            \n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "                for q in range(0,10):\n",
    "                    _c2 = np.zeros((1,10),dtype = np.float32)\n",
    "                    _c2[0,q] = 1\n",
    "                    _c2 = Variable(torch.Tensor(_c2).cuda())\n",
    "                    noise = torch.cat([_c1_fix,_c2,_c3_fix,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'dis_1.png',nrow=10)\n",
    "            \n",
    "\n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c3_fix = Variable(torch.randn(1,1).cuda().uniform_(-1,1))\n",
    "                for q in range(0,10):\n",
    "                    _c1 = np.zeros((1,10),dtype = np.float32)\n",
    "                    _c1[0,q] = 1\n",
    "                    _c1 = Variable(torch.Tensor(_c1).cuda())\n",
    "                    noise = torch.cat([_c1,_c2_fix,_c3_fix,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'dis_2.png',nrow=10)\n",
    "            #vutils.save_image(torch.FloatTensor(storage), '{0}_info_{1}.png'.format(epoch, gen_iterations),nrow=10)\n",
    "            \n",
    "            for k in range(0,10):\n",
    "                z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "                _c1_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "                _c2_fix = Variable(torch.randn(1,10).cuda().normal_(0, 1))\n",
    "                for q in range(0,10):\n",
    "                    _c3 = Variable(torch.Tensor(np.asarray([float(q-5)*0.4],dtype=np.float32).reshape(1,1)).cuda())\n",
    "                    noise = torch.cat([_c1_fix,_c2_fix,_c3,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'cont_1.png',nrow=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.parallel\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 64, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG()\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 64, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD()\n",
    "print (netD)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_2, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 10, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "      #  x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "class _netD_Q_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_Q_3, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q()\n",
    "netD_Q_2 = _netD_Q_2()\n",
    "netD_Q_3 = _netD_Q_3()\n",
    "\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "netD, netG, netD_D, netD_Q, netD_Q_2, netD_Q_3 = [torch.nn.DataParallel(netD.cuda(),device_ids=device_ids),\n",
    "                                                torch.nn.DataParallel(netG.cuda(),device_ids=device_ids),\n",
    "                                                torch.nn.DataParallel(netD_D.cuda(),device_ids=device_ids),\n",
    "                                                torch.nn.DataParallel(netD_Q.cuda(),device_ids=device_ids),\n",
    "                                                torch.nn.DataParallel(netD_Q_2.cuda(),device_ids=device_ids),\n",
    "                                                torch.nn.DataParallel(netD_Q_3.cuda(),device_ids=device_ids)]\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_Q_2.apply(weights_init)\n",
    "netD_Q_3.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.RMSprop([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 0.00005)\n",
    "\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.00005)\n",
    "\n",
    "optimizerQ = optim.RMSprop([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "                {'params': netD_Q_2.parameters()},\n",
    "                {'params': netD_Q_3.parameters()}\n",
    "            ], 0.00004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(batchsize, 3, 64, 64)\n",
    "noise = torch.FloatTensor(batchsize, 149,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "c2 = torch.randn(batchsize, 10)\n",
    "c3 = torch.FloatTensor(np.random.uniform(-1,1,(batchsize,1)))\n",
    "z = torch.randn(batchsize, 128)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c, c2, c3 = z.cuda(), c.cuda(), c2.cuda(), c3.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize):\n",
    "    rand_c = np.zeros((batchsize,10),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, 10*[0.1], size=1)\n",
    "        rand_c[i] = rand\n",
    "    \n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_Q_2.zero_grad()\n",
    "    netD_Q_3.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_Q.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_Q_2.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_Q_3.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        weight_clamp()\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "        else:\n",
    "            Diters = 5\n",
    "        \n",
    "        j = 0\n",
    "        while j < Diters and i < len(train_loader):\n",
    "            j += 1\n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            \n",
    "            image_ = image_.cuda()\n",
    "            \n",
    "            i +=1\n",
    "            weight_clamp()\n",
    "    #train on D\n",
    "    #sending real data \n",
    "            zero_grad()\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "         #   label.data.resize_(1).fill_(real_label)\n",
    "            D_real =netD_D(netD(inputv)).mean(0).view(1)\n",
    "            #D_loss_real = criterion(D_real, label)\n",
    "            D_real.backward(one)\n",
    "\n",
    "    #sending noise\n",
    "            z.normal_(0, 1)\n",
    "            rand_c,label_c = sample_c(batchsize)\n",
    "            c.copy_(rand_c)\n",
    "            rand_c_2,label_c_2 = sample_c(batchsize)\n",
    "            c2.copy_(rand_c_2)\n",
    "            c3.uniform_(-1,1)\n",
    "            noise = torch.cat([c,c2,c3,z],1)\n",
    "            noise_resize = noise.view(batchsize,149,1,1)\n",
    "            noisev = Variable(noise_resize)\n",
    "            \n",
    "            G_sample = Variable(netG(noisev).data)\n",
    "            inputv = G_sample\n",
    "            D_fake = netD_D(netD(inputv)).mean(0).view(1)\n",
    "         #   label.data.resize_(1).fill_(fake_label)\n",
    "           # D_loss_fake = criterion(D_fake, label)\n",
    "            D_fake.backward(mone)\n",
    "        \n",
    "    # update D\n",
    "            optimizerD.step()\n",
    "    \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        for p in netD_D.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "\n",
    "    # update G  \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        D_fake = netD_D(netD(G_sample)).mean(0).view(1)\n",
    "      #  label.data.resize_(1).fill_(real_label)\n",
    "       # G_loss = criterion(D_fake, label)\n",
    "        D_fake.backward(one)\n",
    "        optimizerG.step()\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True # to avoid computation\n",
    "        for p in netD_D.parameters():\n",
    "            p.requires_grad = True # to avoid computation\n",
    "\n",
    "    # update Q\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, 10)\n",
    "        Q_c_given_x_2 = netD_Q_2(netD(G_sample)).view(batchsize, 10)\n",
    "        Q_c_given_x_3 = netD_Q_3(netD(G_sample))\n",
    "        \n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "       # print (Q_c_given_x)\n",
    "        crossent_loss_2 = criterion_logli(Q_c_given_x_2, Variable(label_c_2.cuda())) \n",
    "        crossent_loss_3 = criterion_mse(Q_c_given_x_3, Variable(c3)) \n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "\n",
    "        mi_loss = 1*crossent_loss + 1*crossent_loss_2 + 0.1*crossent_loss_3\n",
    "\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            errD = D_real - D_fake\n",
    "            with open(\"output.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -errD.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -errD.data[0] , mi_loss.data[0]))\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            vutils.save_image(G_sample.data, 'fake_samples.png')\n",
    "            \n",
    "            storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "            z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "        \n",
    "            for k in range(0,10):\n",
    "                _c1 = np.zeros((1,10),dtype = np.float32)\n",
    "                _c1[0,k] = 1\n",
    "                _c1 = Variable(torch.Tensor(_c1).cuda())\n",
    "                for q in range(0,10):\n",
    "                    _c2 = np.zeros((1,10),dtype = np.float32)\n",
    "                    _c2[0,q] = 1\n",
    "                    _c2 = Variable(torch.Tensor(_c2).cuda())\n",
    "                    _c3 = Variable(torch.Tensor(np.asarray([0],dtype=np.float32).reshape(1,1)).cuda())\n",
    "                    noise = torch.cat([_c1,_c2,_c3,z_fix],1)\n",
    "                    G_sample = netG(noise)\n",
    "                    storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            \n",
    "            #vutils.save_image(torch.FloatTensor(storage), '{0}_info_{1}.png'.format(epoch, gen_iterations),nrow=10)\n",
    "            vutils.save_image(torch.FloatTensor(storage), 'info.png',nrow=10)\n",
    "        \n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))\n",
    "        \n",
    "'''        \n",
    "        for k in range(0,10):\n",
    "            _c1 = np.zeros((1,10),dtype = np.float32)\n",
    "            _c1[0,k] = 1\n",
    "            _c1 = Variable(torch.Tensor(_c1).cuda())\n",
    "            for q in range(0,10):\n",
    "                _c2 = np.zeros((1,10),dtype = np.float32)\n",
    "                _c2[0,q] = 1\n",
    "                _c2 = Variable(torch.Tensor(_c2).cuda())\n",
    "                _c3 = Variable(torch.Tensor(np.asarray([0],dtype=np.float32).reshape(1,1)).cuda())\n",
    "                noise = torch.cat([_c1,_c2,_c3,z_fix],1)\n",
    "                G_sample = netG(noise)\n",
    "                storage[k*10+q] = G_sample.data.cpu().numpy()\n",
    "            \n",
    "        vutils.save_image(torch.FloatTensor(storage), '{0}_info_{1}.png'.format(epoch, gen_iterations),nrow=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -errD.data[0] , mi_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
