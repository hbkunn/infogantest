{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final_1024.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1024, 4, 1, 0, bias=False))\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(1024, 1, 1, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        output = output.mean(0)\n",
    "        return output.view(1)\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G (\n",
      "  (main): Sequential (\n",
      "    (initial.100-256.convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "DCGAN_D (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "    (final_1024.256-1.conv): Conv2d(256, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (final.256-1.conv): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "[19/10000][61/61][20] Loss_D: -7.370266 Loss_G: 3.654199 Loss_D_real: -3.734325 Loss_D_fake 3.635941\n",
      "[26/10000][10/61][40] Loss_D: -7.219688 Loss_G: 3.564960 Loss_D_real: -3.725961 Loss_D_fake 3.493727\n",
      "[27/10000][45/61][60] Loss_D: -7.268008 Loss_G: 3.622650 Loss_D_real: -3.731556 Loss_D_fake 3.536452\n",
      "[29/10000][15/61][80] Loss_D: -6.737903 Loss_G: 3.343882 Loss_D_real: -3.458827 Loss_D_fake 3.279076\n",
      "[30/10000][50/61][100] Loss_D: -6.373490 Loss_G: 3.036792 Loss_D_real: -2.989350 Loss_D_fake 3.384140\n",
      "[32/10000][20/61][120] Loss_D: -5.673310 Loss_G: 2.581334 Loss_D_real: -2.399499 Loss_D_fake 3.273811\n",
      "[33/10000][55/61][140] Loss_D: -5.769225 Loss_G: 3.240546 Loss_D_real: -3.048036 Loss_D_fake 2.721189\n",
      "[35/10000][25/61][160] Loss_D: -5.175170 Loss_G: 3.184843 Loss_D_real: -2.902338 Loss_D_fake 2.272832\n",
      "[36/10000][60/61][180] Loss_D: -5.037080 Loss_G: 2.662061 Loss_D_real: -2.316640 Loss_D_fake 2.720440\n",
      "[38/10000][30/61][200] Loss_D: -5.410782 Loss_G: 2.809397 Loss_D_real: -2.644528 Loss_D_fake 2.766254\n",
      "[39/10000][61/61][220] Loss_D: -5.258082 Loss_G: 2.136402 Loss_D_real: -2.457688 Loss_D_fake 2.800395\n",
      "[41/10000][35/61][240] Loss_D: -5.361303 Loss_G: 2.974983 Loss_D_real: -2.785848 Loss_D_fake 2.575456\n",
      "[43/10000][5/61][260] Loss_D: -4.884511 Loss_G: 2.396287 Loss_D_real: -1.950515 Loss_D_fake 2.933997\n",
      "[44/10000][40/61][280] Loss_D: -5.044376 Loss_G: 2.687709 Loss_D_real: -2.597606 Loss_D_fake 2.446771\n",
      "[46/10000][10/61][300] Loss_D: -4.848483 Loss_G: 2.179028 Loss_D_real: -1.962330 Loss_D_fake 2.886153\n",
      "[47/10000][45/61][320] Loss_D: -5.048387 Loss_G: 2.415103 Loss_D_real: -2.246980 Loss_D_fake 2.801407\n",
      "[49/10000][15/61][340] Loss_D: -4.827475 Loss_G: 2.773759 Loss_D_real: -2.477726 Loss_D_fake 2.349748\n",
      "[50/10000][50/61][360] Loss_D: -4.664386 Loss_G: 2.069207 Loss_D_real: -2.028394 Loss_D_fake 2.635992\n",
      "[52/10000][20/61][380] Loss_D: -4.349004 Loss_G: 2.411854 Loss_D_real: -2.292114 Loss_D_fake 2.056890\n",
      "[53/10000][55/61][400] Loss_D: -4.134223 Loss_G: 1.468785 Loss_D_real: -1.557055 Loss_D_fake 2.577168\n",
      "[55/10000][25/61][420] Loss_D: -4.277314 Loss_G: 2.508337 Loss_D_real: -2.473200 Loss_D_fake 1.804114\n",
      "[56/10000][60/61][440] Loss_D: -3.960370 Loss_G: 2.328892 Loss_D_real: -2.167889 Loss_D_fake 1.792480\n",
      "[58/10000][30/61][460] Loss_D: -3.554367 Loss_G: 1.982100 Loss_D_real: -1.877041 Loss_D_fake 1.677325\n",
      "[59/10000][61/61][480] Loss_D: -3.403030 Loss_G: 2.747899 Loss_D_real: -3.050069 Loss_D_fake 0.352961\n",
      "[61/10000][35/61][500] Loss_D: -3.994047 Loss_G: 2.308398 Loss_D_real: -2.164688 Loss_D_fake 1.829360\n",
      "[63/10000][30/61][520] Loss_D: -3.635760 Loss_G: 2.344462 Loss_D_real: -2.260371 Loss_D_fake 1.375389\n",
      "[64/10000][61/61][540] Loss_D: -3.495912 Loss_G: 1.792571 Loss_D_real: -1.996575 Loss_D_fake 1.499337\n",
      "[66/10000][35/61][560] Loss_D: -3.737928 Loss_G: 2.490715 Loss_D_real: -2.246665 Loss_D_fake 1.491263\n",
      "[68/10000][5/61][580] Loss_D: -3.865411 Loss_G: 1.908106 Loss_D_real: -1.788108 Loss_D_fake 2.077303\n",
      "[69/10000][40/61][600] Loss_D: -3.424546 Loss_G: 1.941510 Loss_D_real: -1.870480 Loss_D_fake 1.554066\n",
      "[71/10000][10/61][620] Loss_D: -2.974046 Loss_G: 2.216595 Loss_D_real: -2.135914 Loss_D_fake 0.838132\n",
      "[72/10000][45/61][640] Loss_D: -3.260502 Loss_G: 0.927430 Loss_D_real: -0.929465 Loss_D_fake 2.331037\n",
      "[74/10000][15/61][660] Loss_D: -2.720101 Loss_G: 0.557552 Loss_D_real: -0.512357 Loss_D_fake 2.207744\n",
      "[75/10000][50/61][680] Loss_D: -2.968830 Loss_G: 1.342093 Loss_D_real: -1.289369 Loss_D_fake 1.679462\n",
      "[77/10000][20/61][700] Loss_D: -2.619739 Loss_G: 2.035815 Loss_D_real: -2.022234 Loss_D_fake 0.597505\n",
      "[78/10000][55/61][720] Loss_D: -2.771721 Loss_G: 1.040564 Loss_D_real: -1.090358 Loss_D_fake 1.681363\n",
      "[80/10000][25/61][740] Loss_D: -2.612334 Loss_G: 1.781948 Loss_D_real: -1.596590 Loss_D_fake 1.015744\n",
      "[81/10000][60/61][760] Loss_D: -2.434918 Loss_G: 1.834904 Loss_D_real: -1.756845 Loss_D_fake 0.678073\n",
      "[83/10000][30/61][780] Loss_D: -2.377986 Loss_G: 1.988980 Loss_D_real: -1.996797 Loss_D_fake 0.381189\n",
      "[84/10000][61/61][800] Loss_D: -1.958615 Loss_G: 1.435611 Loss_D_real: -1.983894 Loss_D_fake -0.025279\n",
      "[86/10000][35/61][820] Loss_D: -2.133689 Loss_G: 0.635840 Loss_D_real: -0.651904 Loss_D_fake 1.481784\n",
      "[88/10000][5/61][840] Loss_D: -2.150304 Loss_G: 1.243967 Loss_D_real: -1.341035 Loss_D_fake 0.809268\n",
      "[89/10000][40/61][860] Loss_D: -1.982312 Loss_G: 0.781221 Loss_D_real: -0.841713 Loss_D_fake 1.140600\n",
      "[91/10000][10/61][880] Loss_D: -1.936840 Loss_G: 0.766855 Loss_D_real: -0.975598 Loss_D_fake 0.961242\n",
      "[92/10000][45/61][900] Loss_D: -1.738098 Loss_G: 1.425620 Loss_D_real: -1.356193 Loss_D_fake 0.381905\n",
      "[94/10000][15/61][920] Loss_D: -1.842815 Loss_G: 1.677563 Loss_D_real: -1.540281 Loss_D_fake 0.302534\n",
      "[95/10000][50/61][940] Loss_D: -1.674839 Loss_G: 0.509770 Loss_D_real: -0.577606 Loss_D_fake 1.097233\n",
      "[97/10000][20/61][960] Loss_D: -2.011803 Loss_G: 0.168817 Loss_D_real: -0.471795 Loss_D_fake 1.540008\n",
      "[98/10000][55/61][980] Loss_D: -1.701694 Loss_G: -0.123405 Loss_D_real: -0.300597 Loss_D_fake 1.401096\n",
      "[100/10000][25/61][1000] Loss_D: -1.642283 Loss_G: -0.303135 Loss_D_real: -0.234733 Loss_D_fake 1.407551\n",
      "[102/10000][30/61][1020] Loss_D: -1.684703 Loss_G: 1.663145 Loss_D_real: -1.730852 Loss_D_fake -0.046149\n",
      "[103/10000][61/61][1040] Loss_D: -0.744280 Loss_G: -0.332833 Loss_D_real: -0.044724 Loss_D_fake 0.699556\n",
      "[105/10000][35/61][1060] Loss_D: -1.730038 Loss_G: -0.100647 Loss_D_real: -0.247379 Loss_D_fake 1.482658\n",
      "[107/10000][5/61][1080] Loss_D: -1.389760 Loss_G: 0.825682 Loss_D_real: -1.026552 Loss_D_fake 0.363209\n",
      "[108/10000][40/61][1100] Loss_D: -1.517776 Loss_G: 0.264967 Loss_D_real: -0.625119 Loss_D_fake 0.892657\n",
      "[110/10000][10/61][1120] Loss_D: -1.587033 Loss_G: 1.060046 Loss_D_real: -1.116543 Loss_D_fake 0.470490\n",
      "[111/10000][45/61][1140] Loss_D: -1.393140 Loss_G: 0.245831 Loss_D_real: -0.350331 Loss_D_fake 1.042809\n",
      "[113/10000][15/61][1160] Loss_D: -1.562317 Loss_G: -0.036300 Loss_D_real: -0.409275 Loss_D_fake 1.153042\n",
      "[114/10000][50/61][1180] Loss_D: -1.354102 Loss_G: 1.036095 Loss_D_real: -1.013906 Loss_D_fake 0.340196\n",
      "[116/10000][20/61][1200] Loss_D: -1.280791 Loss_G: 1.460145 Loss_D_real: -1.304505 Loss_D_fake -0.023713\n",
      "[117/10000][55/61][1220] Loss_D: -1.297842 Loss_G: 0.221959 Loss_D_real: -0.346935 Loss_D_fake 0.950907\n",
      "[119/10000][25/61][1240] Loss_D: -1.128176 Loss_G: 1.117670 Loss_D_real: -1.175110 Loss_D_fake -0.046934\n",
      "[120/10000][60/61][1260] Loss_D: -1.217910 Loss_G: -0.093842 Loss_D_real: -0.128997 Loss_D_fake 1.088913\n",
      "[122/10000][30/61][1280] Loss_D: -1.218542 Loss_G: 1.092804 Loss_D_real: -1.245101 Loss_D_fake -0.026559\n",
      "[123/10000][61/61][1300] Loss_D: -1.083434 Loss_G: 0.550943 Loss_D_real: -1.202883 Loss_D_fake -0.119450\n",
      "[125/10000][35/61][1320] Loss_D: -0.938971 Loss_G: 1.137275 Loss_D_real: -1.195590 Loss_D_fake -0.256619\n",
      "[127/10000][5/61][1340] Loss_D: -0.714512 Loss_G: 0.230229 Loss_D_real: -0.684635 Loss_D_fake 0.029877\n",
      "[128/10000][40/61][1360] Loss_D: -1.067382 Loss_G: 0.617082 Loss_D_real: -0.614346 Loss_D_fake 0.453036\n",
      "[130/10000][10/61][1380] Loss_D: -1.169339 Loss_G: 0.149518 Loss_D_real: -0.473124 Loss_D_fake 0.696214\n",
      "[131/10000][45/61][1400] Loss_D: -1.171792 Loss_G: 0.425001 Loss_D_real: -0.504844 Loss_D_fake 0.666949\n",
      "[133/10000][15/61][1420] Loss_D: -1.157811 Loss_G: 0.780023 Loss_D_real: -0.553867 Loss_D_fake 0.603944\n",
      "[134/10000][50/61][1440] Loss_D: -1.132238 Loss_G: 0.763355 Loss_D_real: -0.673286 Loss_D_fake 0.458952\n",
      "[136/10000][20/61][1460] Loss_D: -1.175282 Loss_G: -0.557324 Loss_D_real: -0.091043 Loss_D_fake 1.084240\n",
      "[137/10000][55/61][1480] Loss_D: -1.280002 Loss_G: 0.538125 Loss_D_real: -0.783919 Loss_D_fake 0.496083\n",
      "[139/10000][25/61][1500] Loss_D: -1.173981 Loss_G: -0.163405 Loss_D_real: -0.082660 Loss_D_fake 1.091321\n",
      "[141/10000][30/61][1520] Loss_D: -0.971818 Loss_G: -0.448393 Loss_D_real: 0.156534 Loss_D_fake 1.128353\n",
      "[142/10000][61/61][1540] Loss_D: -1.618464 Loss_G: 1.209401 Loss_D_real: -0.732488 Loss_D_fake 0.885977\n",
      "[144/10000][35/61][1560] Loss_D: -0.946970 Loss_G: 0.041454 Loss_D_real: 0.343396 Loss_D_fake 1.290366\n",
      "[146/10000][5/61][1580] Loss_D: -1.139024 Loss_G: 0.932187 Loss_D_real: -0.933816 Loss_D_fake 0.205208\n",
      "[147/10000][40/61][1600] Loss_D: -1.031105 Loss_G: 0.049755 Loss_D_real: -0.281746 Loss_D_fake 0.749359\n",
      "[149/10000][10/61][1620] Loss_D: -1.154524 Loss_G: 1.390913 Loss_D_real: -1.000072 Loss_D_fake 0.154452\n",
      "[150/10000][45/61][1640] Loss_D: -1.218735 Loss_G: 1.216896 Loss_D_real: -0.630980 Loss_D_fake 0.587755\n",
      "[152/10000][15/61][1660] Loss_D: -1.069133 Loss_G: 1.138157 Loss_D_real: -1.408043 Loss_D_fake -0.338910\n",
      "[153/10000][50/61][1680] Loss_D: -1.088304 Loss_G: 0.919518 Loss_D_real: -1.078754 Loss_D_fake 0.009550\n",
      "[155/10000][20/61][1700] Loss_D: -1.194130 Loss_G: 0.027220 Loss_D_real: -0.514476 Loss_D_fake 0.679654\n",
      "[156/10000][55/61][1720] Loss_D: -1.131154 Loss_G: 0.459919 Loss_D_real: -0.432751 Loss_D_fake 0.698403\n",
      "[158/10000][25/61][1740] Loss_D: -0.916573 Loss_G: 1.039378 Loss_D_real: -1.284128 Loss_D_fake -0.367556\n",
      "[159/10000][60/61][1760] Loss_D: -0.936144 Loss_G: 0.121198 Loss_D_real: 0.270482 Loss_D_fake 1.206626\n",
      "[161/10000][30/61][1780] Loss_D: -0.994813 Loss_G: 1.228439 Loss_D_real: -1.121309 Loss_D_fake -0.126497\n",
      "[162/10000][61/61][1800] Loss_D: -1.094618 Loss_G: -0.275917 Loss_D_real: 0.158101 Loss_D_fake 1.252719\n",
      "[164/10000][35/61][1820] Loss_D: -1.052751 Loss_G: -0.292089 Loss_D_real: -0.177605 Loss_D_fake 0.875146\n",
      "[166/10000][5/61][1840] Loss_D: -1.009129 Loss_G: 0.808082 Loss_D_real: -1.077840 Loss_D_fake -0.068711\n",
      "[167/10000][40/61][1860] Loss_D: -1.039886 Loss_G: 1.473900 Loss_D_real: -1.121814 Loss_D_fake -0.081928\n",
      "[169/10000][10/61][1880] Loss_D: -1.038811 Loss_G: 1.276020 Loss_D_real: -1.291517 Loss_D_fake -0.252706\n",
      "[170/10000][45/61][1900] Loss_D: -1.010045 Loss_G: -0.229114 Loss_D_real: 0.217700 Loss_D_fake 1.227745\n",
      "[172/10000][15/61][1920] Loss_D: -0.848818 Loss_G: 0.168297 Loss_D_real: 0.365540 Loss_D_fake 1.214358\n",
      "[173/10000][50/61][1940] Loss_D: -0.957116 Loss_G: -0.174656 Loss_D_real: 0.229085 Loss_D_fake 1.186201\n",
      "[175/10000][20/61][1960] Loss_D: -1.107597 Loss_G: -0.309147 Loss_D_real: 0.187839 Loss_D_fake 1.295437\n",
      "[176/10000][55/61][1980] Loss_D: -1.102861 Loss_G: 0.611427 Loss_D_real: -0.770904 Loss_D_fake 0.331957\n",
      "[178/10000][25/61][2000] Loss_D: -0.969259 Loss_G: -0.297505 Loss_D_real: -0.003119 Loss_D_fake 0.966140\n",
      "[180/10000][30/61][2020] Loss_D: -0.916877 Loss_G: -0.026040 Loss_D_real: -0.133241 Loss_D_fake 0.783636\n",
      "[181/10000][61/61][2040] Loss_D: -0.591533 Loss_G: -0.435603 Loss_D_real: 0.152100 Loss_D_fake 0.743633\n",
      "[183/10000][35/61][2060] Loss_D: -1.147372 Loss_G: 0.426491 Loss_D_real: -0.700257 Loss_D_fake 0.447115\n",
      "[185/10000][5/61][2080] Loss_D: -0.985757 Loss_G: 0.710641 Loss_D_real: -0.455195 Loss_D_fake 0.530562\n",
      "[186/10000][40/61][2100] Loss_D: -0.863271 Loss_G: 0.861313 Loss_D_real: -0.597263 Loss_D_fake 0.266008\n",
      "[188/10000][10/61][2120] Loss_D: -1.036738 Loss_G: 0.879130 Loss_D_real: -0.981243 Loss_D_fake 0.055495\n",
      "[189/10000][45/61][2140] Loss_D: -1.013300 Loss_G: 0.083756 Loss_D_real: -0.188266 Loss_D_fake 0.825034\n",
      "[191/10000][15/61][2160] Loss_D: -0.991725 Loss_G: 1.068767 Loss_D_real: -0.936534 Loss_D_fake 0.055192\n",
      "[192/10000][50/61][2180] Loss_D: -1.076872 Loss_G: 0.457362 Loss_D_real: -0.894717 Loss_D_fake 0.182155\n",
      "[194/10000][20/61][2200] Loss_D: -1.059180 Loss_G: -0.291362 Loss_D_real: 0.003953 Loss_D_fake 1.063133\n",
      "[195/10000][55/61][2220] Loss_D: -1.020507 Loss_G: 0.006984 Loss_D_real: -0.437413 Loss_D_fake 0.583094\n",
      "[197/10000][25/61][2240] Loss_D: -0.718716 Loss_G: 0.348161 Loss_D_real: 0.154538 Loss_D_fake 0.873254\n",
      "[198/10000][60/61][2260] Loss_D: -1.031019 Loss_G: 0.940764 Loss_D_real: -0.785930 Loss_D_fake 0.245089\n",
      "[200/10000][30/61][2280] Loss_D: -0.870558 Loss_G: 0.911257 Loss_D_real: -1.186856 Loss_D_fake -0.316298\n",
      "[201/10000][61/61][2300] Loss_D: -0.075379 Loss_G: -0.030335 Loss_D_real: 0.364433 Loss_D_fake 0.439812\n",
      "[203/10000][35/61][2320] Loss_D: -1.088083 Loss_G: 0.843405 Loss_D_real: -0.903521 Loss_D_fake 0.184562\n",
      "[205/10000][5/61][2340] Loss_D: -0.965963 Loss_G: 0.241305 Loss_D_real: -0.449491 Loss_D_fake 0.516471\n",
      "[206/10000][40/61][2360] Loss_D: -0.837383 Loss_G: 0.029534 Loss_D_real: 0.265182 Loss_D_fake 1.102565\n",
      "[208/10000][10/61][2380] Loss_D: -1.005170 Loss_G: 0.588997 Loss_D_real: -0.455442 Loss_D_fake 0.549728\n",
      "[209/10000][45/61][2400] Loss_D: -0.962300 Loss_G: 0.517847 Loss_D_real: -0.564985 Loss_D_fake 0.397315\n",
      "[211/10000][15/61][2420] Loss_D: -0.998912 Loss_G: 1.182045 Loss_D_real: -1.257343 Loss_D_fake -0.258431\n",
      "[212/10000][50/61][2440] Loss_D: -1.004240 Loss_G: 0.199670 Loss_D_real: -0.476530 Loss_D_fake 0.527710\n",
      "[214/10000][20/61][2460] Loss_D: -1.067880 Loss_G: 1.071024 Loss_D_real: -0.903484 Loss_D_fake 0.164396\n",
      "[215/10000][55/61][2480] Loss_D: -0.952815 Loss_G: 0.762023 Loss_D_real: -0.400931 Loss_D_fake 0.551884\n",
      "[217/10000][25/61][2500] Loss_D: -1.003992 Loss_G: -0.053276 Loss_D_real: -0.131136 Loss_D_fake 0.872856\n",
      "[219/10000][30/61][2520] Loss_D: -0.959251 Loss_G: 0.736664 Loss_D_real: -0.596527 Loss_D_fake 0.362724\n",
      "[220/10000][61/61][2540] Loss_D: -0.580569 Loss_G: -0.191349 Loss_D_real: 0.319398 Loss_D_fake 0.899967\n",
      "[222/10000][35/61][2560] Loss_D: -0.860176 Loss_G: -0.266547 Loss_D_real: 0.159517 Loss_D_fake 1.019693\n",
      "[224/10000][5/61][2580] Loss_D: -1.068288 Loss_G: 0.719240 Loss_D_real: -0.469834 Loss_D_fake 0.598454\n",
      "[225/10000][40/61][2600] Loss_D: -0.952012 Loss_G: 0.874473 Loss_D_real: -0.446394 Loss_D_fake 0.505618\n",
      "[227/10000][10/61][2620] Loss_D: -0.891141 Loss_G: 0.342931 Loss_D_real: -0.593637 Loss_D_fake 0.297504\n",
      "[228/10000][45/61][2640] Loss_D: -1.143136 Loss_G: 1.242360 Loss_D_real: -0.853632 Loss_D_fake 0.289504\n",
      "[230/10000][15/61][2660] Loss_D: -0.983540 Loss_G: 0.720520 Loss_D_real: -0.871827 Loss_D_fake 0.111713\n",
      "[231/10000][50/61][2680] Loss_D: -1.016264 Loss_G: 0.943982 Loss_D_real: -0.892382 Loss_D_fake 0.123882\n",
      "[233/10000][20/61][2700] Loss_D: -1.111189 Loss_G: 0.368036 Loss_D_real: -0.503964 Loss_D_fake 0.607225\n",
      "[234/10000][55/61][2720] Loss_D: -0.958941 Loss_G: 0.048869 Loss_D_real: -0.505294 Loss_D_fake 0.453647\n",
      "[236/10000][25/61][2740] Loss_D: -0.882602 Loss_G: -0.022839 Loss_D_real: 0.131503 Loss_D_fake 1.014105\n",
      "[237/10000][60/61][2760] Loss_D: -0.852908 Loss_G: 0.930521 Loss_D_real: -0.907810 Loss_D_fake -0.054902\n",
      "[239/10000][30/61][2780] Loss_D: -1.103875 Loss_G: 0.889567 Loss_D_real: -0.598876 Loss_D_fake 0.504999\n",
      "[240/10000][61/61][2800] Loss_D: -0.483603 Loss_G: -0.275764 Loss_D_real: -0.743820 Loss_D_fake -0.260217\n",
      "[242/10000][35/61][2820] Loss_D: -1.008020 Loss_G: 0.031794 Loss_D_real: 0.099705 Loss_D_fake 1.107725\n",
      "[244/10000][5/61][2840] Loss_D: -0.999534 Loss_G: 1.104581 Loss_D_real: -0.890213 Loss_D_fake 0.109320\n",
      "[245/10000][40/61][2860] Loss_D: -0.972595 Loss_G: -0.132708 Loss_D_real: -0.064915 Loss_D_fake 0.907681\n",
      "[247/10000][10/61][2880] Loss_D: -0.920018 Loss_G: 0.123082 Loss_D_real: 0.106177 Loss_D_fake 1.026196\n",
      "[248/10000][45/61][2900] Loss_D: -0.678742 Loss_G: -0.444913 Loss_D_real: 0.347326 Loss_D_fake 1.026068\n",
      "[250/10000][15/61][2920] Loss_D: -0.922585 Loss_G: -0.323530 Loss_D_real: 0.151238 Loss_D_fake 1.073823\n",
      "[251/10000][50/61][2940] Loss_D: -0.914424 Loss_G: -0.137207 Loss_D_real: -0.132555 Loss_D_fake 0.781869\n",
      "[253/10000][20/61][2960] Loss_D: -0.915471 Loss_G: 0.009881 Loss_D_real: -0.048926 Loss_D_fake 0.866546\n",
      "[254/10000][55/61][2980] Loss_D: -0.843476 Loss_G: -0.197360 Loss_D_real: 0.563419 Loss_D_fake 1.406895\n",
      "[256/10000][25/61][3000] Loss_D: -1.034731 Loss_G: 0.617096 Loss_D_real: -0.605330 Loss_D_fake 0.429401\n",
      "[258/10000][30/61][3020] Loss_D: -0.890151 Loss_G: -0.042754 Loss_D_real: -0.179461 Loss_D_fake 0.710690\n",
      "[259/10000][61/61][3040] Loss_D: -1.179437 Loss_G: 1.613108 Loss_D_real: -0.285214 Loss_D_fake 0.894223\n",
      "[261/10000][35/61][3060] Loss_D: -0.975810 Loss_G: -0.000128 Loss_D_real: 0.531858 Loss_D_fake 1.507667\n",
      "[263/10000][5/61][3080] Loss_D: -0.877910 Loss_G: 0.406281 Loss_D_real: -0.435830 Loss_D_fake 0.442081\n",
      "[264/10000][40/61][3100] Loss_D: -1.046020 Loss_G: 0.014436 Loss_D_real: -0.105327 Loss_D_fake 0.940693\n",
      "[266/10000][10/61][3120] Loss_D: -0.996426 Loss_G: 0.190041 Loss_D_real: -0.262583 Loss_D_fake 0.733842\n",
      "[267/10000][45/61][3140] Loss_D: -0.922510 Loss_G: 0.426233 Loss_D_real: -0.121823 Loss_D_fake 0.800687\n",
      "[269/10000][15/61][3160] Loss_D: -0.917509 Loss_G: 0.047310 Loss_D_real: -0.371260 Loss_D_fake 0.546249\n",
      "[270/10000][50/61][3180] Loss_D: -0.997025 Loss_G: -0.172677 Loss_D_real: 0.011694 Loss_D_fake 1.008719\n",
      "[272/10000][20/61][3200] Loss_D: -0.757889 Loss_G: 1.277903 Loss_D_real: -1.298590 Loss_D_fake -0.540701\n",
      "[273/10000][55/61][3220] Loss_D: -0.889443 Loss_G: -0.437389 Loss_D_real: 0.306843 Loss_D_fake 1.196287\n",
      "[275/10000][25/61][3240] Loss_D: -1.017153 Loss_G: 0.017006 Loss_D_real: 0.000015 Loss_D_fake 1.017167\n",
      "[276/10000][60/61][3260] Loss_D: -0.973694 Loss_G: 0.405113 Loss_D_real: -0.582942 Loss_D_fake 0.390752\n",
      "[278/10000][30/61][3280] Loss_D: -1.109524 Loss_G: 1.108822 Loss_D_real: -1.071707 Loss_D_fake 0.037817\n",
      "[279/10000][61/61][3300] Loss_D: -0.010369 Loss_G: -0.470961 Loss_D_real: -0.067182 Loss_D_fake -0.056813\n",
      "[281/10000][35/61][3320] Loss_D: -1.053917 Loss_G: -0.056476 Loss_D_real: -0.160047 Loss_D_fake 0.893870\n",
      "[283/10000][5/61][3340] Loss_D: -0.999717 Loss_G: 0.118563 Loss_D_real: -0.476940 Loss_D_fake 0.522777\n",
      "[284/10000][40/61][3360] Loss_D: -0.961885 Loss_G: 1.241081 Loss_D_real: -1.089971 Loss_D_fake -0.128086\n",
      "[286/10000][10/61][3380] Loss_D: -1.073678 Loss_G: 0.628944 Loss_D_real: -0.641437 Loss_D_fake 0.432241\n",
      "[287/10000][45/61][3400] Loss_D: -1.050513 Loss_G: 0.819214 Loss_D_real: -0.675676 Loss_D_fake 0.374837\n",
      "[289/10000][15/61][3420] Loss_D: -0.819658 Loss_G: -0.291889 Loss_D_real: -0.011355 Loss_D_fake 0.808303\n",
      "[290/10000][50/61][3440] Loss_D: -0.849624 Loss_G: 0.068216 Loss_D_real: -0.543643 Loss_D_fake 0.305982\n",
      "[292/10000][20/61][3460] Loss_D: -1.091220 Loss_G: 1.053814 Loss_D_real: -0.932519 Loss_D_fake 0.158701\n",
      "[293/10000][55/61][3480] Loss_D: -1.018027 Loss_G: 0.710736 Loss_D_real: -0.788755 Loss_D_fake 0.229272\n",
      "[295/10000][25/61][3500] Loss_D: -0.875100 Loss_G: 0.939832 Loss_D_real: -0.445415 Loss_D_fake 0.429685\n",
      "[297/10000][30/61][3520] Loss_D: -0.913148 Loss_G: 0.590115 Loss_D_real: -0.540468 Loss_D_fake 0.372680\n",
      "[298/10000][61/61][3540] Loss_D: -1.281949 Loss_G: 0.764298 Loss_D_real: -1.698364 Loss_D_fake -0.416415\n",
      "[300/10000][35/61][3560] Loss_D: -0.902846 Loss_G: 0.648518 Loss_D_real: -0.758876 Loss_D_fake 0.143971\n",
      "[302/10000][5/61][3580] Loss_D: -0.990937 Loss_G: 0.648668 Loss_D_real: -0.711646 Loss_D_fake 0.279291\n",
      "[303/10000][40/61][3600] Loss_D: -0.978207 Loss_G: -0.332048 Loss_D_real: -0.216843 Loss_D_fake 0.761364\n",
      "[305/10000][10/61][3620] Loss_D: -1.085858 Loss_G: 0.173342 Loss_D_real: -0.223552 Loss_D_fake 0.862305\n",
      "[306/10000][45/61][3640] Loss_D: -0.994277 Loss_G: 0.592095 Loss_D_real: -0.465569 Loss_D_fake 0.528708\n",
      "[308/10000][15/61][3660] Loss_D: -1.024901 Loss_G: 1.299884 Loss_D_real: -1.209730 Loss_D_fake -0.184829\n",
      "[309/10000][50/61][3680] Loss_D: -0.757937 Loss_G: 0.973783 Loss_D_real: -0.985225 Loss_D_fake -0.227288\n",
      "[311/10000][20/61][3700] Loss_D: -1.059111 Loss_G: 0.723672 Loss_D_real: -0.622080 Loss_D_fake 0.437031\n",
      "[312/10000][55/61][3720] Loss_D: -0.949659 Loss_G: 1.157613 Loss_D_real: -0.810156 Loss_D_fake 0.139502\n",
      "[314/10000][25/61][3740] Loss_D: -0.840469 Loss_G: 0.823443 Loss_D_real: -0.835535 Loss_D_fake 0.004934\n",
      "[315/10000][60/61][3760] Loss_D: -0.965968 Loss_G: 0.492101 Loss_D_real: -0.519634 Loss_D_fake 0.446334\n",
      "[317/10000][30/61][3780] Loss_D: -0.972340 Loss_G: 0.792784 Loss_D_real: -0.897346 Loss_D_fake 0.074993\n",
      "[318/10000][61/61][3800] Loss_D: -1.339131 Loss_G: 0.173096 Loss_D_real: -1.331751 Loss_D_fake 0.007380\n",
      "[320/10000][35/61][3820] Loss_D: -1.087678 Loss_G: 0.853844 Loss_D_real: -0.809290 Loss_D_fake 0.278388\n",
      "[322/10000][5/61][3840] Loss_D: -0.599393 Loss_G: 0.290553 Loss_D_real: -0.124658 Loss_D_fake 0.474735\n",
      "[323/10000][40/61][3860] Loss_D: -0.849395 Loss_G: 1.074010 Loss_D_real: -0.803237 Loss_D_fake 0.046158\n",
      "[325/10000][10/61][3880] Loss_D: -0.889445 Loss_G: 0.263829 Loss_D_real: -0.358811 Loss_D_fake 0.530635\n",
      "[326/10000][45/61][3900] Loss_D: -0.999834 Loss_G: 0.598509 Loss_D_real: -0.357232 Loss_D_fake 0.642602\n",
      "[328/10000][15/61][3920] Loss_D: -0.963587 Loss_G: 0.267550 Loss_D_real: -0.198067 Loss_D_fake 0.765519\n",
      "[329/10000][50/61][3940] Loss_D: -1.082981 Loss_G: 0.516443 Loss_D_real: -0.414967 Loss_D_fake 0.668014\n",
      "[331/10000][20/61][3960] Loss_D: -0.890184 Loss_G: -0.544631 Loss_D_real: 0.076466 Loss_D_fake 0.966651\n",
      "[332/10000][55/61][3980] Loss_D: -0.826577 Loss_G: 0.541864 Loss_D_real: -0.537931 Loss_D_fake 0.288646\n",
      "[334/10000][25/61][4000] Loss_D: -0.810856 Loss_G: 0.799375 Loss_D_real: -1.248687 Loss_D_fake -0.437831\n",
      "[336/10000][30/61][4020] Loss_D: -0.918273 Loss_G: 1.113847 Loss_D_real: -0.923989 Loss_D_fake -0.005716\n",
      "[337/10000][61/61][4040] Loss_D: 0.071541 Loss_G: -0.109574 Loss_D_real: 0.073029 Loss_D_fake 0.001487\n",
      "[339/10000][35/61][4060] Loss_D: -0.914885 Loss_G: 0.144796 Loss_D_real: -0.016135 Loss_D_fake 0.898750\n",
      "[341/10000][5/61][4080] Loss_D: -0.757534 Loss_G: 0.078882 Loss_D_real: -0.241364 Loss_D_fake 0.516170\n",
      "[342/10000][40/61][4100] Loss_D: -0.977427 Loss_G: 0.578437 Loss_D_real: -0.340100 Loss_D_fake 0.637328\n",
      "[344/10000][10/61][4120] Loss_D: -0.853813 Loss_G: 0.994723 Loss_D_real: -0.867752 Loss_D_fake -0.013939\n",
      "[345/10000][45/61][4140] Loss_D: -0.804119 Loss_G: 0.070465 Loss_D_real: -0.086704 Loss_D_fake 0.717414\n",
      "[347/10000][15/61][4160] Loss_D: -1.036475 Loss_G: 0.998337 Loss_D_real: -1.104563 Loss_D_fake -0.068088\n",
      "[348/10000][50/61][4180] Loss_D: -0.838390 Loss_G: 0.890751 Loss_D_real: -0.732382 Loss_D_fake 0.106008\n",
      "[350/10000][20/61][4200] Loss_D: -1.078383 Loss_G: 1.188583 Loss_D_real: -1.051343 Loss_D_fake 0.027040\n",
      "[351/10000][55/61][4220] Loss_D: -0.859934 Loss_G: -0.285129 Loss_D_real: 0.027692 Loss_D_fake 0.887626\n",
      "[353/10000][25/61][4240] Loss_D: -0.946072 Loss_G: 0.587798 Loss_D_real: -0.538002 Loss_D_fake 0.408071\n",
      "[354/10000][60/61][4260] Loss_D: -0.987375 Loss_G: 0.357070 Loss_D_real: -0.084365 Loss_D_fake 0.903010\n",
      "[356/10000][30/61][4280] Loss_D: -0.919883 Loss_G: 0.344688 Loss_D_real: -0.367645 Loss_D_fake 0.552238\n",
      "[357/10000][61/61][4300] Loss_D: -0.414129 Loss_G: -1.643971 Loss_D_real: 0.665216 Loss_D_fake 1.079346\n",
      "[359/10000][35/61][4320] Loss_D: -0.896062 Loss_G: 0.571316 Loss_D_real: -0.789980 Loss_D_fake 0.106083\n",
      "[361/10000][5/61][4340] Loss_D: -0.826550 Loss_G: 0.461909 Loss_D_real: -0.602675 Loss_D_fake 0.223875\n",
      "[362/10000][40/61][4360] Loss_D: -0.965495 Loss_G: 0.690207 Loss_D_real: -0.854528 Loss_D_fake 0.110967\n",
      "[364/10000][10/61][4380] Loss_D: -0.935454 Loss_G: 0.027482 Loss_D_real: -0.066687 Loss_D_fake 0.868767\n",
      "[365/10000][45/61][4400] Loss_D: -0.939951 Loss_G: 0.770732 Loss_D_real: -1.095710 Loss_D_fake -0.155759\n",
      "[367/10000][15/61][4420] Loss_D: -0.808223 Loss_G: 0.417981 Loss_D_real: -0.264467 Loss_D_fake 0.543756\n",
      "[368/10000][50/61][4440] Loss_D: -0.726010 Loss_G: 0.272126 Loss_D_real: 0.112577 Loss_D_fake 0.838586\n",
      "[370/10000][20/61][4460] Loss_D: -0.804764 Loss_G: 0.015605 Loss_D_real: -0.264751 Loss_D_fake 0.540013\n",
      "[371/10000][55/61][4480] Loss_D: -0.952720 Loss_G: 0.366446 Loss_D_real: -0.162309 Loss_D_fake 0.790411\n",
      "[373/10000][25/61][4500] Loss_D: -1.105203 Loss_G: 0.751666 Loss_D_real: -0.565549 Loss_D_fake 0.539654\n",
      "[375/10000][30/61][4520] Loss_D: -0.904048 Loss_G: 0.808662 Loss_D_real: -1.066863 Loss_D_fake -0.162816\n",
      "[376/10000][61/61][4540] Loss_D: -0.307085 Loss_G: -1.009610 Loss_D_real: -0.501427 Loss_D_fake -0.194342\n",
      "[378/10000][35/61][4560] Loss_D: -0.765409 Loss_G: 0.694552 Loss_D_real: -0.508487 Loss_D_fake 0.256922\n",
      "[380/10000][5/61][4580] Loss_D: -0.750187 Loss_G: 0.701332 Loss_D_real: -0.285976 Loss_D_fake 0.464211\n",
      "[381/10000][40/61][4600] Loss_D: -0.884045 Loss_G: 0.197471 Loss_D_real: 0.028919 Loss_D_fake 0.912964\n",
      "[383/10000][10/61][4620] Loss_D: -0.860957 Loss_G: 0.160220 Loss_D_real: -0.259319 Loss_D_fake 0.601638\n",
      "[384/10000][45/61][4640] Loss_D: -0.916368 Loss_G: 0.628248 Loss_D_real: -0.585434 Loss_D_fake 0.330935\n",
      "[386/10000][15/61][4660] Loss_D: -0.909045 Loss_G: 0.074707 Loss_D_real: -0.591914 Loss_D_fake 0.317132\n",
      "[387/10000][50/61][4680] Loss_D: -0.968450 Loss_G: 0.070392 Loss_D_real: -0.385822 Loss_D_fake 0.582628\n",
      "[389/10000][20/61][4700] Loss_D: -0.935285 Loss_G: 1.023235 Loss_D_real: -1.311748 Loss_D_fake -0.376463\n",
      "[390/10000][55/61][4720] Loss_D: -0.674688 Loss_G: 1.193990 Loss_D_real: -1.131795 Loss_D_fake -0.457107\n",
      "[392/10000][25/61][4740] Loss_D: -0.993386 Loss_G: 0.060827 Loss_D_real: -0.475871 Loss_D_fake 0.517516\n",
      "[393/10000][60/61][4760] Loss_D: -0.923593 Loss_G: 0.449835 Loss_D_real: -0.343843 Loss_D_fake 0.579750\n",
      "[395/10000][30/61][4780] Loss_D: -0.847415 Loss_G: -0.005961 Loss_D_real: 0.322461 Loss_D_fake 1.169875\n",
      "[396/10000][61/61][4800] Loss_D: -0.469951 Loss_G: -0.136770 Loss_D_real: -0.227009 Loss_D_fake 0.242942\n",
      "[398/10000][35/61][4820] Loss_D: -1.035342 Loss_G: 0.025594 Loss_D_real: -0.213261 Loss_D_fake 0.822081\n",
      "[400/10000][5/61][4840] Loss_D: -0.884136 Loss_G: 0.522484 Loss_D_real: -0.375033 Loss_D_fake 0.509102\n",
      "[401/10000][40/61][4860] Loss_D: -1.025957 Loss_G: -0.049471 Loss_D_real: 0.129419 Loss_D_fake 1.155376\n",
      "[403/10000][10/61][4880] Loss_D: -0.856063 Loss_G: 0.466123 Loss_D_real: -0.884443 Loss_D_fake -0.028380\n",
      "[404/10000][45/61][4900] Loss_D: -0.919913 Loss_G: 0.373586 Loss_D_real: -0.299553 Loss_D_fake 0.620361\n",
      "[406/10000][15/61][4920] Loss_D: -0.774210 Loss_G: -0.211333 Loss_D_real: -0.067249 Loss_D_fake 0.706961\n",
      "[407/10000][50/61][4940] Loss_D: -0.905227 Loss_G: 0.193282 Loss_D_real: -0.653767 Loss_D_fake 0.251460\n",
      "[409/10000][20/61][4960] Loss_D: -0.992651 Loss_G: 0.460764 Loss_D_real: -0.720924 Loss_D_fake 0.271727\n",
      "[410/10000][55/61][4980] Loss_D: -0.803132 Loss_G: 0.697323 Loss_D_real: -0.328468 Loss_D_fake 0.474664\n",
      "[412/10000][25/61][5000] Loss_D: -0.884620 Loss_G: 0.227381 Loss_D_real: -0.259578 Loss_D_fake 0.625042\n",
      "[414/10000][30/61][5020] Loss_D: -0.915429 Loss_G: 0.462392 Loss_D_real: -0.810351 Loss_D_fake 0.105078\n",
      "[415/10000][61/61][5040] Loss_D: -0.428949 Loss_G: 0.779124 Loss_D_real: 0.109223 Loss_D_fake 0.538172\n",
      "[417/10000][35/61][5060] Loss_D: -0.810347 Loss_G: -0.099435 Loss_D_real: 0.105060 Loss_D_fake 0.915407\n",
      "[419/10000][5/61][5080] Loss_D: -0.605217 Loss_G: 0.157138 Loss_D_real: -0.234466 Loss_D_fake 0.370751\n",
      "[420/10000][40/61][5100] Loss_D: -0.833214 Loss_G: 0.145998 Loss_D_real: -0.159609 Loss_D_fake 0.673604\n",
      "[422/10000][10/61][5120] Loss_D: -0.861986 Loss_G: 0.498851 Loss_D_real: -0.356710 Loss_D_fake 0.505276\n",
      "[423/10000][45/61][5140] Loss_D: -0.782717 Loss_G: 0.163377 Loss_D_real: -0.268712 Loss_D_fake 0.514005\n",
      "[425/10000][15/61][5160] Loss_D: -0.857372 Loss_G: 0.858654 Loss_D_real: -0.555210 Loss_D_fake 0.302162\n",
      "[426/10000][50/61][5180] Loss_D: -0.756720 Loss_G: 1.101885 Loss_D_real: -0.498743 Loss_D_fake 0.257978\n",
      "[428/10000][20/61][5200] Loss_D: -0.837554 Loss_G: -0.020257 Loss_D_real: -0.506519 Loss_D_fake 0.331036\n",
      "[429/10000][55/61][5220] Loss_D: -0.760837 Loss_G: 0.734293 Loss_D_real: -0.867821 Loss_D_fake -0.106984\n",
      "[431/10000][25/61][5240] Loss_D: -0.839313 Loss_G: 1.187314 Loss_D_real: -0.706613 Loss_D_fake 0.132700\n",
      "[432/10000][60/61][5260] Loss_D: -0.697331 Loss_G: -0.438396 Loss_D_real: 0.444476 Loss_D_fake 1.141807\n",
      "[434/10000][30/61][5280] Loss_D: -0.732875 Loss_G: 0.200883 Loss_D_real: -0.116168 Loss_D_fake 0.616707\n",
      "[435/10000][61/61][5300] Loss_D: -1.076277 Loss_G: 0.890881 Loss_D_real: -0.315481 Loss_D_fake 0.760796\n",
      "[437/10000][35/61][5320] Loss_D: -0.823189 Loss_G: 0.710207 Loss_D_real: -0.894476 Loss_D_fake -0.071288\n",
      "[439/10000][5/61][5340] Loss_D: -0.834563 Loss_G: 0.013093 Loss_D_real: -0.016947 Loss_D_fake 0.817616\n",
      "[440/10000][40/61][5360] Loss_D: -0.751949 Loss_G: -0.280040 Loss_D_real: 0.240065 Loss_D_fake 0.992014\n",
      "[442/10000][10/61][5380] Loss_D: -0.834240 Loss_G: -0.035932 Loss_D_real: -0.513423 Loss_D_fake 0.320818\n",
      "[443/10000][45/61][5400] Loss_D: -0.939438 Loss_G: -0.079250 Loss_D_real: -0.573319 Loss_D_fake 0.366120\n",
      "[445/10000][15/61][5420] Loss_D: -0.875746 Loss_G: 0.951997 Loss_D_real: -0.866507 Loss_D_fake 0.009238\n",
      "[446/10000][50/61][5440] Loss_D: -0.716129 Loss_G: 0.917911 Loss_D_real: -0.475107 Loss_D_fake 0.241022\n",
      "[448/10000][20/61][5460] Loss_D: -0.763315 Loss_G: 0.707363 Loss_D_real: -1.041948 Loss_D_fake -0.278633\n",
      "[449/10000][55/61][5480] Loss_D: -0.998429 Loss_G: -0.120131 Loss_D_real: -0.412892 Loss_D_fake 0.585537\n",
      "[451/10000][25/61][5500] Loss_D: -0.838458 Loss_G: 0.582880 Loss_D_real: -0.625962 Loss_D_fake 0.212496\n",
      "[453/10000][30/61][5520] Loss_D: -0.807060 Loss_G: 0.670492 Loss_D_real: -0.745518 Loss_D_fake 0.061542\n",
      "[454/10000][61/61][5540] Loss_D: -0.126053 Loss_G: -1.671775 Loss_D_real: 0.038374 Loss_D_fake 0.164427\n",
      "[456/10000][35/61][5560] Loss_D: -0.731204 Loss_G: 0.336781 Loss_D_real: -0.393318 Loss_D_fake 0.337886\n",
      "[458/10000][5/61][5580] Loss_D: -0.769196 Loss_G: 0.021454 Loss_D_real: 0.045850 Loss_D_fake 0.815046\n",
      "[459/10000][40/61][5600] Loss_D: -0.796630 Loss_G: 0.797947 Loss_D_real: -0.590605 Loss_D_fake 0.206025\n",
      "[461/10000][10/61][5620] Loss_D: -0.489980 Loss_G: -0.419574 Loss_D_real: 0.194630 Loss_D_fake 0.684609\n",
      "[462/10000][45/61][5640] Loss_D: -0.700279 Loss_G: -0.354563 Loss_D_real: -0.180090 Loss_D_fake 0.520189\n",
      "[464/10000][15/61][5660] Loss_D: -0.830845 Loss_G: 0.350736 Loss_D_real: -0.673102 Loss_D_fake 0.157743\n",
      "[465/10000][50/61][5680] Loss_D: -0.587175 Loss_G: 0.903091 Loss_D_real: -1.164214 Loss_D_fake -0.577038\n",
      "[467/10000][20/61][5700] Loss_D: -0.760661 Loss_G: 0.798943 Loss_D_real: -1.125629 Loss_D_fake -0.364968\n",
      "[468/10000][55/61][5720] Loss_D: -0.662376 Loss_G: 0.737636 Loss_D_real: -0.793610 Loss_D_fake -0.131233\n",
      "[470/10000][25/61][5740] Loss_D: -0.896132 Loss_G: 0.319744 Loss_D_real: -0.342759 Loss_D_fake 0.553373\n",
      "[471/10000][60/61][5760] Loss_D: -0.830614 Loss_G: 0.382533 Loss_D_real: -0.669036 Loss_D_fake 0.161578\n",
      "[473/10000][30/61][5780] Loss_D: -0.804640 Loss_G: 0.516819 Loss_D_real: -0.734578 Loss_D_fake 0.070062\n",
      "[474/10000][61/61][5800] Loss_D: -0.236688 Loss_G: -0.212056 Loss_D_real: 0.380473 Loss_D_fake 0.617161\n",
      "[476/10000][35/61][5820] Loss_D: -0.764162 Loss_G: 0.238819 Loss_D_real: 0.381172 Loss_D_fake 1.145333\n",
      "[478/10000][5/61][5840] Loss_D: -0.642033 Loss_G: 0.202589 Loss_D_real: -0.242572 Loss_D_fake 0.399461\n",
      "[479/10000][40/61][5860] Loss_D: -0.737034 Loss_G: 0.556428 Loss_D_real: -0.711886 Loss_D_fake 0.025148\n",
      "[481/10000][10/61][5880] Loss_D: -0.694213 Loss_G: 0.419371 Loss_D_real: 0.581184 Loss_D_fake 1.275396\n",
      "[482/10000][45/61][5900] Loss_D: -0.757063 Loss_G: 0.425730 Loss_D_real: -0.297600 Loss_D_fake 0.459463\n",
      "[484/10000][15/61][5920] Loss_D: -0.659884 Loss_G: 0.728994 Loss_D_real: -0.538684 Loss_D_fake 0.121200\n",
      "[485/10000][50/61][5940] Loss_D: -0.672788 Loss_G: -0.100681 Loss_D_real: -0.317331 Loss_D_fake 0.355457\n",
      "[487/10000][20/61][5960] Loss_D: -0.640938 Loss_G: -0.101078 Loss_D_real: 0.021053 Loss_D_fake 0.661991\n",
      "[488/10000][55/61][5980] Loss_D: -0.934714 Loss_G: 0.795326 Loss_D_real: -0.646278 Loss_D_fake 0.288436\n",
      "[490/10000][25/61][6000] Loss_D: -0.840037 Loss_G: 0.460427 Loss_D_real: -1.024190 Loss_D_fake -0.184153\n",
      "[492/10000][30/61][6020] Loss_D: -0.854035 Loss_G: 0.992883 Loss_D_real: -0.650666 Loss_D_fake 0.203369\n",
      "[493/10000][61/61][6040] Loss_D: -1.288970 Loss_G: 1.312906 Loss_D_real: -1.825555 Loss_D_fake -0.536585\n",
      "[495/10000][35/61][6060] Loss_D: -0.752549 Loss_G: 0.111560 Loss_D_real: 0.082292 Loss_D_fake 0.834841\n",
      "[497/10000][5/61][6080] Loss_D: -0.372043 Loss_G: 0.818285 Loss_D_real: -0.098841 Loss_D_fake 0.273203\n",
      "[498/10000][40/61][6100] Loss_D: -0.799663 Loss_G: 0.788880 Loss_D_real: -1.223425 Loss_D_fake -0.423762\n",
      "[500/10000][10/61][6120] Loss_D: -0.805938 Loss_G: 0.536435 Loss_D_real: -0.484682 Loss_D_fake 0.321256\n",
      "[501/10000][45/61][6140] Loss_D: -0.697327 Loss_G: -0.245355 Loss_D_real: 0.124158 Loss_D_fake 0.821485\n",
      "[503/10000][15/61][6160] Loss_D: -0.624912 Loss_G: -0.202924 Loss_D_real: 0.326709 Loss_D_fake 0.951621\n",
      "[504/10000][50/61][6180] Loss_D: -0.660480 Loss_G: 0.272340 Loss_D_real: -0.934251 Loss_D_fake -0.273771\n",
      "[506/10000][20/61][6200] Loss_D: -0.733995 Loss_G: -0.156250 Loss_D_real: -0.062217 Loss_D_fake 0.671778\n",
      "[507/10000][55/61][6220] Loss_D: -0.648164 Loss_G: 0.148953 Loss_D_real: -0.116875 Loss_D_fake 0.531289\n",
      "[509/10000][25/61][6240] Loss_D: -0.552310 Loss_G: -0.116636 Loss_D_real: -0.515341 Loss_D_fake 0.036970\n",
      "[510/10000][60/61][6260] Loss_D: -0.725008 Loss_G: 0.105011 Loss_D_real: -0.629925 Loss_D_fake 0.095083\n",
      "[512/10000][30/61][6280] Loss_D: -0.783172 Loss_G: 0.568548 Loss_D_real: -0.804609 Loss_D_fake -0.021436\n",
      "[513/10000][61/61][6300] Loss_D: -1.152253 Loss_G: -0.618711 Loss_D_real: -1.183389 Loss_D_fake -0.031136\n",
      "[515/10000][35/61][6320] Loss_D: -0.693085 Loss_G: 0.380683 Loss_D_real: 0.203609 Loss_D_fake 0.896694\n",
      "[517/10000][5/61][6340] Loss_D: -0.646108 Loss_G: 0.492399 Loss_D_real: -0.292525 Loss_D_fake 0.353584\n",
      "[518/10000][40/61][6360] Loss_D: -0.821238 Loss_G: 0.162474 Loss_D_real: -0.076385 Loss_D_fake 0.744853\n",
      "[520/10000][10/61][6380] Loss_D: -0.667503 Loss_G: 0.243615 Loss_D_real: -0.565843 Loss_D_fake 0.101660\n",
      "[521/10000][45/61][6400] Loss_D: -0.688820 Loss_G: 0.257046 Loss_D_real: -0.550073 Loss_D_fake 0.138748\n",
      "[523/10000][15/61][6420] Loss_D: -0.636504 Loss_G: 0.593687 Loss_D_real: -0.952971 Loss_D_fake -0.316467\n",
      "[524/10000][50/61][6440] Loss_D: -0.762611 Loss_G: 1.037966 Loss_D_real: -1.072710 Loss_D_fake -0.310099\n",
      "[526/10000][20/61][6460] Loss_D: -0.823869 Loss_G: 0.504229 Loss_D_real: -0.501133 Loss_D_fake 0.322736\n",
      "[527/10000][55/61][6480] Loss_D: -0.640735 Loss_G: 0.494857 Loss_D_real: -0.559467 Loss_D_fake 0.081268\n",
      "[529/10000][25/61][6500] Loss_D: -0.740988 Loss_G: 0.916221 Loss_D_real: -0.829674 Loss_D_fake -0.088686\n",
      "[531/10000][30/61][6520] Loss_D: -0.722422 Loss_G: 0.174004 Loss_D_real: -0.182054 Loss_D_fake 0.540368\n",
      "[532/10000][61/61][6540] Loss_D: -0.269544 Loss_G: 1.153334 Loss_D_real: 0.058455 Loss_D_fake 0.327999\n",
      "[534/10000][35/61][6560] Loss_D: -0.731871 Loss_G: 0.173228 Loss_D_real: -0.082063 Loss_D_fake 0.649807\n",
      "[536/10000][5/61][6580] Loss_D: -0.398255 Loss_G: 0.479873 Loss_D_real: -0.225910 Loss_D_fake 0.172345\n",
      "[537/10000][40/61][6600] Loss_D: -0.678687 Loss_G: -0.148769 Loss_D_real: -0.274670 Loss_D_fake 0.404017\n",
      "[539/10000][10/61][6620] Loss_D: -0.608249 Loss_G: 0.808951 Loss_D_real: -0.672296 Loss_D_fake -0.064047\n",
      "[540/10000][45/61][6640] Loss_D: -0.706935 Loss_G: 0.566240 Loss_D_real: -0.474480 Loss_D_fake 0.232455\n",
      "[542/10000][15/61][6660] Loss_D: -0.816949 Loss_G: 1.061507 Loss_D_real: -1.047248 Loss_D_fake -0.230299\n",
      "[543/10000][50/61][6680] Loss_D: -0.714517 Loss_G: 0.608582 Loss_D_real: -0.776214 Loss_D_fake -0.061697\n",
      "[545/10000][20/61][6700] Loss_D: -0.815128 Loss_G: 0.491779 Loss_D_real: -0.565719 Loss_D_fake 0.249409\n",
      "[546/10000][55/61][6720] Loss_D: -0.816406 Loss_G: 0.109410 Loss_D_real: -0.328521 Loss_D_fake 0.487885\n",
      "[548/10000][25/61][6740] Loss_D: -0.697064 Loss_G: 0.218569 Loss_D_real: -0.225029 Loss_D_fake 0.472035\n",
      "[549/10000][60/61][6760] Loss_D: -0.643467 Loss_G: 0.744714 Loss_D_real: -1.207345 Loss_D_fake -0.563879\n",
      "[551/10000][30/61][6780] Loss_D: -0.653862 Loss_G: 0.972857 Loss_D_real: -0.550722 Loss_D_fake 0.103140\n",
      "[552/10000][61/61][6800] Loss_D: -1.423420 Loss_G: 0.943543 Loss_D_real: -1.658694 Loss_D_fake -0.235274\n",
      "[554/10000][35/61][6820] Loss_D: -0.725784 Loss_G: -0.096707 Loss_D_real: -0.034088 Loss_D_fake 0.691696\n",
      "[556/10000][5/61][6840] Loss_D: -0.605518 Loss_G: 0.076766 Loss_D_real: -0.355452 Loss_D_fake 0.250066\n",
      "[557/10000][40/61][6860] Loss_D: -0.666002 Loss_G: -0.061380 Loss_D_real: 0.062734 Loss_D_fake 0.728736\n",
      "[559/10000][10/61][6880] Loss_D: -0.664939 Loss_G: 1.159831 Loss_D_real: -1.077150 Loss_D_fake -0.412211\n",
      "[560/10000][45/61][6900] Loss_D: -0.731715 Loss_G: -0.236874 Loss_D_real: -0.266494 Loss_D_fake 0.465221\n",
      "[562/10000][15/61][6920] Loss_D: -0.683666 Loss_G: 0.404023 Loss_D_real: -0.610981 Loss_D_fake 0.072685\n",
      "[563/10000][50/61][6940] Loss_D: -0.683317 Loss_G: -0.213829 Loss_D_real: 0.104947 Loss_D_fake 0.788265\n",
      "[565/10000][20/61][6960] Loss_D: -0.669508 Loss_G: 0.352868 Loss_D_real: -0.767462 Loss_D_fake -0.097955\n",
      "[566/10000][55/61][6980] Loss_D: -0.701026 Loss_G: 0.911244 Loss_D_real: -0.872976 Loss_D_fake -0.171949\n",
      "[568/10000][25/61][7000] Loss_D: -0.829682 Loss_G: 0.676070 Loss_D_real: -0.840636 Loss_D_fake -0.010954\n",
      "[570/10000][30/61][7020] Loss_D: -0.766970 Loss_G: 0.191799 Loss_D_real: -0.491766 Loss_D_fake 0.275204\n",
      "[571/10000][61/61][7040] Loss_D: -0.417936 Loss_G: -0.304740 Loss_D_real: -0.504762 Loss_D_fake -0.086826\n",
      "[573/10000][35/61][7060] Loss_D: -0.677499 Loss_G: 0.685630 Loss_D_real: -0.841941 Loss_D_fake -0.164443\n",
      "[575/10000][5/61][7080] Loss_D: -0.496625 Loss_G: 0.367028 Loss_D_real: -0.305120 Loss_D_fake 0.191506\n",
      "[576/10000][40/61][7100] Loss_D: -0.776981 Loss_G: 0.277791 Loss_D_real: -0.147971 Loss_D_fake 0.629010\n",
      "[578/10000][10/61][7120] Loss_D: -0.648536 Loss_G: 0.300947 Loss_D_real: -0.095043 Loss_D_fake 0.553492\n",
      "[579/10000][45/61][7140] Loss_D: -0.641692 Loss_G: 0.576213 Loss_D_real: -0.710033 Loss_D_fake -0.068342\n",
      "[581/10000][15/61][7160] Loss_D: -0.761201 Loss_G: 0.502124 Loss_D_real: -0.711025 Loss_D_fake 0.050177\n",
      "[582/10000][50/61][7180] Loss_D: -0.735904 Loss_G: 0.049646 Loss_D_real: -0.402956 Loss_D_fake 0.332948\n",
      "[584/10000][20/61][7200] Loss_D: -0.668846 Loss_G: -0.309701 Loss_D_real: -0.129081 Loss_D_fake 0.539765\n",
      "[585/10000][55/61][7220] Loss_D: -0.853191 Loss_G: -0.130334 Loss_D_real: -0.589731 Loss_D_fake 0.263460\n",
      "[587/10000][25/61][7240] Loss_D: -0.663085 Loss_G: 0.406177 Loss_D_real: -0.381967 Loss_D_fake 0.281118\n",
      "[588/10000][60/61][7260] Loss_D: -0.748429 Loss_G: 0.170740 Loss_D_real: -0.069215 Loss_D_fake 0.679214\n",
      "[590/10000][30/61][7280] Loss_D: -0.666363 Loss_G: 0.078660 Loss_D_real: 0.046918 Loss_D_fake 0.713280\n",
      "[591/10000][61/61][7300] Loss_D: -0.362217 Loss_G: 0.568689 Loss_D_real: -0.564268 Loss_D_fake -0.202052\n",
      "[593/10000][35/61][7320] Loss_D: -0.761510 Loss_G: 0.347017 Loss_D_real: -0.610693 Loss_D_fake 0.150817\n",
      "[595/10000][5/61][7340] Loss_D: -0.571894 Loss_G: 0.392816 Loss_D_real: -0.656248 Loss_D_fake -0.084354\n",
      "[596/10000][40/61][7360] Loss_D: -0.567827 Loss_G: 0.538555 Loss_D_real: -0.486873 Loss_D_fake 0.080953\n",
      "[598/10000][10/61][7380] Loss_D: -0.766715 Loss_G: 0.436050 Loss_D_real: -0.275872 Loss_D_fake 0.490842\n",
      "[599/10000][45/61][7400] Loss_D: -0.739982 Loss_G: 0.695714 Loss_D_real: -0.432853 Loss_D_fake 0.307129\n",
      "[601/10000][15/61][7420] Loss_D: -0.880228 Loss_G: 0.895398 Loss_D_real: -0.676607 Loss_D_fake 0.203621\n",
      "[602/10000][50/61][7440] Loss_D: -0.665674 Loss_G: -0.532623 Loss_D_real: 0.122316 Loss_D_fake 0.787989\n",
      "[604/10000][20/61][7460] Loss_D: -0.655457 Loss_G: 0.041854 Loss_D_real: -0.814797 Loss_D_fake -0.159341\n",
      "[605/10000][55/61][7480] Loss_D: -0.648714 Loss_G: 0.659769 Loss_D_real: -0.574088 Loss_D_fake 0.074626\n",
      "[607/10000][25/61][7500] Loss_D: -0.702147 Loss_G: 0.706406 Loss_D_real: -0.602883 Loss_D_fake 0.099264\n",
      "[609/10000][30/61][7520] Loss_D: -0.576729 Loss_G: 0.561261 Loss_D_real: -0.392600 Loss_D_fake 0.184130\n",
      "[610/10000][61/61][7540] Loss_D: -1.114355 Loss_G: -0.001263 Loss_D_real: -0.858947 Loss_D_fake 0.255408\n",
      "[612/10000][35/61][7560] Loss_D: -0.885635 Loss_G: -0.493579 Loss_D_real: -0.097363 Loss_D_fake 0.788272\n",
      "[614/10000][5/61][7580] Loss_D: -0.302669 Loss_G: 0.558230 Loss_D_real: 0.020582 Loss_D_fake 0.323252\n",
      "[615/10000][40/61][7600] Loss_D: -0.576900 Loss_G: 0.469092 Loss_D_real: -0.753568 Loss_D_fake -0.176668\n",
      "[617/10000][10/61][7620] Loss_D: -0.804329 Loss_G: 0.644283 Loss_D_real: -0.238448 Loss_D_fake 0.565881\n",
      "[618/10000][45/61][7640] Loss_D: -0.605669 Loss_G: -0.606149 Loss_D_real: -0.117629 Loss_D_fake 0.488040\n",
      "[620/10000][15/61][7660] Loss_D: -0.714938 Loss_G: 0.143797 Loss_D_real: -0.591780 Loss_D_fake 0.123158\n",
      "[621/10000][50/61][7680] Loss_D: -0.765609 Loss_G: 0.426180 Loss_D_real: -0.410716 Loss_D_fake 0.354893\n",
      "[623/10000][20/61][7700] Loss_D: -0.615228 Loss_G: -0.421849 Loss_D_real: -0.152195 Loss_D_fake 0.463033\n",
      "[624/10000][55/61][7720] Loss_D: -0.674985 Loss_G: 0.986319 Loss_D_real: -0.920312 Loss_D_fake -0.245326\n",
      "[626/10000][25/61][7740] Loss_D: -0.661767 Loss_G: 0.183116 Loss_D_real: -0.248323 Loss_D_fake 0.413444\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 64\n",
    "\n",
    "def bonemarrow_cell():\n",
    "    X = np.load(\"../data/data.npy\")\n",
    "    img = X\n",
    "    X = np.asarray([cv2.resize(x, (32,32)) for x in X])\n",
    "    X = np.asarray([x[:,:,::-1].transpose((2,0,1)) for x in X])\n",
    "    X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "    return X\n",
    "\n",
    "X_train = bonemarrow_cell()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr = 0.00005)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.00005)\n",
    "\n",
    "gen_iterations = 0\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "\n",
    "        # train the discriminator Diters times\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "        else:\n",
    "            Diters = 5\n",
    "        j = 0\n",
    "        while j < Diters and i < len(dataloader):\n",
    "            j += 1\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "\n",
    "            # train with real\n",
    "            real_cpu, _ = data\n",
    "            netD.zero_grad()\n",
    "            batch_size = real_cpu.size(0)\n",
    "\n",
    "            real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "\n",
    "            errD_real = netD(inputv)\n",
    "            errD_real.backward(one)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(inputv)\n",
    "            errD_fake.backward(mone)\n",
    "            errD = errD_real - errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake)\n",
    "        errG.backward(one)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'real_samples.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'fake_samples_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
