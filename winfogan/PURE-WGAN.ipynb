{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "batchsize=64\n",
    "\n",
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ndf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        output = output.mean(0)\n",
    "        return output.view(1)\n",
    "\n",
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize=32, nz=100, nc=3, ngf=64, ngpu=0, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN_G (\n",
      "  (main): Sequential (\n",
      "    (initial.100-256.convt): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "DCGAN_D (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "    (final.256-1.conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "[19/10000][61/61][20] Loss_D: -0.712746 Loss_G: 0.352658 Loss_D_real: -0.359220 Loss_D_fake 0.353526\n",
      "[26/10000][10/61][40] Loss_D: -0.688950 Loss_G: 0.343286 Loss_D_real: -0.349258 Loss_D_fake 0.339693\n",
      "[27/10000][45/61][60] Loss_D: -0.684779 Loss_G: 0.338662 Loss_D_real: -0.346291 Loss_D_fake 0.338489\n",
      "[29/10000][15/61][80] Loss_D: -0.677072 Loss_G: 0.336997 Loss_D_real: -0.341223 Loss_D_fake 0.335849\n",
      "[30/10000][50/61][100] Loss_D: -0.696357 Loss_G: 0.347438 Loss_D_real: -0.352079 Loss_D_fake 0.344278\n",
      "[32/10000][20/61][120] Loss_D: -0.702816 Loss_G: 0.351717 Loss_D_real: -0.355799 Loss_D_fake 0.347017\n",
      "[33/10000][55/61][140] Loss_D: -0.696102 Loss_G: 0.347499 Loss_D_real: -0.352013 Loss_D_fake 0.344089\n",
      "[35/10000][25/61][160] Loss_D: -0.635940 Loss_G: 0.321645 Loss_D_real: -0.323892 Loss_D_fake 0.312049\n",
      "[36/10000][60/61][180] Loss_D: -0.556209 Loss_G: 0.319745 Loss_D_real: -0.310324 Loss_D_fake 0.245885\n",
      "[38/10000][30/61][200] Loss_D: -0.535494 Loss_G: 0.295929 Loss_D_real: -0.273080 Loss_D_fake 0.262413\n",
      "[39/10000][61/61][220] Loss_D: -0.278900 Loss_G: 0.192367 Loss_D_real: -0.048409 Loss_D_fake 0.230492\n",
      "[41/10000][35/61][240] Loss_D: -0.511259 Loss_G: 0.297156 Loss_D_real: -0.276158 Loss_D_fake 0.235100\n",
      "[43/10000][5/61][260] Loss_D: -0.514072 Loss_G: 0.276047 Loss_D_real: -0.246566 Loss_D_fake 0.267507\n",
      "[44/10000][40/61][280] Loss_D: -0.499575 Loss_G: 0.258217 Loss_D_real: -0.237264 Loss_D_fake 0.262311\n",
      "[46/10000][10/61][300] Loss_D: -0.503156 Loss_G: 0.277085 Loss_D_real: -0.243383 Loss_D_fake 0.259773\n",
      "[47/10000][45/61][320] Loss_D: -0.522360 Loss_G: 0.275437 Loss_D_real: -0.250657 Loss_D_fake 0.271703\n",
      "[49/10000][15/61][340] Loss_D: -0.484677 Loss_G: 0.241907 Loss_D_real: -0.227838 Loss_D_fake 0.256839\n",
      "[50/10000][50/61][360] Loss_D: -0.424541 Loss_G: 0.290767 Loss_D_real: -0.276483 Loss_D_fake 0.148058\n",
      "[52/10000][20/61][380] Loss_D: -0.445211 Loss_G: 0.195337 Loss_D_real: -0.179989 Loss_D_fake 0.265222\n",
      "[53/10000][55/61][400] Loss_D: -0.396997 Loss_G: 0.232740 Loss_D_real: -0.205134 Loss_D_fake 0.191863\n",
      "[55/10000][25/61][420] Loss_D: -0.393365 Loss_G: 0.263090 Loss_D_real: -0.234069 Loss_D_fake 0.159295\n",
      "[56/10000][60/61][440] Loss_D: -0.410523 Loss_G: 0.209968 Loss_D_real: -0.183744 Loss_D_fake 0.226779\n",
      "[58/10000][30/61][460] Loss_D: -0.418847 Loss_G: 0.224401 Loss_D_real: -0.195332 Loss_D_fake 0.223515\n",
      "[59/10000][61/61][480] Loss_D: -0.397998 Loss_G: 0.197524 Loss_D_real: -0.209461 Loss_D_fake 0.188537\n",
      "[61/10000][35/61][500] Loss_D: -0.358563 Loss_G: 0.149766 Loss_D_real: -0.130886 Loss_D_fake 0.227677\n",
      "[63/10000][30/61][520] Loss_D: -0.390323 Loss_G: 0.202192 Loss_D_real: -0.182806 Loss_D_fake 0.207517\n",
      "[64/10000][61/61][540] Loss_D: -0.426871 Loss_G: 0.199188 Loss_D_real: -0.209762 Loss_D_fake 0.217109\n",
      "[66/10000][35/61][560] Loss_D: -0.370261 Loss_G: 0.253164 Loss_D_real: -0.226158 Loss_D_fake 0.144103\n",
      "[68/10000][5/61][580] Loss_D: -0.376388 Loss_G: 0.236368 Loss_D_real: -0.198544 Loss_D_fake 0.177844\n",
      "[69/10000][40/61][600] Loss_D: -0.373394 Loss_G: 0.246413 Loss_D_real: -0.216302 Loss_D_fake 0.157092\n",
      "[71/10000][10/61][620] Loss_D: -0.335597 Loss_G: 0.244398 Loss_D_real: -0.208714 Loss_D_fake 0.126883\n",
      "[72/10000][45/61][640] Loss_D: -0.319071 Loss_G: 0.202081 Loss_D_real: -0.175528 Loss_D_fake 0.143543\n",
      "[74/10000][15/61][660] Loss_D: -0.323275 Loss_G: 0.156684 Loss_D_real: -0.128800 Loss_D_fake 0.194475\n",
      "[75/10000][50/61][680] Loss_D: -0.332356 Loss_G: 0.206743 Loss_D_real: -0.195342 Loss_D_fake 0.137013\n",
      "[77/10000][20/61][700] Loss_D: -0.291087 Loss_G: 0.217209 Loss_D_real: -0.203687 Loss_D_fake 0.087399\n",
      "[78/10000][55/61][720] Loss_D: -0.277012 Loss_G: 0.136655 Loss_D_real: -0.116170 Loss_D_fake 0.160841\n",
      "[80/10000][25/61][740] Loss_D: -0.257293 Loss_G: 0.225184 Loss_D_real: -0.186404 Loss_D_fake 0.070889\n",
      "[81/10000][60/61][760] Loss_D: -0.256587 Loss_G: 0.202470 Loss_D_real: -0.172378 Loss_D_fake 0.084209\n",
      "[83/10000][30/61][780] Loss_D: -0.249785 Loss_G: 0.167507 Loss_D_real: -0.152365 Loss_D_fake 0.097421\n",
      "[84/10000][61/61][800] Loss_D: -0.161457 Loss_G: -0.008660 Loss_D_real: -0.030952 Loss_D_fake 0.130505\n",
      "[86/10000][35/61][820] Loss_D: -0.220342 Loss_G: 0.053492 Loss_D_real: -0.067816 Loss_D_fake 0.152526\n",
      "[88/10000][5/61][840] Loss_D: -0.206410 Loss_G: 0.048937 Loss_D_real: -0.051857 Loss_D_fake 0.154553\n",
      "[89/10000][40/61][860] Loss_D: -0.180369 Loss_G: 0.075792 Loss_D_real: -0.040083 Loss_D_fake 0.140286\n",
      "[91/10000][10/61][880] Loss_D: -0.197921 Loss_G: 0.106813 Loss_D_real: -0.089738 Loss_D_fake 0.108183\n",
      "[92/10000][45/61][900] Loss_D: -0.189835 Loss_G: 0.182985 Loss_D_real: -0.161313 Loss_D_fake 0.028523\n",
      "[94/10000][15/61][920] Loss_D: -0.195958 Loss_G: 0.027653 Loss_D_real: -0.052079 Loss_D_fake 0.143879\n",
      "[95/10000][50/61][940] Loss_D: -0.167459 Loss_G: 0.041684 Loss_D_real: -0.032285 Loss_D_fake 0.135173\n",
      "[97/10000][20/61][960] Loss_D: -0.174373 Loss_G: 0.033901 Loss_D_real: -0.050630 Loss_D_fake 0.123743\n",
      "[98/10000][55/61][980] Loss_D: -0.183994 Loss_G: 0.166085 Loss_D_real: -0.142439 Loss_D_fake 0.041555\n",
      "[100/10000][25/61][1000] Loss_D: -0.172626 Loss_G: 0.107967 Loss_D_real: -0.101148 Loss_D_fake 0.071478\n",
      "[102/10000][30/61][1020] Loss_D: -0.159420 Loss_G: 0.021080 Loss_D_real: -0.016753 Loss_D_fake 0.142667\n",
      "[103/10000][61/61][1040] Loss_D: -0.144710 Loss_G: 0.096051 Loss_D_real: -0.164801 Loss_D_fake -0.020091\n",
      "[105/10000][35/61][1060] Loss_D: -0.163549 Loss_G: 0.043538 Loss_D_real: -0.051939 Loss_D_fake 0.111609\n",
      "[107/10000][5/61][1080] Loss_D: -0.165804 Loss_G: 0.121306 Loss_D_real: -0.102794 Loss_D_fake 0.063010\n",
      "[108/10000][40/61][1100] Loss_D: -0.147760 Loss_G: 0.134627 Loss_D_real: -0.113905 Loss_D_fake 0.033856\n",
      "[110/10000][10/61][1120] Loss_D: -0.168720 Loss_G: 0.093625 Loss_D_real: -0.087068 Loss_D_fake 0.081652\n",
      "[111/10000][45/61][1140] Loss_D: -0.139334 Loss_G: 0.022531 Loss_D_real: 0.001833 Loss_D_fake 0.141168\n",
      "[113/10000][15/61][1160] Loss_D: -0.128844 Loss_G: -0.004374 Loss_D_real: -0.007025 Loss_D_fake 0.121820\n",
      "[114/10000][50/61][1180] Loss_D: -0.138933 Loss_G: 0.046395 Loss_D_real: -0.066483 Loss_D_fake 0.072450\n",
      "[116/10000][20/61][1200] Loss_D: -0.134796 Loss_G: 0.159687 Loss_D_real: -0.142513 Loss_D_fake -0.007717\n",
      "[117/10000][55/61][1220] Loss_D: -0.143841 Loss_G: 0.015298 Loss_D_real: -0.018681 Loss_D_fake 0.125160\n",
      "[119/10000][25/61][1240] Loss_D: -0.143451 Loss_G: 0.019427 Loss_D_real: -0.036879 Loss_D_fake 0.106572\n",
      "[120/10000][60/61][1260] Loss_D: -0.127990 Loss_G: 0.008732 Loss_D_real: 0.030432 Loss_D_fake 0.158423\n",
      "[122/10000][30/61][1280] Loss_D: -0.147778 Loss_G: 0.022063 Loss_D_real: -0.042649 Loss_D_fake 0.105129\n",
      "[123/10000][61/61][1300] Loss_D: -0.077692 Loss_G: 0.033948 Loss_D_real: 0.007434 Loss_D_fake 0.085126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125/10000][35/61][1320] Loss_D: -0.118510 Loss_G: -0.012028 Loss_D_real: 0.024256 Loss_D_fake 0.142765\n",
      "[127/10000][5/61][1340] Loss_D: -0.114992 Loss_G: 0.076524 Loss_D_real: -0.063968 Loss_D_fake 0.051024\n",
      "[128/10000][40/61][1360] Loss_D: -0.122773 Loss_G: 0.108266 Loss_D_real: -0.078519 Loss_D_fake 0.044253\n",
      "[130/10000][10/61][1380] Loss_D: -0.126638 Loss_G: 0.013695 Loss_D_real: -0.059307 Loss_D_fake 0.067331\n",
      "[131/10000][45/61][1400] Loss_D: -0.113202 Loss_G: 0.084305 Loss_D_real: -0.045928 Loss_D_fake 0.067274\n",
      "[133/10000][15/61][1420] Loss_D: -0.110331 Loss_G: 0.026579 Loss_D_real: -0.036456 Loss_D_fake 0.073875\n",
      "[134/10000][50/61][1440] Loss_D: -0.117054 Loss_G: 0.105128 Loss_D_real: -0.073063 Loss_D_fake 0.043991\n",
      "[136/10000][20/61][1460] Loss_D: -0.132143 Loss_G: 0.037141 Loss_D_real: -0.053850 Loss_D_fake 0.078293\n",
      "[137/10000][55/61][1480] Loss_D: -0.116861 Loss_G: 0.030940 Loss_D_real: -0.031243 Loss_D_fake 0.085618\n",
      "[139/10000][25/61][1500] Loss_D: -0.132910 Loss_G: 0.140849 Loss_D_real: -0.094451 Loss_D_fake 0.038460\n",
      "[141/10000][30/61][1520] Loss_D: -0.106269 Loss_G: 0.088356 Loss_D_real: -0.065972 Loss_D_fake 0.040297\n",
      "[142/10000][61/61][1540] Loss_D: -0.051575 Loss_G: 0.023411 Loss_D_real: 0.068000 Loss_D_fake 0.119575\n",
      "[144/10000][35/61][1560] Loss_D: -0.122740 Loss_G: 0.049096 Loss_D_real: -0.047379 Loss_D_fake 0.075361\n",
      "[146/10000][5/61][1580] Loss_D: -0.118719 Loss_G: 0.050148 Loss_D_real: -0.074227 Loss_D_fake 0.044492\n",
      "[147/10000][40/61][1600] Loss_D: -0.118817 Loss_G: 0.065145 Loss_D_real: -0.054312 Loss_D_fake 0.064505\n",
      "[149/10000][10/61][1620] Loss_D: -0.106896 Loss_G: 0.032214 Loss_D_real: -0.042622 Loss_D_fake 0.064274\n",
      "[150/10000][45/61][1640] Loss_D: -0.101790 Loss_G: 0.002233 Loss_D_real: 0.033636 Loss_D_fake 0.135425\n",
      "[152/10000][15/61][1660] Loss_D: -0.111975 Loss_G: 0.081570 Loss_D_real: -0.064762 Loss_D_fake 0.047213\n",
      "[153/10000][50/61][1680] Loss_D: -0.113887 Loss_G: 0.021637 Loss_D_real: -0.029179 Loss_D_fake 0.084708\n",
      "[155/10000][20/61][1700] Loss_D: -0.114528 Loss_G: 0.040943 Loss_D_real: -0.019946 Loss_D_fake 0.094582\n",
      "[156/10000][55/61][1720] Loss_D: -0.122093 Loss_G: 0.107244 Loss_D_real: -0.076020 Loss_D_fake 0.046073\n",
      "[158/10000][25/61][1740] Loss_D: -0.102775 Loss_G: 0.106211 Loss_D_real: -0.091158 Loss_D_fake 0.011616\n",
      "[159/10000][60/61][1760] Loss_D: -0.113618 Loss_G: 0.023822 Loss_D_real: -0.001986 Loss_D_fake 0.111632\n",
      "[161/10000][30/61][1780] Loss_D: -0.103291 Loss_G: -0.007669 Loss_D_real: -0.002153 Loss_D_fake 0.101138\n",
      "[162/10000][61/61][1800] Loss_D: -0.123940 Loss_G: 0.040924 Loss_D_real: -0.124358 Loss_D_fake -0.000419\n",
      "[164/10000][35/61][1820] Loss_D: -0.103921 Loss_G: 0.099095 Loss_D_real: -0.067145 Loss_D_fake 0.036776\n",
      "[166/10000][5/61][1840] Loss_D: -0.109111 Loss_G: 0.053297 Loss_D_real: -0.034041 Loss_D_fake 0.075070\n",
      "[167/10000][40/61][1860] Loss_D: -0.112706 Loss_G: 0.085210 Loss_D_real: -0.062375 Loss_D_fake 0.050331\n",
      "[169/10000][10/61][1880] Loss_D: -0.104875 Loss_G: 0.018180 Loss_D_real: 0.010623 Loss_D_fake 0.115498\n",
      "[170/10000][45/61][1900] Loss_D: -0.098081 Loss_G: 0.013752 Loss_D_real: 0.008585 Loss_D_fake 0.106666\n",
      "[172/10000][15/61][1920] Loss_D: -0.098460 Loss_G: 0.102972 Loss_D_real: -0.087168 Loss_D_fake 0.011293\n",
      "[173/10000][50/61][1940] Loss_D: -0.110869 Loss_G: 0.145374 Loss_D_real: -0.096558 Loss_D_fake 0.014311\n",
      "[175/10000][20/61][1960] Loss_D: -0.115892 Loss_G: 0.006369 Loss_D_real: -0.010449 Loss_D_fake 0.105443\n",
      "[176/10000][55/61][1980] Loss_D: -0.117471 Loss_G: 0.097138 Loss_D_real: -0.049643 Loss_D_fake 0.067828\n",
      "[178/10000][25/61][2000] Loss_D: -0.118541 Loss_G: 0.084532 Loss_D_real: -0.031397 Loss_D_fake 0.087144\n",
      "[180/10000][30/61][2020] Loss_D: -0.100878 Loss_G: 0.111785 Loss_D_real: -0.079710 Loss_D_fake 0.021168\n",
      "[181/10000][61/61][2040] Loss_D: -0.018376 Loss_G: 0.028202 Loss_D_real: -0.007357 Loss_D_fake 0.011020\n",
      "[183/10000][35/61][2060] Loss_D: -0.120748 Loss_G: 0.125905 Loss_D_real: -0.119556 Loss_D_fake 0.001191\n",
      "[185/10000][5/61][2080] Loss_D: -0.095629 Loss_G: 0.072937 Loss_D_real: -0.023202 Loss_D_fake 0.072427\n",
      "[186/10000][40/61][2100] Loss_D: -0.106850 Loss_G: 0.019435 Loss_D_real: 0.001651 Loss_D_fake 0.108501\n",
      "[188/10000][10/61][2120] Loss_D: -0.113079 Loss_G: 0.045768 Loss_D_real: -0.021679 Loss_D_fake 0.091399\n",
      "[189/10000][45/61][2140] Loss_D: -0.095657 Loss_G: 0.077142 Loss_D_real: -0.080822 Loss_D_fake 0.014836\n",
      "[191/10000][15/61][2160] Loss_D: -0.098817 Loss_G: 0.102012 Loss_D_real: -0.097226 Loss_D_fake 0.001591\n",
      "[192/10000][50/61][2180] Loss_D: -0.106757 Loss_G: 0.108844 Loss_D_real: -0.090742 Loss_D_fake 0.016015\n",
      "[194/10000][20/61][2200] Loss_D: -0.122516 Loss_G: -0.008467 Loss_D_real: 0.017664 Loss_D_fake 0.140179\n",
      "[195/10000][55/61][2220] Loss_D: -0.099266 Loss_G: 0.048018 Loss_D_real: -0.000417 Loss_D_fake 0.098849\n",
      "[197/10000][25/61][2240] Loss_D: -0.113546 Loss_G: 0.068546 Loss_D_real: -0.024079 Loss_D_fake 0.089467\n",
      "[198/10000][60/61][2260] Loss_D: -0.092239 Loss_G: 0.017299 Loss_D_real: 0.048214 Loss_D_fake 0.140453\n",
      "[200/10000][30/61][2280] Loss_D: -0.096924 Loss_G: 0.078190 Loss_D_real: -0.045211 Loss_D_fake 0.051713\n",
      "[201/10000][61/61][2300] Loss_D: -0.121887 Loss_G: 0.120210 Loss_D_real: -0.069655 Loss_D_fake 0.052231\n",
      "[203/10000][35/61][2320] Loss_D: -0.096054 Loss_G: 0.011165 Loss_D_real: -0.013159 Loss_D_fake 0.082894\n",
      "[205/10000][5/61][2340] Loss_D: -0.060225 Loss_G: 0.055689 Loss_D_real: 0.023708 Loss_D_fake 0.083934\n",
      "[206/10000][40/61][2360] Loss_D: -0.107708 Loss_G: 0.081990 Loss_D_real: -0.088350 Loss_D_fake 0.019358\n",
      "[208/10000][10/61][2380] Loss_D: -0.105375 Loss_G: 0.006328 Loss_D_real: 0.010506 Loss_D_fake 0.115881\n",
      "[209/10000][45/61][2400] Loss_D: -0.112142 Loss_G: 0.055485 Loss_D_real: -0.019853 Loss_D_fake 0.092289\n",
      "[211/10000][15/61][2420] Loss_D: -0.109760 Loss_G: 0.022286 Loss_D_real: -0.002102 Loss_D_fake 0.107657\n",
      "[212/10000][50/61][2440] Loss_D: -0.104517 Loss_G: 0.113920 Loss_D_real: -0.073431 Loss_D_fake 0.031086\n",
      "[214/10000][20/61][2460] Loss_D: -0.118761 Loss_G: 0.070252 Loss_D_real: -0.041661 Loss_D_fake 0.077100\n",
      "[215/10000][55/61][2480] Loss_D: -0.104411 Loss_G: 0.047469 Loss_D_real: -0.013539 Loss_D_fake 0.090871\n",
      "[217/10000][25/61][2500] Loss_D: -0.122189 Loss_G: 0.092358 Loss_D_real: -0.083851 Loss_D_fake 0.038338\n",
      "[219/10000][30/61][2520] Loss_D: -0.098633 Loss_G: 0.092026 Loss_D_real: -0.088265 Loss_D_fake 0.010367\n",
      "[220/10000][61/61][2540] Loss_D: -0.041558 Loss_G: 0.039680 Loss_D_real: 0.069862 Loss_D_fake 0.111420\n",
      "[222/10000][35/61][2560] Loss_D: -0.096236 Loss_G: 0.093557 Loss_D_real: -0.089964 Loss_D_fake 0.006272\n",
      "[224/10000][5/61][2580] Loss_D: -0.095197 Loss_G: 0.063162 Loss_D_real: -0.040994 Loss_D_fake 0.054203\n",
      "[225/10000][40/61][2600] Loss_D: -0.116622 Loss_G: 0.050332 Loss_D_real: -0.052999 Loss_D_fake 0.063624\n",
      "[227/10000][10/61][2620] Loss_D: -0.098758 Loss_G: 0.069307 Loss_D_real: -0.037793 Loss_D_fake 0.060965\n",
      "[228/10000][45/61][2640] Loss_D: -0.095851 Loss_G: 0.041219 Loss_D_real: -0.050071 Loss_D_fake 0.045781\n",
      "[230/10000][15/61][2660] Loss_D: -0.099378 Loss_G: 0.043357 Loss_D_real: 0.005656 Loss_D_fake 0.105034\n",
      "[231/10000][50/61][2680] Loss_D: -0.099239 Loss_G: 0.001501 Loss_D_real: -0.015907 Loss_D_fake 0.083332\n",
      "[233/10000][20/61][2700] Loss_D: -0.090376 Loss_G: 0.134035 Loss_D_real: -0.104335 Loss_D_fake -0.013959\n",
      "[234/10000][55/61][2720] Loss_D: -0.098960 Loss_G: 0.070716 Loss_D_real: -0.046393 Loss_D_fake 0.052567\n",
      "[236/10000][25/61][2740] Loss_D: -0.108083 Loss_G: 0.041062 Loss_D_real: -0.015177 Loss_D_fake 0.092906\n",
      "[237/10000][60/61][2760] Loss_D: -0.107952 Loss_G: 0.023217 Loss_D_real: -0.006998 Loss_D_fake 0.100955\n",
      "[239/10000][30/61][2780] Loss_D: -0.102483 Loss_G: 0.090728 Loss_D_real: -0.057169 Loss_D_fake 0.045314\n",
      "[240/10000][61/61][2800] Loss_D: -0.031339 Loss_G: -0.065770 Loss_D_real: -0.020624 Loss_D_fake 0.010715\n",
      "[242/10000][35/61][2820] Loss_D: -0.078872 Loss_G: 0.044172 Loss_D_real: 0.087639 Loss_D_fake 0.166511\n",
      "[244/10000][5/61][2840] Loss_D: -0.084704 Loss_G: 0.009363 Loss_D_real: -0.034930 Loss_D_fake 0.049774\n",
      "[245/10000][40/61][2860] Loss_D: -0.101140 Loss_G: 0.098859 Loss_D_real: -0.063142 Loss_D_fake 0.037999\n",
      "[247/10000][10/61][2880] Loss_D: -0.096707 Loss_G: 0.128952 Loss_D_real: -0.090857 Loss_D_fake 0.005850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248/10000][45/61][2900] Loss_D: -0.098372 Loss_G: 0.073065 Loss_D_real: -0.018290 Loss_D_fake 0.080082\n",
      "[250/10000][15/61][2920] Loss_D: -0.112909 Loss_G: 0.052201 Loss_D_real: -0.025748 Loss_D_fake 0.087161\n",
      "[251/10000][50/61][2940] Loss_D: -0.104802 Loss_G: 0.143292 Loss_D_real: -0.099527 Loss_D_fake 0.005275\n",
      "[253/10000][20/61][2960] Loss_D: -0.086962 Loss_G: 0.027913 Loss_D_real: -0.000293 Loss_D_fake 0.086668\n",
      "[254/10000][55/61][2980] Loss_D: -0.087279 Loss_G: 0.031786 Loss_D_real: -0.004654 Loss_D_fake 0.082625\n",
      "[256/10000][25/61][3000] Loss_D: -0.110896 Loss_G: 0.076489 Loss_D_real: -0.042811 Loss_D_fake 0.068086\n",
      "[258/10000][30/61][3020] Loss_D: -0.096443 Loss_G: -0.001696 Loss_D_real: -0.001581 Loss_D_fake 0.094862\n",
      "[259/10000][61/61][3040] Loss_D: -0.040640 Loss_G: -0.076623 Loss_D_real: 0.072774 Loss_D_fake 0.113414\n",
      "[261/10000][35/61][3060] Loss_D: -0.104652 Loss_G: 0.033564 Loss_D_real: -0.030131 Loss_D_fake 0.074522\n",
      "[263/10000][5/61][3080] Loss_D: -0.068258 Loss_G: 0.018460 Loss_D_real: 0.014945 Loss_D_fake 0.083203\n",
      "[264/10000][40/61][3100] Loss_D: -0.085815 Loss_G: 0.043395 Loss_D_real: -0.012324 Loss_D_fake 0.073492\n",
      "[266/10000][10/61][3120] Loss_D: -0.098251 Loss_G: 0.105200 Loss_D_real: -0.061537 Loss_D_fake 0.036714\n",
      "[267/10000][45/61][3140] Loss_D: -0.097808 Loss_G: 0.076852 Loss_D_real: -0.046237 Loss_D_fake 0.051571\n",
      "[269/10000][15/61][3160] Loss_D: -0.104434 Loss_G: 0.010252 Loss_D_real: -0.000511 Loss_D_fake 0.103923\n",
      "[270/10000][50/61][3180] Loss_D: -0.116078 Loss_G: 0.126105 Loss_D_real: -0.070899 Loss_D_fake 0.045179\n",
      "[272/10000][20/61][3200] Loss_D: -0.109961 Loss_G: 0.090957 Loss_D_real: -0.069733 Loss_D_fake 0.040228\n",
      "[273/10000][55/61][3220] Loss_D: -0.126192 Loss_G: 0.098058 Loss_D_real: -0.061724 Loss_D_fake 0.064468\n",
      "[275/10000][25/61][3240] Loss_D: -0.102516 Loss_G: 0.113046 Loss_D_real: -0.058516 Loss_D_fake 0.044000\n",
      "[276/10000][60/61][3260] Loss_D: -0.110410 Loss_G: -0.003175 Loss_D_real: -0.020625 Loss_D_fake 0.089785\n",
      "[278/10000][30/61][3280] Loss_D: -0.100244 Loss_G: 0.117781 Loss_D_real: -0.102532 Loss_D_fake -0.002288\n",
      "[279/10000][61/61][3300] Loss_D: -0.009440 Loss_G: 0.025355 Loss_D_real: -0.021810 Loss_D_fake -0.012371\n",
      "[281/10000][35/61][3320] Loss_D: -0.088549 Loss_G: 0.094851 Loss_D_real: -0.083255 Loss_D_fake 0.005294\n",
      "[283/10000][5/61][3340] Loss_D: -0.106168 Loss_G: 0.076104 Loss_D_real: -0.092790 Loss_D_fake 0.013378\n",
      "[284/10000][40/61][3360] Loss_D: -0.081725 Loss_G: 0.055720 Loss_D_real: 0.017971 Loss_D_fake 0.099695\n",
      "[286/10000][10/61][3380] Loss_D: -0.099561 Loss_G: 0.153822 Loss_D_real: -0.093188 Loss_D_fake 0.006373\n",
      "[287/10000][45/61][3400] Loss_D: -0.100290 Loss_G: 0.128271 Loss_D_real: -0.090987 Loss_D_fake 0.009303\n",
      "[289/10000][15/61][3420] Loss_D: -0.101765 Loss_G: 0.022603 Loss_D_real: 0.031311 Loss_D_fake 0.133077\n",
      "[290/10000][50/61][3440] Loss_D: -0.110409 Loss_G: 0.003561 Loss_D_real: 0.012794 Loss_D_fake 0.123203\n",
      "[292/10000][20/61][3460] Loss_D: -0.092388 Loss_G: 0.002366 Loss_D_real: -0.029691 Loss_D_fake 0.062697\n",
      "[293/10000][55/61][3480] Loss_D: -0.091608 Loss_G: 0.107200 Loss_D_real: -0.086956 Loss_D_fake 0.004652\n",
      "[295/10000][25/61][3500] Loss_D: -0.093354 Loss_G: 0.114368 Loss_D_real: -0.083181 Loss_D_fake 0.010173\n",
      "[297/10000][30/61][3520] Loss_D: -0.107697 Loss_G: 0.035616 Loss_D_real: -0.007997 Loss_D_fake 0.099700\n",
      "[298/10000][61/61][3540] Loss_D: -0.122693 Loss_G: 0.084895 Loss_D_real: -0.067392 Loss_D_fake 0.055301\n",
      "[300/10000][35/61][3560] Loss_D: -0.109395 Loss_G: -0.037152 Loss_D_real: 0.022423 Loss_D_fake 0.131818\n",
      "[302/10000][5/61][3580] Loss_D: -0.064891 Loss_G: 0.067432 Loss_D_real: -0.024791 Loss_D_fake 0.040100\n",
      "[303/10000][40/61][3600] Loss_D: -0.103767 Loss_G: 0.003530 Loss_D_real: 0.003303 Loss_D_fake 0.107070\n",
      "[305/10000][10/61][3620] Loss_D: -0.092377 Loss_G: 0.107748 Loss_D_real: -0.080775 Loss_D_fake 0.011601\n",
      "[306/10000][45/61][3640] Loss_D: -0.102868 Loss_G: 0.042384 Loss_D_real: -0.004106 Loss_D_fake 0.098762\n",
      "[308/10000][15/61][3660] Loss_D: -0.108943 Loss_G: 0.047283 Loss_D_real: -0.040354 Loss_D_fake 0.068589\n",
      "[309/10000][50/61][3680] Loss_D: -0.108418 Loss_G: 0.031111 Loss_D_real: -0.035059 Loss_D_fake 0.073359\n",
      "[311/10000][20/61][3700] Loss_D: -0.108526 Loss_G: 0.128790 Loss_D_real: -0.118937 Loss_D_fake -0.010411\n",
      "[312/10000][55/61][3720] Loss_D: -0.109447 Loss_G: 0.105247 Loss_D_real: -0.085989 Loss_D_fake 0.023458\n",
      "[314/10000][25/61][3740] Loss_D: -0.104093 Loss_G: 0.079461 Loss_D_real: -0.052911 Loss_D_fake 0.051182\n",
      "[315/10000][60/61][3760] Loss_D: -0.102296 Loss_G: 0.111679 Loss_D_real: -0.078868 Loss_D_fake 0.023428\n",
      "[317/10000][30/61][3780] Loss_D: -0.080819 Loss_G: 0.119628 Loss_D_real: -0.080587 Loss_D_fake 0.000232\n",
      "[318/10000][61/61][3800] Loss_D: -0.069139 Loss_G: 0.006995 Loss_D_real: -0.020177 Loss_D_fake 0.048962\n",
      "[320/10000][35/61][3820] Loss_D: -0.101325 Loss_G: 0.041477 Loss_D_real: -0.021598 Loss_D_fake 0.079727\n",
      "[322/10000][5/61][3840] Loss_D: -0.107419 Loss_G: 0.043850 Loss_D_real: -0.051560 Loss_D_fake 0.055860\n",
      "[323/10000][40/61][3860] Loss_D: -0.098401 Loss_G: -0.007112 Loss_D_real: -0.001042 Loss_D_fake 0.097358\n",
      "[325/10000][10/61][3880] Loss_D: -0.098541 Loss_G: 0.042669 Loss_D_real: -0.002273 Loss_D_fake 0.096269\n",
      "[326/10000][45/61][3900] Loss_D: -0.116841 Loss_G: 0.040634 Loss_D_real: -0.016520 Loss_D_fake 0.100321\n",
      "[328/10000][15/61][3920] Loss_D: -0.102260 Loss_G: 0.045144 Loss_D_real: -0.023493 Loss_D_fake 0.078767\n",
      "[329/10000][50/61][3940] Loss_D: -0.102350 Loss_G: 0.085006 Loss_D_real: -0.054086 Loss_D_fake 0.048264\n",
      "[331/10000][20/61][3960] Loss_D: -0.095346 Loss_G: 0.098194 Loss_D_real: -0.057078 Loss_D_fake 0.038267\n",
      "[332/10000][55/61][3980] Loss_D: -0.102110 Loss_G: 0.029303 Loss_D_real: -0.044657 Loss_D_fake 0.057453\n",
      "[334/10000][25/61][4000] Loss_D: -0.095385 Loss_G: 0.086451 Loss_D_real: -0.069312 Loss_D_fake 0.026073\n",
      "[336/10000][30/61][4020] Loss_D: -0.097198 Loss_G: 0.009122 Loss_D_real: 0.018500 Loss_D_fake 0.115698\n",
      "[337/10000][61/61][4040] Loss_D: -0.214602 Loss_G: 0.145022 Loss_D_real: -0.139344 Loss_D_fake 0.075258\n",
      "[339/10000][35/61][4060] Loss_D: -0.096084 Loss_G: 0.020845 Loss_D_real: -0.028586 Loss_D_fake 0.067498\n",
      "[341/10000][5/61][4080] Loss_D: -0.103799 Loss_G: 0.044449 Loss_D_real: -0.040354 Loss_D_fake 0.063446\n",
      "[342/10000][40/61][4100] Loss_D: -0.102117 Loss_G: 0.027973 Loss_D_real: -0.014764 Loss_D_fake 0.087353\n",
      "[344/10000][10/61][4120] Loss_D: -0.091282 Loss_G: -0.010877 Loss_D_real: 0.023579 Loss_D_fake 0.114861\n",
      "[345/10000][45/61][4140] Loss_D: -0.113119 Loss_G: 0.084118 Loss_D_real: -0.064563 Loss_D_fake 0.048557\n",
      "[347/10000][15/61][4160] Loss_D: -0.088738 Loss_G: 0.019892 Loss_D_real: -0.007835 Loss_D_fake 0.080904\n",
      "[348/10000][50/61][4180] Loss_D: -0.098713 Loss_G: 0.036300 Loss_D_real: -0.012641 Loss_D_fake 0.086072\n",
      "[350/10000][20/61][4200] Loss_D: -0.092109 Loss_G: 0.040309 Loss_D_real: 0.022540 Loss_D_fake 0.114649\n",
      "[351/10000][55/61][4220] Loss_D: -0.099595 Loss_G: 0.098954 Loss_D_real: -0.066407 Loss_D_fake 0.033189\n",
      "[353/10000][25/61][4240] Loss_D: -0.103590 Loss_G: 0.071449 Loss_D_real: -0.059211 Loss_D_fake 0.044378\n",
      "[354/10000][60/61][4260] Loss_D: -0.098783 Loss_G: 0.112110 Loss_D_real: -0.067811 Loss_D_fake 0.030973\n",
      "[356/10000][30/61][4280] Loss_D: -0.103190 Loss_G: 0.037479 Loss_D_real: -0.017124 Loss_D_fake 0.086067\n",
      "[357/10000][61/61][4300] Loss_D: -0.110999 Loss_G: -0.067543 Loss_D_real: -0.121716 Loss_D_fake -0.010717\n",
      "[359/10000][35/61][4320] Loss_D: -0.094271 Loss_G: 0.032146 Loss_D_real: -0.024329 Loss_D_fake 0.069942\n",
      "[361/10000][5/61][4340] Loss_D: -0.098445 Loss_G: 0.101698 Loss_D_real: -0.063305 Loss_D_fake 0.035139\n",
      "[362/10000][40/61][4360] Loss_D: -0.088941 Loss_G: 0.050974 Loss_D_real: -0.046911 Loss_D_fake 0.042030\n",
      "[364/10000][10/61][4380] Loss_D: -0.094909 Loss_G: 0.068581 Loss_D_real: -0.002501 Loss_D_fake 0.092408\n",
      "[365/10000][45/61][4400] Loss_D: -0.080742 Loss_G: 0.107785 Loss_D_real: -0.062613 Loss_D_fake 0.018129\n",
      "[367/10000][15/61][4420] Loss_D: -0.102940 Loss_G: 0.114716 Loss_D_real: -0.093883 Loss_D_fake 0.009057\n",
      "[368/10000][50/61][4440] Loss_D: -0.089482 Loss_G: 0.059060 Loss_D_real: -0.033731 Loss_D_fake 0.055751\n",
      "[370/10000][20/61][4460] Loss_D: -0.092476 Loss_G: 0.085234 Loss_D_real: -0.042099 Loss_D_fake 0.050376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[371/10000][55/61][4480] Loss_D: -0.090223 Loss_G: 0.085669 Loss_D_real: -0.028831 Loss_D_fake 0.061392\n",
      "[373/10000][25/61][4500] Loss_D: -0.094823 Loss_G: 0.106182 Loss_D_real: -0.070323 Loss_D_fake 0.024500\n",
      "[375/10000][30/61][4520] Loss_D: -0.106764 Loss_G: 0.119487 Loss_D_real: -0.075698 Loss_D_fake 0.031067\n",
      "[376/10000][61/61][4540] Loss_D: -0.039616 Loss_G: 0.001329 Loss_D_real: 0.037938 Loss_D_fake 0.077553\n",
      "[378/10000][35/61][4560] Loss_D: -0.100652 Loss_G: 0.015681 Loss_D_real: 0.001046 Loss_D_fake 0.101699\n",
      "[380/10000][5/61][4580] Loss_D: -0.068679 Loss_G: 0.091053 Loss_D_real: 0.014561 Loss_D_fake 0.083241\n",
      "[381/10000][40/61][4600] Loss_D: -0.092518 Loss_G: -0.028156 Loss_D_real: -0.003693 Loss_D_fake 0.088826\n",
      "[383/10000][10/61][4620] Loss_D: -0.093178 Loss_G: 0.082780 Loss_D_real: -0.069326 Loss_D_fake 0.023853\n",
      "[384/10000][45/61][4640] Loss_D: -0.102293 Loss_G: 0.034158 Loss_D_real: -0.028415 Loss_D_fake 0.073878\n",
      "[386/10000][15/61][4660] Loss_D: -0.103066 Loss_G: -0.006705 Loss_D_real: 0.016164 Loss_D_fake 0.119231\n",
      "[387/10000][50/61][4680] Loss_D: -0.109736 Loss_G: 0.091752 Loss_D_real: -0.057945 Loss_D_fake 0.051791\n",
      "[389/10000][20/61][4700] Loss_D: -0.093628 Loss_G: 0.069462 Loss_D_real: -0.008034 Loss_D_fake 0.085594\n",
      "[390/10000][55/61][4720] Loss_D: -0.101116 Loss_G: 0.087036 Loss_D_real: -0.036402 Loss_D_fake 0.064714\n",
      "[392/10000][25/61][4740] Loss_D: -0.090033 Loss_G: 0.028944 Loss_D_real: -0.041720 Loss_D_fake 0.048313\n",
      "[393/10000][60/61][4760] Loss_D: -0.096580 Loss_G: 0.075336 Loss_D_real: -0.059184 Loss_D_fake 0.037397\n",
      "[395/10000][30/61][4780] Loss_D: -0.073559 Loss_G: 0.012916 Loss_D_real: -0.012080 Loss_D_fake 0.061479\n",
      "[396/10000][61/61][4800] Loss_D: -0.040641 Loss_G: 0.127729 Loss_D_real: 0.059307 Loss_D_fake 0.099948\n",
      "[398/10000][35/61][4820] Loss_D: -0.101722 Loss_G: 0.033278 Loss_D_real: -0.025481 Loss_D_fake 0.076241\n",
      "[400/10000][5/61][4840] Loss_D: -0.078092 Loss_G: 0.059495 Loss_D_real: -0.046178 Loss_D_fake 0.031913\n",
      "[401/10000][40/61][4860] Loss_D: -0.087950 Loss_G: -0.012932 Loss_D_real: 0.000301 Loss_D_fake 0.088251\n",
      "[403/10000][10/61][4880] Loss_D: -0.097565 Loss_G: 0.043629 Loss_D_real: -0.012116 Loss_D_fake 0.085449\n",
      "[404/10000][45/61][4900] Loss_D: -0.097399 Loss_G: 0.026036 Loss_D_real: -0.038593 Loss_D_fake 0.058805\n",
      "[406/10000][15/61][4920] Loss_D: -0.072420 Loss_G: 0.118664 Loss_D_real: -0.087888 Loss_D_fake -0.015468\n",
      "[407/10000][50/61][4940] Loss_D: -0.082652 Loss_G: 0.040932 Loss_D_real: -0.016633 Loss_D_fake 0.066019\n",
      "[409/10000][20/61][4960] Loss_D: -0.088620 Loss_G: 0.013607 Loss_D_real: -0.006658 Loss_D_fake 0.081962\n",
      "[410/10000][55/61][4980] Loss_D: -0.084135 Loss_G: 0.048147 Loss_D_real: -0.027766 Loss_D_fake 0.056369\n",
      "[412/10000][25/61][5000] Loss_D: -0.090194 Loss_G: 0.118155 Loss_D_real: -0.063621 Loss_D_fake 0.026572\n",
      "[414/10000][30/61][5020] Loss_D: -0.104717 Loss_G: 0.061452 Loss_D_real: -0.059336 Loss_D_fake 0.045382\n",
      "[415/10000][61/61][5040] Loss_D: -0.077060 Loss_G: -0.098696 Loss_D_real: -0.005644 Loss_D_fake 0.071416\n",
      "[417/10000][35/61][5060] Loss_D: -0.098621 Loss_G: 0.105020 Loss_D_real: -0.081624 Loss_D_fake 0.016997\n",
      "[419/10000][5/61][5080] Loss_D: -0.077575 Loss_G: 0.052998 Loss_D_real: 0.008117 Loss_D_fake 0.085692\n",
      "[420/10000][40/61][5100] Loss_D: -0.069598 Loss_G: 0.019312 Loss_D_real: -0.026554 Loss_D_fake 0.043044\n",
      "[422/10000][10/61][5120] Loss_D: -0.081405 Loss_G: 0.071131 Loss_D_real: -0.051349 Loss_D_fake 0.030056\n",
      "[423/10000][45/61][5140] Loss_D: -0.099797 Loss_G: 0.028487 Loss_D_real: -0.003299 Loss_D_fake 0.096497\n",
      "[425/10000][15/61][5160] Loss_D: -0.094410 Loss_G: 0.035005 Loss_D_real: -0.006540 Loss_D_fake 0.087871\n",
      "[426/10000][50/61][5180] Loss_D: -0.086720 Loss_G: 0.083338 Loss_D_real: -0.072404 Loss_D_fake 0.014316\n",
      "[428/10000][20/61][5200] Loss_D: -0.099540 Loss_G: -0.000202 Loss_D_real: -0.018338 Loss_D_fake 0.081202\n",
      "[429/10000][55/61][5220] Loss_D: -0.093394 Loss_G: 0.003429 Loss_D_real: 0.010429 Loss_D_fake 0.103823\n",
      "[431/10000][25/61][5240] Loss_D: -0.099684 Loss_G: 0.086338 Loss_D_real: -0.056439 Loss_D_fake 0.043246\n",
      "[432/10000][60/61][5260] Loss_D: -0.097935 Loss_G: 0.060150 Loss_D_real: -0.060770 Loss_D_fake 0.037165\n",
      "[434/10000][30/61][5280] Loss_D: -0.088668 Loss_G: 0.006106 Loss_D_real: 0.012739 Loss_D_fake 0.101407\n",
      "[435/10000][61/61][5300] Loss_D: -0.079647 Loss_G: 0.133908 Loss_D_real: -0.038624 Loss_D_fake 0.041023\n",
      "[437/10000][35/61][5320] Loss_D: -0.100202 Loss_G: 0.007514 Loss_D_real: -0.042855 Loss_D_fake 0.057347\n",
      "[439/10000][5/61][5340] Loss_D: -0.057219 Loss_G: 0.032310 Loss_D_real: 0.003333 Loss_D_fake 0.060552\n",
      "[440/10000][40/61][5360] Loss_D: -0.086679 Loss_G: 0.012207 Loss_D_real: 0.022076 Loss_D_fake 0.108755\n",
      "[442/10000][10/61][5380] Loss_D: -0.108750 Loss_G: 0.113271 Loss_D_real: -0.064290 Loss_D_fake 0.044460\n",
      "[443/10000][45/61][5400] Loss_D: -0.087914 Loss_G: 0.071515 Loss_D_real: -0.030039 Loss_D_fake 0.057875\n",
      "[445/10000][15/61][5420] Loss_D: -0.077268 Loss_G: 0.051049 Loss_D_real: -0.015945 Loss_D_fake 0.061323\n",
      "[446/10000][50/61][5440] Loss_D: -0.072209 Loss_G: 0.102870 Loss_D_real: -0.064326 Loss_D_fake 0.007883\n",
      "[448/10000][20/61][5460] Loss_D: -0.071476 Loss_G: 0.021875 Loss_D_real: 0.047994 Loss_D_fake 0.119470\n",
      "[449/10000][55/61][5480] Loss_D: -0.095344 Loss_G: 0.101368 Loss_D_real: -0.052238 Loss_D_fake 0.043106\n",
      "[451/10000][25/61][5500] Loss_D: -0.099183 Loss_G: 0.011799 Loss_D_real: -0.052121 Loss_D_fake 0.047062\n",
      "[453/10000][30/61][5520] Loss_D: -0.083974 Loss_G: 0.070105 Loss_D_real: -0.045234 Loss_D_fake 0.038741\n",
      "[454/10000][61/61][5540] Loss_D: -0.067140 Loss_G: -0.032227 Loss_D_real: -0.071218 Loss_D_fake -0.004078\n",
      "[456/10000][35/61][5560] Loss_D: -0.086952 Loss_G: 0.014010 Loss_D_real: -0.002244 Loss_D_fake 0.084708\n",
      "[458/10000][5/61][5580] Loss_D: -0.058005 Loss_G: 0.022773 Loss_D_real: -0.010613 Loss_D_fake 0.047392\n",
      "[459/10000][40/61][5600] Loss_D: -0.070465 Loss_G: 0.074311 Loss_D_real: -0.082710 Loss_D_fake -0.012245\n",
      "[461/10000][10/61][5620] Loss_D: -0.073561 Loss_G: 0.065665 Loss_D_real: -0.015459 Loss_D_fake 0.058103\n",
      "[462/10000][45/61][5640] Loss_D: -0.092358 Loss_G: 0.060190 Loss_D_real: -0.026000 Loss_D_fake 0.066358\n",
      "[464/10000][15/61][5660] Loss_D: -0.081756 Loss_G: 0.068593 Loss_D_real: -0.043034 Loss_D_fake 0.038722\n",
      "[465/10000][50/61][5680] Loss_D: -0.084288 Loss_G: 0.042128 Loss_D_real: -0.074826 Loss_D_fake 0.009462\n",
      "[467/10000][20/61][5700] Loss_D: -0.088559 Loss_G: 0.032510 Loss_D_real: -0.027758 Loss_D_fake 0.060801\n",
      "[468/10000][55/61][5720] Loss_D: -0.073463 Loss_G: 0.044862 Loss_D_real: -0.030286 Loss_D_fake 0.043177\n",
      "[470/10000][25/61][5740] Loss_D: -0.077323 Loss_G: 0.067957 Loss_D_real: -0.048434 Loss_D_fake 0.028889\n",
      "[471/10000][60/61][5760] Loss_D: -0.083419 Loss_G: 0.070994 Loss_D_real: -0.049275 Loss_D_fake 0.034144\n",
      "[473/10000][30/61][5780] Loss_D: -0.089690 Loss_G: 0.053904 Loss_D_real: -0.031628 Loss_D_fake 0.058063\n",
      "[474/10000][61/61][5800] Loss_D: -0.069520 Loss_G: -0.049308 Loss_D_real: -0.066663 Loss_D_fake 0.002857\n",
      "[476/10000][35/61][5820] Loss_D: -0.086311 Loss_G: -0.012133 Loss_D_real: 0.002048 Loss_D_fake 0.088359\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchSize = 64\n",
    "\n",
    "def bonemarrow_cell():\n",
    "    X = np.load(\"/home/msragpu/cellwork/data/data.npy\")\n",
    "    img = X\n",
    "    X = np.asarray([cv2.resize(x, (32,32)) for x in X])\n",
    "    X = np.asarray([x[:,:,::-1].transpose((2,0,1)) for x in X])\n",
    "    X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "    return X\n",
    "\n",
    "X_train = bonemarrow_cell()\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "dataloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize)\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "ngpu = 0\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "n_extra_layers = 0\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG = DCGAN_G()\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = DCGAN_D()\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr = 0.00005)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = 0.00005)\n",
    "\n",
    "gen_iterations = 0\n",
    "for epoch in range(10000):\n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "\n",
    "        # train the discriminator Diters times\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "        else:\n",
    "            Diters = 5\n",
    "        j = 0\n",
    "        while j < Diters and i < len(dataloader):\n",
    "            j += 1\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "\n",
    "            # train with real\n",
    "            real_cpu, _ = data\n",
    "            netD.zero_grad()\n",
    "            batch_size = real_cpu.size(0)\n",
    "\n",
    "            real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "\n",
    "            errD_real = netD(inputv)\n",
    "            errD_real.backward(one)\n",
    "\n",
    "            # train with fake\n",
    "            noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(inputv)\n",
    "            errD_fake.backward(mone)\n",
    "            errD = errD_real - errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(64, 100, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake)\n",
    "        errG.backward(one)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "\n",
    "\n",
    "        if gen_iterations % 20 == 0:\n",
    "            \n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, 10000, i, len(dataloader), gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]))\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, 'real_samples.png')\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, 'fake_samples_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
