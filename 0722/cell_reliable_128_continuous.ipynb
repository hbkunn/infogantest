{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19876, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#pool = ThreadPool(12) \n",
    "root_dir = '/disk1/0721_fullfill/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = []\n",
    "for i in npyList:\n",
    "    result.append(np.load(i))\n",
    "#result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchsize = 10\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=128\n",
    "conti = 128\n",
    "\n",
    "dis=0\n",
    "dis_category = 0\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category+conti)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (linear): Linear (128 -> 1)\n",
      ")\n",
      "_netD_Q (\n",
      "  (linear2): Linear (128 -> 128)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "\n",
    "'''\n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "'''\n",
    "\n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nc = 128):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "        self.linear2 = nn.Linear(128,nc, bias=True)\n",
    "        self.nc = nc\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nc,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "#netD_Q = _netD_Q(dis_category)\n",
    "netD_Q = _netD_Q(conti)\n",
    "\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    #fan_in /= 2.\n",
    "    #fan_out /= 2.\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        #print(\"3:\",m)\n",
    "    else: # Normalized init (Glorot & Bengio)\n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        #print(\"1:\",m)\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        m.weight.data.copy_(weight,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "ld = 1e-4\n",
    "lg = 1e-4\n",
    "lq_d = 1e-4\n",
    "lq_g = 1e-4\n",
    "\n",
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam([\n",
    "                {'params': netG.parameters()},\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()}\n",
    "            ],lg, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ_D = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerQ_G = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "            ], lg, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+conti,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "c_conti = torch.FloatTensor(np.random.uniform(-1,1,(batchsize,conti)))\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c, c_conti = z.cuda(), c.cuda(), c_conti.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "        \n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "def exp_lr_scheduler(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \n",
    "    lr = init_lr* (1.0 - iteration / float(lr_decay_iter))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def exp_lr_scheduler_2(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    if iteration<200000:\n",
    "        lr = init_lr + init_lr*(1*(iteration // 20000))\n",
    "    if iteration>=200000:\n",
    "        lr = init_lr*2 - init_lr*(1*(iteration // 20000))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=dis_category)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.684720993 20 -34.2114219666 40.2236824036\n",
      "14.0530378819 40 33.0138168335 13.7289552689\n",
      "14.0737419128 60 15.3261175156 1.88536906242\n",
      "14.1343419552 80 9.65149307251 4.54908847809\n",
      "14.1402218342 100 12.6510248184 1.04353165627\n",
      "14.2323482037 120 11.1052083969 0.912113845348\n",
      "14.2183270454 140 14.14427948 0.857152581215\n",
      "14.2140500546 160 11.8891162872 1.00184595585\n",
      "14.2651050091 180 7.5283613205 0.571705460548\n",
      "14.4106390476 200 7.38425827026 0.823793232441\n",
      "14.2670650482 220 6.01981019974 0.40208029747\n",
      "14.184098959 240 6.36203432083 0.467750072479\n",
      "14.2642230988 260 5.18885564804 0.616124629974\n",
      "14.2549130917 280 5.30729103088 0.436119377613\n",
      "14.1667790413 300 3.80770206451 0.534380018711\n",
      "14.3389620781 320 3.4978415966 0.529752492905\n",
      "14.2282030582 340 4.67249536514 0.525169253349\n",
      "14.3191399574 360 5.10848045349 0.574217915535\n",
      "14.3653841019 380 4.3252620697 0.562976062298\n",
      "14.1598119736 400 4.28765726089 0.630503475666\n",
      "14.2528970242 420 4.81042861938 0.485614448786\n",
      "14.204447031 440 3.9351079464 0.41338673234\n",
      "14.1513462067 460 6.18239068985 0.819193661213\n",
      "14.1619689465 480 6.20533084869 0.60855281353\n",
      "14.3352510929 500 3.37011671066 0.461472600698\n",
      "14.3826069832 520 5.19695186615 0.419772446156\n",
      "14.3694820404 540 6.95128679276 0.486598789692\n",
      "14.2503838539 560 7.27410554886 0.504221916199\n",
      "14.4404091835 580 1.91812252998 0.445965200663\n",
      "14.1202399731 600 3.26295495033 0.53169709444\n",
      "14.3116509914 620 3.52523827553 0.414582163095\n",
      "14.3395330906 640 4.95626735687 0.41687899828\n",
      "14.4429860115 660 6.12031888962 0.420597225428\n",
      "14.2383079529 680 4.04492282867 0.383052676916\n",
      "14.2230448723 700 2.74498128891 0.441284835339\n",
      "14.173635006 720 5.34011793137 0.411666095257\n",
      "14.2733500004 740 4.93953227997 0.386381775141\n",
      "14.3416039944 760 3.54385209084 0.399355709553\n",
      "14.3295350075 780 4.38712358475 0.419676691294\n",
      "14.2368750572 800 3.08543372154 0.394944936037\n",
      "14.5278360844 820 3.79843783379 0.399491876364\n",
      "14.3324990273 840 2.48809218407 0.484586060047\n",
      "14.3055889606 860 1.92949521542 0.415378332138\n",
      "14.2733421326 880 6.96713399887 0.400449573994\n",
      "14.4848849773 900 5.96639823914 0.425686836243\n",
      "14.1470370293 920 3.63207292557 0.401208102703\n",
      "14.270676136 940 3.43902301788 0.423594623804\n",
      "14.1768770218 960 1.07473242283 0.380323708057\n",
      "14.2104330063 980 3.18057727814 0.365247428417\n",
      "14.4379279613 1000 4.8792848587 0.393608152866\n",
      "14.2787008286 1020 5.41996765137 0.387997090816\n",
      "14.1443510056 1040 3.65033888817 0.388822495937\n",
      "14.1054098606 1060 6.25856018066 0.403860479593\n",
      "14.3581848145 1080 3.5573759079 0.361044496298\n",
      "14.1982388496 1100 3.96640706062 0.376582562923\n",
      "14.3598308563 1120 2.9332075119 0.420189201832\n",
      "14.1666879654 1140 3.9809756279 0.346527099609\n",
      "14.2720050812 1160 3.08425712585 0.417630374432\n",
      "14.1005680561 1180 3.49041318893 0.392835915089\n",
      "14.0175309181 1200 5.20259857178 0.388351291418\n",
      "14.142524004 1220 6.16486644745 0.414229810238\n",
      "14.0639460087 1240 4.86627054214 0.372380197048\n",
      "14.1278860569 1260 1.8507194519 0.380688011646\n",
      "14.2300219536 1280 3.93906450272 0.378700494766\n",
      "14.3239350319 1300 4.64344835281 0.373576909304\n",
      "14.3027410507 1320 4.25913906097 0.393315702677\n",
      "14.3659491539 1340 2.73201799393 0.368533164263\n",
      "14.0910358429 1360 1.27899873257 0.36873203516\n",
      "14.0841429234 1380 4.34731149673 0.371349722147\n",
      "14.1421790123 1400 2.57177019119 0.397964149714\n",
      "14.3393940926 1420 5.70560741425 0.394342362881\n",
      "14.3374519348 1440 3.58589339256 0.373254150152\n",
      "14.450704813 1460 1.14252948761 0.389653265476\n",
      "14.3402729034 1480 2.21683764458 0.392546474934\n",
      "14.3247051239 1500 1.67826545238 0.388947576284\n",
      "14.2232880592 1520 3.98748826981 0.373962253332\n",
      "14.125262022 1540 1.48639857769 0.381546765566\n",
      "14.2473270893 1560 3.75942397118 0.365710824728\n",
      "14.2178609371 1580 4.43834161758 0.379807382822\n",
      "14.0924649239 1600 3.60010075569 0.36780962348\n",
      "14.2291331291 1620 3.8296444416 0.38153013587\n",
      "14.3181118965 1640 2.42127871513 0.392389595509\n",
      "14.2427010536 1660 2.90182948112 0.384549260139\n",
      "14.1630470753 1680 4.53882265091 0.399966657162\n",
      "14.3104419708 1700 4.57205629349 0.373463004827\n",
      "14.2449500561 1720 5.31310129166 0.368962109089\n",
      "14.182320118 1740 5.15416383743 0.394234627485\n",
      "14.1560099125 1760 4.82736349106 0.361447542906\n",
      "14.1850349903 1780 3.34854149818 0.386555343866\n",
      "14.284350872 1800 4.36007452011 0.382812589407\n",
      "14.2096779346 1820 1.81711125374 0.392775774002\n",
      "14.3256709576 1840 4.60783672333 0.355953931808\n",
      "14.1736550331 1860 5.3842086792 0.352991759777\n",
      "14.2794470787 1880 3.28692317009 0.449066638947\n",
      "14.2399659157 1900 3.43253874779 0.362792879343\n",
      "14.2116401196 1920 4.18991470337 0.356660395861\n",
      "14.0395860672 1940 4.26913547516 0.446904480457\n",
      "14.2407729626 1960 4.29198169708 0.435881048441\n",
      "14.1419920921 1980 4.33711099625 0.353390783072\n",
      "14.0132269859 2000 0.0629945397377 0.387744218111\n",
      "14.1173012257 2020 3.38228774071 0.362079560757\n",
      "14.2594208717 2040 3.18651294708 0.402187258005\n",
      "14.2202110291 2060 2.04934835434 0.357662379742\n",
      "14.348954916 2080 4.4646229744 0.356344044209\n",
      "14.3399031162 2100 5.3841586113 0.375218778849\n",
      "14.1559720039 2120 3.28674817085 0.351902574301\n",
      "14.2625091076 2140 2.67583465576 0.378263950348\n",
      "14.2482550144 2160 3.66566061974 0.364284694195\n",
      "14.1699659824 2180 3.92719864845 0.391923516989\n",
      "14.2839939594 2200 1.5793068409 0.36420544982\n",
      "14.2171421051 2220 3.86028313637 0.39044713974\n",
      "14.1349811554 2240 5.80466127396 0.403678804636\n",
      "14.2427420616 2260 1.27316761017 0.381808668375\n",
      "14.2763359547 2280 3.58587265015 0.367387145758\n",
      "14.2033839226 2300 5.90848398209 0.413136661053\n",
      "14.1818950176 2320 6.93866872787 0.385334610939\n",
      "14.3160760403 2340 3.89874577522 0.362678468227\n",
      "14.2762930393 2360 3.99019694328 0.35949793458\n",
      "14.4027018547 2380 3.29635858536 0.374061524868\n",
      "14.035820961 2400 4.80193662643 0.392032057047\n",
      "14.2132129669 2420 3.72983360291 0.353263199329\n",
      "14.1397781372 2440 2.01356053352 0.37726944685\n",
      "14.3567259312 2460 4.52310943604 0.373703539371\n",
      "14.2290759087 2480 3.97503709793 0.364695250988\n",
      "14.1970770359 2500 5.09022521973 0.379923045635\n",
      "14.2158761024 2520 3.47599649429 0.366247594357\n",
      "14.2728760242 2540 4.12227725983 0.363334089518\n",
      "14.2895629406 2560 5.58725643158 0.383378326893\n",
      "14.2884531021 2580 4.03028869629 0.369413286448\n",
      "14.3826389313 2600 3.99482774734 0.400994926691\n",
      "14.2446060181 2620 3.59350371361 0.402319520712\n",
      "14.3317120075 2640 1.60508561134 0.371138036251\n",
      "14.1526451111 2660 6.32827186584 0.396278619766\n",
      "14.2165539265 2680 1.04878973961 0.361770451069\n",
      "14.2897222042 2700 3.65556001663 0.404223531485\n",
      "14.291962862 2720 3.49843001366 0.404226213694\n",
      "14.3185758591 2740 4.0941529274 0.383090436459\n",
      "14.1148769855 2760 2.97873663902 0.430322706699\n",
      "14.0952179432 2780 3.19580054283 0.413055807352\n",
      "14.1050970554 2800 7.66564416885 0.416604816914\n",
      "14.2757759094 2820 5.2729678154 0.367827117443\n",
      "14.222771883 2840 3.53740954399 0.375936806202\n",
      "14.5656750202 2860 0.945348262787 0.369330942631\n",
      "14.2036440372 2880 2.43544077873 0.375836610794\n",
      "14.1936039925 2900 2.02451825142 0.355983614922\n",
      "14.2697861195 2920 4.03029108047 0.369079440832\n",
      "14.1922581196 2940 3.75238418579 0.383006632328\n",
      "14.029211998 2960 7.84705400467 0.433634459972\n",
      "14.2642199993 2980 4.26493692398 0.361796796322\n",
      "14.3252010345 3000 5.70798301697 0.377075523138\n",
      "14.3666529655 3020 5.0235452652 0.410568475723\n",
      "14.2353379726 3040 4.54893398285 0.41800865531\n",
      "14.185888052 3060 5.18618822098 0.385438501835\n",
      "14.2611949444 3080 3.29428911209 0.383268594742\n",
      "14.1747858524 3100 2.80674815178 0.347011476755\n",
      "14.3025181293 3120 0.356738626957 0.393727630377\n",
      "14.2572848797 3140 5.96180534363 0.370199322701\n",
      "14.4303908348 3160 2.67773580551 0.408407509327\n",
      "14.2346889973 3180 5.55056476593 0.37153801322\n",
      "14.0362949371 3200 2.58223199844 0.391531288624\n",
      "14.1856341362 3220 3.3142414093 0.357184976339\n",
      "14.2970690727 3240 1.09392440319 0.37826436758\n",
      "14.174574852 3260 3.48168754578 0.467974334955\n",
      "14.1073288918 3280 1.49491488934 0.372046560049\n",
      "14.1240730286 3300 5.00161600113 0.447582244873\n",
      "14.3566660881 3320 4.73528194427 0.460422843695\n",
      "14.1224560738 3340 3.40818786621 0.383733272552\n",
      "14.3026340008 3360 4.21503067017 0.394930273294\n",
      "14.3693089485 3380 2.95611023903 0.374085217714\n",
      "14.2792677879 3400 4.86588144302 0.378769695759\n",
      "14.2743618488 3420 2.65804409981 0.383152723312\n",
      "14.2627468109 3440 3.759953022 0.403228372335\n",
      "13.9632120132 3460 2.05159330368 0.377564102411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1188139915 3480 2.40317201614 0.400020360947\n",
      "14.3383169174 3500 4.68916797638 0.362593740225\n",
      "14.194163084 3520 5.69502449036 0.380170673132\n",
      "14.1920230389 3540 4.16623210907 0.364924669266\n",
      "14.1206390858 3560 3.85402870178 0.383557081223\n",
      "14.0902700424 3580 4.59667921066 0.368728160858\n",
      "14.0944080353 3600 5.29686641693 0.388652414083\n",
      "14.3440890312 3620 1.20523452759 0.401563793421\n",
      "14.0896811485 3640 3.98382091522 0.371189415455\n",
      "14.1342329979 3660 1.32600080967 0.406156539917\n",
      "14.1779880524 3680 4.5581817627 0.40834531188\n",
      "14.1649320126 3700 5.10113382339 0.367642879486\n",
      "14.3461720943 3720 1.37493550777 0.457036346197\n",
      "14.356981039 3740 2.74655771255 0.405988991261\n",
      "14.1837420464 3760 5.14043188095 0.393229305744\n",
      "14.2193300724 3780 -0.425687968731 0.366875112057\n",
      "14.227077961 3800 2.61721682549 0.404992878437\n",
      "14.1855831146 3820 3.29877543449 0.390870630741\n",
      "14.3697218895 3840 3.58883976936 0.382619708776\n",
      "14.4239828587 3860 5.22522115707 0.37043428421\n",
      "14.3725531101 3880 4.04754972458 0.381680965424\n",
      "14.157763958 3900 4.40190982819 0.42553204298\n",
      "14.1683180332 3920 4.25329828262 0.369424641132\n",
      "14.1024949551 3940 3.52223229408 0.375295341015\n",
      "14.0983300209 3960 2.30598568916 0.407146155834\n",
      "13.8755528927 3980 3.23874402046 0.373813867569\n",
      "14.2538819313 4000 4.76479721069 0.391428798437\n",
      "14.2101540565 4020 4.35071182251 0.393556177616\n",
      "14.1611299515 4040 3.06377029419 0.388765603304\n",
      "14.0677921772 4060 4.24112081528 0.419702619314\n",
      "14.0457220078 4080 3.97004699707 0.431036293507\n",
      "14.2143850327 4100 6.62934112549 0.399559259415\n",
      "14.3021020889 4120 4.680103302 0.393902033567\n",
      "14.1552670002 4140 3.84686946869 0.418077379465\n",
      "14.3593940735 4160 2.28550124168 0.357900202274\n",
      "14.2416241169 4180 6.61129713058 0.368357717991\n",
      "14.2812619209 4200 5.50125694275 0.418516218662\n",
      "14.2107751369 4220 1.62892007828 0.460924327374\n",
      "14.2552289963 4240 4.07337331772 0.422871828079\n",
      "14.1920781136 4260 4.75022125244 0.366558372974\n",
      "14.2655479908 4280 2.18981289864 0.390041440725\n",
      "14.2855451107 4300 1.87916707993 0.41308838129\n",
      "14.2359800339 4320 2.69873642921 0.360700428486\n",
      "14.2478890419 4340 5.11485719681 0.407386362553\n",
      "14.3396799564 4360 2.18731617928 0.379535019398\n",
      "14.1561419964 4380 4.40736961365 0.387728869915\n",
      "14.3285899162 4400 0.601286172867 0.398854911327\n",
      "14.3235750198 4420 0.117078334093 0.365401655436\n",
      "14.2919440269 4440 0.992281794548 0.389683634043\n",
      "14.4310700893 4460 4.99245929718 0.38236528635\n",
      "14.3120951653 4480 6.02729558945 0.367544740438\n",
      "14.4741828442 4500 3.56830596924 0.40132266283\n",
      "14.1390929222 4520 4.29786300659 0.364412158728\n",
      "14.2120900154 4540 6.62734174728 0.413289070129\n",
      "14.31727314 4560 2.90625452995 0.365589082241\n",
      "14.2649519444 4580 4.9131360054 0.360828310251\n",
      "14.2701900005 4600 1.29091918468 0.394019693136\n",
      "14.2062399387 4620 4.29507493973 0.399334043264\n",
      "14.1280329227 4640 2.23918676376 0.373103678226\n",
      "14.2580659389 4660 3.15138483047 0.410318702459\n",
      "14.3893859386 4680 1.98120033741 0.405061900616\n",
      "14.3811058998 4700 3.72744846344 0.378956735134\n",
      "14.3715970516 4720 3.51566243172 0.403688281775\n",
      "14.440112114 4740 3.16854453087 0.405602157116\n",
      "14.263201952 4760 2.10760211945 0.389581143856\n",
      "13.9069280624 4780 2.94335985184 0.415792524815\n",
      "14.2951500416 4800 3.16663742065 0.346134006977\n",
      "14.3003780842 4820 5.75464105606 0.40448102355\n",
      "14.3728580475 4840 2.94434928894 0.390411138535\n",
      "14.3436200619 4860 3.43521666527 0.405972242355\n",
      "14.2749071121 4880 3.84932041168 0.394457399845\n",
      "14.1816530228 4900 1.71266567707 0.380210310221\n",
      "14.2285938263 4920 5.02328872681 0.360212832689\n",
      "14.2236001492 4940 4.79711437225 0.395138323307\n",
      "14.1185178757 4960 1.54788184166 0.387696683407\n",
      "14.2283318043 4980 3.0720744133 0.3920969069\n",
      "14.3528909683 5000 1.11513924599 0.412392437458\n",
      "14.2993240356 5020 4.11683273315 0.452746093273\n",
      "14.1045799255 5040 2.14069414139 0.385635852814\n",
      "14.3950240612 5060 1.19279170036 0.370873868465\n",
      "14.1863651276 5080 2.70423555374 0.404428809881\n",
      "14.0433280468 5100 2.42936682701 0.370134025812\n",
      "14.1466081142 5120 0.929715692997 0.395571410656\n",
      "14.048183918 5140 2.70390558243 0.381280571222\n",
      "14.4044530392 5160 4.11439228058 0.394781768322\n",
      "14.1764919758 5180 6.23027420044 0.385143607855\n",
      "14.3158259392 5200 3.12976551056 0.3745200634\n",
      "14.2361490726 5220 4.5942902565 0.379403203726\n",
      "14.2486000061 5240 2.79832720757 0.370614349842\n",
      "14.2984890938 5260 3.70041537285 0.375283002853\n",
      "14.1583509445 5280 3.41974520683 0.374089956284\n",
      "14.3338191509 5300 3.17982578278 0.366706460714\n",
      "14.2485229969 5320 3.56145191193 0.402766227722\n",
      "14.2927761078 5340 3.66431450844 0.396663725376\n",
      "14.2666518688 5360 -0.151134341955 0.404229164124\n",
      "14.1709070206 5380 4.30525112152 0.404320001602\n",
      "13.945761919 5400 5.94555616379 0.378523170948\n",
      "14.2666740417 5420 3.73380041122 0.383082151413\n",
      "14.1804020405 5440 0.642545938492 0.395361334085\n",
      "14.3763067722 5460 6.03257417679 0.37465763092\n",
      "14.3956148624 5480 1.78460514545 0.360467612743\n",
      "14.1283500195 5500 1.85085642338 0.393107354641\n",
      "14.3363389969 5520 3.94538712502 0.406894445419\n",
      "14.1509530544 5540 3.51298666 0.374205738306\n",
      "14.2898850441 5560 3.68937277794 0.374221324921\n",
      "14.0230939388 5580 0.540204226971 0.45229691267\n",
      "14.3550510406 5600 6.87648105621 0.415983378887\n",
      "14.2893519402 5620 1.32370257378 0.400856435299\n",
      "14.4101538658 5640 3.93129515648 0.427978038788\n",
      "14.0884339809 5660 -1.18164718151 0.374085426331\n",
      "14.3060441017 5680 3.49238085747 0.39218211174\n",
      "14.4015469551 5700 3.64571022987 0.435555785894\n",
      "14.3385310173 5720 6.39637088776 0.433969408274\n",
      "14.2515208721 5740 2.15388631821 0.406120151281\n",
      "14.2076461315 5760 1.86138761044 0.435317516327\n",
      "14.2019300461 5780 3.73544192314 0.386774539948\n",
      "14.1859350204 5800 3.27292537689 0.377608478069\n",
      "14.3251190186 5820 1.17096078396 0.390022814274\n",
      "14.2225267887 5840 3.84216094017 0.349866092205\n",
      "14.1955850124 5860 3.18447804451 0.41752409935\n",
      "14.3336808681 5880 5.25979804993 0.390788853168\n",
      "14.4097299576 5900 5.26282644272 0.379252523184\n",
      "14.3385009766 5920 2.65043568611 0.375451028347\n",
      "14.0911140442 5940 5.57149028778 0.384570837021\n",
      "14.2941329479 5960 1.45012152195 0.37840166688\n",
      "14.040555954 5980 2.01380348206 0.385554999113\n",
      "14.209815979 6000 1.53510451317 0.391613274813\n",
      "14.2124321461 6020 0.466032743454 0.382484287024\n",
      "14.2829868793 6040 6.52962493896 0.411424726248\n",
      "14.1548588276 6060 0.67120039463 0.385542422533\n",
      "14.1302671432 6080 2.53497672081 0.390040785074\n",
      "14.0938050747 6100 4.58068847656 0.42025834322\n",
      "14.1432909966 6120 4.18391561508 0.385490953922\n",
      "14.2313270569 6140 1.53705585003 0.367668956518\n",
      "14.1323170662 6160 -3.37532663345 0.469712018967\n",
      "14.1786289215 6180 6.49554300308 0.410678952932\n",
      "14.3971128464 6200 4.59647369385 0.386755764484\n",
      "14.4530620575 6220 7.71993207932 0.423112392426\n",
      "14.2496960163 6240 2.15324115753 0.39364284277\n",
      "14.2428240776 6260 4.8150472641 0.403169006109\n",
      "14.1441631317 6280 5.04983711243 0.375980734825\n",
      "14.3447759151 6300 0.963631629944 0.388016790152\n",
      "14.1516342163 6320 1.25090789795 0.397556841373\n",
      "14.2722270489 6340 1.86208176613 0.400341510773\n",
      "14.323777914 6360 4.94547748566 0.42496791482\n",
      "14.3395800591 6380 5.31760597229 0.364154338837\n",
      "14.389302969 6400 4.4184679985 0.379421144724\n",
      "14.2097349167 6420 0.930279374123 0.363590240479\n",
      "14.1334500313 6440 3.63736629486 0.385200828314\n",
      "14.263463974 6460 4.72391939163 0.395244687796\n",
      "14.2058229446 6480 2.60940027237 0.417141526937\n",
      "14.3138689995 6500 2.56365418434 0.378403425217\n",
      "14.2603681087 6520 1.87490618229 0.385663717985\n",
      "14.2085368633 6540 0.435887813568 0.396715819836\n",
      "14.4129428864 6560 3.67917919159 0.403218209743\n",
      "14.1551070213 6580 3.82059860229 0.359354525805\n",
      "14.1482331753 6600 4.24447345734 0.371460139751\n",
      "14.1633110046 6620 2.76910424232 0.417523086071\n",
      "14.2711350918 6640 1.42602157593 0.392732799053\n",
      "14.1591880322 6660 4.02163028717 0.375126987696\n",
      "14.153646946 6680 4.05173301697 0.381677091122\n",
      "14.1280431747 6700 4.45135879517 0.384627252817\n",
      "14.2500920296 6720 3.154774189 0.414017528296\n",
      "14.2355239391 6740 1.31351327896 0.390715777874\n",
      "14.2563350201 6760 2.55519294739 0.387902885675\n",
      "14.0902891159 6780 4.13383197784 0.380849421024\n",
      "14.3528621197 6800 2.09915852547 0.401328027248\n",
      "14.1782300472 6820 2.03516745567 0.422613054514\n",
      "14.2925949097 6840 0.815648913383 0.365977466106\n",
      "14.2438898087 6860 5.32936286926 0.40425363183\n",
      "14.3390789032 6880 1.71407425404 0.379250764847\n",
      "14.4196829796 6900 6.5323176384 0.416330397129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.173238039 6920 3.17440080643 0.368225991726\n",
      "14.4742949009 6940 2.28402686119 0.472310215235\n",
      "14.1252760887 6960 4.34739494324 0.415245711803\n",
      "14.2358319759 6980 2.91271924973 0.36258149147\n",
      "14.16943717 7000 3.19623565674 0.437887907028\n",
      "14.1627850533 7020 3.06750011444 0.392213881016\n",
      "14.325248003 7040 2.27063298225 0.375962495804\n",
      "14.3736400604 7060 3.85008978844 0.396343052387\n",
      "14.6579399109 7080 2.24988293648 0.445597469807\n",
      "14.0353507996 7100 2.23043560982 0.400437533855\n",
      "14.3263339996 7120 3.31758260727 0.395610183477\n",
      "14.3192360401 7140 1.15833604336 0.375348627567\n",
      "14.208178997 7160 0.320879161358 0.404474914074\n",
      "14.2441020012 7180 6.07030630112 0.362863063812\n",
      "14.4000840187 7200 0.330970138311 0.409080207348\n",
      "14.2463991642 7220 3.01781487465 0.356991440058\n",
      "14.2228980064 7240 4.97347784042 0.380742669106\n",
      "14.3105139732 7260 3.92171192169 0.388402760029\n",
      "14.1127488613 7280 4.80337762833 0.393530130386\n",
      "14.4100129604 7300 4.8093957901 0.416306823492\n",
      "14.1670248508 7320 3.22554779053 0.359254330397\n",
      "14.3845870495 7340 2.6170630455 0.397370427847\n",
      "14.0428881645 7360 1.48013591766 0.356118649244\n",
      "14.3426399231 7380 3.33765888214 0.443121820688\n",
      "14.5006070137 7400 -1.46593666077 0.400574266911\n",
      "14.3534340858 7420 0.701560199261 0.393227875233\n",
      "14.1762390137 7440 2.38854193687 0.39141663909\n",
      "14.2215540409 7460 4.05715465546 0.405643224716\n",
      "14.4177639484 7480 2.64860320091 0.37893936038\n",
      "14.4424450397 7500 3.19554328918 0.398425042629\n",
      "14.0467810631 7520 2.96531915665 0.355203449726\n",
      "14.2982699871 7540 2.55872249603 0.373969018459\n",
      "14.4613881111 7560 2.13066911697 0.377288341522\n",
      "14.2169780731 7580 1.79689180851 0.407427310944\n",
      "14.3860459328 7600 4.28844118118 0.375396281481\n",
      "14.3180279732 7620 3.18738603592 0.378679513931\n",
      "14.2969539165 7640 3.67497158051 0.354709059\n",
      "14.1558430195 7660 2.0794813633 0.399026781321\n",
      "14.2597391605 7680 4.07740449905 0.470267683268\n",
      "14.4337439537 7700 2.29261827469 0.388445228338\n",
      "14.3746950626 7720 4.3834066391 0.421369016171\n",
      "14.3416600227 7740 2.65633296967 0.379594802856\n",
      "14.1294660568 7760 -0.845344483852 0.391865074635\n",
      "14.044200182 7780 3.08450651169 0.386016845703\n",
      "14.2275390625 7800 4.39495038986 0.376497328281\n",
      "14.2603209019 7820 0.575705170631 0.416313409805\n",
      "14.3636770248 7840 5.80185890198 0.403980791569\n",
      "14.2036440372 7860 1.72114157677 0.398794144392\n",
      "14.3208050728 7880 1.94788384438 0.376989930868\n",
      "14.2688322067 7900 5.86221218109 0.390276491642\n",
      "14.154047966 7920 1.38150882721 0.390230000019\n",
      "14.2791090012 7940 1.49165856838 0.420299202204\n",
      "14.0536880493 7960 0.923193693161 0.356824487448\n",
      "14.5801270008 7980 4.36803436279 0.435840517282\n",
      "14.3273351192 8000 2.25361871719 0.367936462164\n",
      "14.3021581173 8020 4.64130306244 0.378298074007\n",
      "14.1629970074 8040 3.40644311905 0.386935770512\n",
      "14.1576349735 8060 4.04874706268 0.393617451191\n",
      "14.4878399372 8080 3.20186948776 0.405766785145\n",
      "14.2815330029 8100 1.02872467041 0.3666472435\n",
      "14.4533247948 8120 2.31298398972 0.373049676418\n",
      "14.3179249763 8140 4.22835302353 0.38276219368\n",
      "14.1796219349 8160 2.04245471954 0.375008910894\n",
      "14.3456189632 8180 1.76796960831 0.39351952076\n",
      "14.1143529415 8200 3.86170244217 0.3829677701\n",
      "14.3926780224 8220 1.44717991352 0.378599613905\n",
      "14.2606201172 8240 3.40291118622 0.388559788465\n",
      "14.2315330505 8260 0.601430594921 0.415956258774\n",
      "14.2968459129 8280 0.485423266888 0.382851034403\n",
      "14.2272131443 8300 -0.315975904465 0.374775737524\n",
      "14.0483589172 8320 5.00820970535 0.386161088943\n",
      "14.185997963 8340 1.43415951729 0.411535799503\n",
      "14.0630199909 8360 3.76606965065 0.401700019836\n",
      "14.471807003 8380 4.30030250549 0.367160797119\n",
      "14.2891860008 8400 3.26784324646 0.42112916708\n",
      "14.257874012 8420 0.514951169491 0.396860897541\n",
      "14.3374290466 8440 2.60978341103 0.423824310303\n",
      "13.9628968239 8460 4.64721679688 0.396474480629\n",
      "14.1871190071 8480 1.18048202991 0.394610732794\n",
      "14.3495519161 8500 4.48052358627 0.364808022976\n",
      "14.3150541782 8520 3.42585968971 0.379056453705\n",
      "14.3369178772 8540 1.21842193604 0.413257032633\n",
      "14.189896822 8560 0.750001370907 0.365694671869\n",
      "14.2083210945 8580 0.970261871815 0.388105452061\n",
      "14.3463990688 8600 2.44567155838 0.417787730694\n",
      "14.2504010201 8620 2.3291015625 0.378591299057\n",
      "14.1141941547 8640 1.71893715858 0.396150976419\n",
      "14.2372088432 8660 3.64720821381 0.384542286396\n",
      "14.1771910191 8680 4.54468631744 0.381065100431\n",
      "13.9851338863 8700 4.27075052261 0.407024919987\n",
      "14.2250781059 8720 4.99824762344 0.396679103374\n",
      "14.4098689556 8740 0.197073429823 0.369745999575\n",
      "14.3009369373 8760 4.17273950577 0.384158283472\n",
      "14.2120850086 8780 3.08576750755 0.368377268314\n",
      "14.1990599632 8800 2.31932139397 0.391072183847\n",
      "14.4340047836 8820 1.3741568327 0.361816823483\n",
      "14.2751009464 8840 4.89382791519 0.412448227406\n",
      "14.424022913 8860 2.74862527847 0.406035810709\n",
      "14.2524950504 8880 3.61039614677 0.370576322079\n",
      "14.2671918869 8900 2.77681422234 0.377616345882\n",
      "14.1604959965 8920 2.85858941078 0.351118028164\n",
      "14.1979210377 8940 1.63932716846 0.355279505253\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import time\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "cont_lamda = 1\n",
    "end = time.time()\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            #rand_c,label_c = sample_c(_batchsize,dis_category=dis_category)\n",
    "            #rand_c = rand_c.cuda()\n",
    "            #c.resize_as_(rand_c).copy_(rand_c)\n",
    "            c_conti.resize_(_batchsize, conti, 1, 1).uniform_(-1,1)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            #noise = torch.cat([c,z],1)\n",
    "            noise = torch.cat([c_conti,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+conti,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            \n",
    "            Q_c_given_x = netD_Q(netD(fake))\n",
    "            square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "            mi_loss = 1*square_loss\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty+mi_loss\n",
    "\n",
    "            optimizerD.step()\n",
    "\n",
    "            '''\n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample))\n",
    "            square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "            #square_loss = square_loss.mean()\n",
    "            #crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*square_loss\n",
    "            mi_loss.backward()\n",
    "            \n",
    "            optimizerQ_D.step()\n",
    "            optimizerQ_G.step()\n",
    "            '''\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False\n",
    "\n",
    "        zero_grad()\n",
    "        c_conti.resize_(_batchsize, conti, 1, 1).uniform_(-1,1)\n",
    "        z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            #noise = torch.cat([c,z],1)\n",
    "        noise = torch.cat([c_conti,z],1)\n",
    "        noise_resize = noise.view(_batchsize,rand+conti,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone,retain_variables=True)\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True        \n",
    "            \n",
    "        Q_c_given_x = netD_Q(netD(fake)).view(_batchsize, conti)\n",
    "        square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "        mi_loss = 1*square_loss\n",
    "        mi_loss.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        '''\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, conti)\n",
    "        #square_loss = (Q_c_given_x - Variable(c_conti).view(1,-1,1,1))**2*cont_lamda\n",
    "        #square_loss = square_loss.mean()\n",
    "        #crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "        \n",
    "        square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "        mi_loss = 1*square_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ_D.step()\n",
    "        optimizerQ_G.step()\n",
    "        '''\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_white.txt\",\"a\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            #G_sample = netG(Variable(fixed_noise))\n",
    "            #vutils.save_image(G_sample.data, 'fake_cell_white_10.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(fake.data, 'fake_128.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_c_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = (Q_c_given_x - Variable(c_conti).view(1,-1,1,1))**2*cont_lamda\n",
    "square_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueous_noise(rand=128):\n",
    "    fixed_z = np.random.randn(1, rand).repeat(5 ,axis=0)\n",
    "    changing_c = np.numpy([[-2],[-1],[0],[1],[2]], dtype=np.float32)\n",
    "    fixed_c = ([0,0,0,0,0], dtype=np.float32)\n",
    "    list = []\n",
    "    for i in range(0,128):\n",
    "        \n",
    "    #changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    continueous_code = np.zeros((640,128),dtype=int)\n",
    "    for row in range(0, 128):\n",
    "        for row_code in range(0,5):\n",
    "            continueous_code[row_code*128+, ] = \n",
    "            continueous_code[row*128]\n",
    "        \n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
