{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19876, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#pool = ThreadPool(12) \n",
    "root_dir = '/disk1/0721_fullfill/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = []\n",
    "for i in npyList:\n",
    "    result.append(np.load(i))\n",
    "#result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchsize = 10\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=128\n",
    "conti = 128\n",
    "\n",
    "dis=0\n",
    "dis_category = 0\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category+conti)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (linear): Linear (128 -> 1)\n",
      ")\n",
      "_netD_Q (\n",
      "  (linear2): Linear (128 -> 128)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "\n",
    "'''\n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "'''\n",
    "\n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nc = 128):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "        self.linear2 = nn.Linear(128,nc, bias=True)\n",
    "        self.nc = nc\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nc,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "#netD_Q = _netD_Q(dis_category)\n",
    "netD_Q = _netD_Q(conti)\n",
    "\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    #fan_in /= 2.\n",
    "    #fan_out /= 2.\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        #print(\"3:\",m)\n",
    "    else: # Normalized init (Glorot & Bengio)\n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        #print(\"1:\",m)\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        m.weight.data.copy_(weight,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "ld = 1e-4\n",
    "lg = 1e-4\n",
    "lq_d = 1e-4\n",
    "lq_g = 1e-4\n",
    "\n",
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam([\n",
    "                {'params': netG.parameters()},\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()}\n",
    "            ],lg, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ_D = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerQ_G = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "            ], lg, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+conti,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "c_conti = torch.FloatTensor(np.random.uniform(-1,1,(batchsize,conti)))\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c, c_conti = z.cuda(), c.cuda(), c_conti.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "        \n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "def exp_lr_scheduler(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \n",
    "    lr = init_lr* (1.0 - iteration / float(lr_decay_iter))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def exp_lr_scheduler_2(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    if iteration<200000:\n",
    "        lr = init_lr + init_lr*(1*(iteration // 20000))\n",
    "    if iteration>=200000:\n",
    "        lr = init_lr*2 - init_lr*(1*(iteration // 20000))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=dis_category)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.684720993 20 -34.2114219666 40.2236824036\n",
      "14.0530378819 40 33.0138168335 13.7289552689\n",
      "14.0737419128 60 15.3261175156 1.88536906242\n",
      "14.1343419552 80 9.65149307251 4.54908847809\n",
      "14.1402218342 100 12.6510248184 1.04353165627\n",
      "14.2323482037 120 11.1052083969 0.912113845348\n",
      "14.2183270454 140 14.14427948 0.857152581215\n",
      "14.2140500546 160 11.8891162872 1.00184595585\n",
      "14.2651050091 180 7.5283613205 0.571705460548\n",
      "14.4106390476 200 7.38425827026 0.823793232441\n",
      "14.2670650482 220 6.01981019974 0.40208029747\n",
      "14.184098959 240 6.36203432083 0.467750072479\n",
      "14.2642230988 260 5.18885564804 0.616124629974\n",
      "14.2549130917 280 5.30729103088 0.436119377613\n",
      "14.1667790413 300 3.80770206451 0.534380018711\n",
      "14.3389620781 320 3.4978415966 0.529752492905\n",
      "14.2282030582 340 4.67249536514 0.525169253349\n",
      "14.3191399574 360 5.10848045349 0.574217915535\n",
      "14.3653841019 380 4.3252620697 0.562976062298\n",
      "14.1598119736 400 4.28765726089 0.630503475666\n",
      "14.2528970242 420 4.81042861938 0.485614448786\n",
      "14.204447031 440 3.9351079464 0.41338673234\n",
      "14.1513462067 460 6.18239068985 0.819193661213\n",
      "14.1619689465 480 6.20533084869 0.60855281353\n",
      "14.3352510929 500 3.37011671066 0.461472600698\n",
      "14.3826069832 520 5.19695186615 0.419772446156\n",
      "14.3694820404 540 6.95128679276 0.486598789692\n",
      "14.2503838539 560 7.27410554886 0.504221916199\n",
      "14.4404091835 580 1.91812252998 0.445965200663\n",
      "14.1202399731 600 3.26295495033 0.53169709444\n",
      "14.3116509914 620 3.52523827553 0.414582163095\n",
      "14.3395330906 640 4.95626735687 0.41687899828\n",
      "14.4429860115 660 6.12031888962 0.420597225428\n",
      "14.2383079529 680 4.04492282867 0.383052676916\n",
      "14.2230448723 700 2.74498128891 0.441284835339\n",
      "14.173635006 720 5.34011793137 0.411666095257\n",
      "14.2733500004 740 4.93953227997 0.386381775141\n",
      "14.3416039944 760 3.54385209084 0.399355709553\n",
      "14.3295350075 780 4.38712358475 0.419676691294\n",
      "14.2368750572 800 3.08543372154 0.394944936037\n",
      "14.5278360844 820 3.79843783379 0.399491876364\n",
      "14.3324990273 840 2.48809218407 0.484586060047\n",
      "14.3055889606 860 1.92949521542 0.415378332138\n",
      "14.2733421326 880 6.96713399887 0.400449573994\n",
      "14.4848849773 900 5.96639823914 0.425686836243\n",
      "14.1470370293 920 3.63207292557 0.401208102703\n",
      "14.270676136 940 3.43902301788 0.423594623804\n",
      "14.1768770218 960 1.07473242283 0.380323708057\n",
      "14.2104330063 980 3.18057727814 0.365247428417\n",
      "14.4379279613 1000 4.8792848587 0.393608152866\n",
      "14.2787008286 1020 5.41996765137 0.387997090816\n",
      "14.1443510056 1040 3.65033888817 0.388822495937\n",
      "14.1054098606 1060 6.25856018066 0.403860479593\n",
      "14.3581848145 1080 3.5573759079 0.361044496298\n",
      "14.1982388496 1100 3.96640706062 0.376582562923\n",
      "14.3598308563 1120 2.9332075119 0.420189201832\n",
      "14.1666879654 1140 3.9809756279 0.346527099609\n",
      "14.2720050812 1160 3.08425712585 0.417630374432\n",
      "14.1005680561 1180 3.49041318893 0.392835915089\n",
      "14.0175309181 1200 5.20259857178 0.388351291418\n",
      "14.142524004 1220 6.16486644745 0.414229810238\n",
      "14.0639460087 1240 4.86627054214 0.372380197048\n",
      "14.1278860569 1260 1.8507194519 0.380688011646\n",
      "14.2300219536 1280 3.93906450272 0.378700494766\n",
      "14.3239350319 1300 4.64344835281 0.373576909304\n",
      "14.3027410507 1320 4.25913906097 0.393315702677\n",
      "14.3659491539 1340 2.73201799393 0.368533164263\n",
      "14.0910358429 1360 1.27899873257 0.36873203516\n",
      "14.0841429234 1380 4.34731149673 0.371349722147\n",
      "14.1421790123 1400 2.57177019119 0.397964149714\n",
      "14.3393940926 1420 5.70560741425 0.394342362881\n",
      "14.3374519348 1440 3.58589339256 0.373254150152\n",
      "14.450704813 1460 1.14252948761 0.389653265476\n",
      "14.3402729034 1480 2.21683764458 0.392546474934\n",
      "14.3247051239 1500 1.67826545238 0.388947576284\n",
      "14.2232880592 1520 3.98748826981 0.373962253332\n",
      "14.125262022 1540 1.48639857769 0.381546765566\n",
      "14.2473270893 1560 3.75942397118 0.365710824728\n",
      "14.2178609371 1580 4.43834161758 0.379807382822\n",
      "14.0924649239 1600 3.60010075569 0.36780962348\n",
      "14.2291331291 1620 3.8296444416 0.38153013587\n",
      "14.3181118965 1640 2.42127871513 0.392389595509\n",
      "14.2427010536 1660 2.90182948112 0.384549260139\n",
      "14.1630470753 1680 4.53882265091 0.399966657162\n",
      "14.3104419708 1700 4.57205629349 0.373463004827\n",
      "14.2449500561 1720 5.31310129166 0.368962109089\n",
      "14.182320118 1740 5.15416383743 0.394234627485\n",
      "14.1560099125 1760 4.82736349106 0.361447542906\n",
      "14.1850349903 1780 3.34854149818 0.386555343866\n",
      "14.284350872 1800 4.36007452011 0.382812589407\n",
      "14.2096779346 1820 1.81711125374 0.392775774002\n",
      "14.3256709576 1840 4.60783672333 0.355953931808\n",
      "14.1736550331 1860 5.3842086792 0.352991759777\n",
      "14.2794470787 1880 3.28692317009 0.449066638947\n",
      "14.2399659157 1900 3.43253874779 0.362792879343\n",
      "14.2116401196 1920 4.18991470337 0.356660395861\n",
      "14.0395860672 1940 4.26913547516 0.446904480457\n",
      "14.2407729626 1960 4.29198169708 0.435881048441\n",
      "14.1419920921 1980 4.33711099625 0.353390783072\n",
      "14.0132269859 2000 0.0629945397377 0.387744218111\n",
      "14.1173012257 2020 3.38228774071 0.362079560757\n",
      "14.2594208717 2040 3.18651294708 0.402187258005\n",
      "14.2202110291 2060 2.04934835434 0.357662379742\n",
      "14.348954916 2080 4.4646229744 0.356344044209\n",
      "14.3399031162 2100 5.3841586113 0.375218778849\n",
      "14.1559720039 2120 3.28674817085 0.351902574301\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import time\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "cont_lamda = 1\n",
    "end = time.time()\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            #rand_c,label_c = sample_c(_batchsize,dis_category=dis_category)\n",
    "            #rand_c = rand_c.cuda()\n",
    "            #c.resize_as_(rand_c).copy_(rand_c)\n",
    "            c_conti.resize_(_batchsize, conti, 1, 1).uniform_(-1,1)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            #noise = torch.cat([c,z],1)\n",
    "            noise = torch.cat([c_conti,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+conti,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            \n",
    "            Q_c_given_x = netD_Q(netD(fake))\n",
    "            square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "            mi_loss = 1*square_loss\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty+mi_loss\n",
    "\n",
    "            optimizerD.step()\n",
    "\n",
    "            '''\n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample))\n",
    "            square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "            #square_loss = square_loss.mean()\n",
    "            #crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*square_loss\n",
    "            mi_loss.backward()\n",
    "            \n",
    "            optimizerQ_D.step()\n",
    "            optimizerQ_G.step()\n",
    "            '''\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False\n",
    "\n",
    "        zero_grad()\n",
    "        c_conti.resize_(_batchsize, conti, 1, 1).uniform_(-1,1)\n",
    "        z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            #noise = torch.cat([c,z],1)\n",
    "        noise = torch.cat([c_conti,z],1)\n",
    "        noise_resize = noise.view(_batchsize,rand+conti,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone,retain_variables=True)\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True        \n",
    "            \n",
    "        Q_c_given_x = netD_Q(netD(fake)).view(_batchsize, conti)\n",
    "        square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "        mi_loss = 1*square_loss\n",
    "        mi_loss.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        '''\n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, conti)\n",
    "        #square_loss = (Q_c_given_x - Variable(c_conti).view(1,-1,1,1))**2*cont_lamda\n",
    "        #square_loss = square_loss.mean()\n",
    "        #crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "        \n",
    "        square_loss = criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False))\n",
    "        mi_loss = 1*square_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ_D.step()\n",
    "        optimizerQ_G.step()\n",
    "        '''\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_white.txt\",\"a\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            #G_sample = netG(Variable(fixed_noise))\n",
    "            #vutils.save_image(G_sample.data, 'fake_cell_white_10.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(fake.data, 'fake_128.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_c_given_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = (Q_c_given_x - Variable(c_conti).view(1,-1,1,1))**2*cont_lamda\n",
    "square_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_mse(Q_c_given_x, Variable(c_conti,requires_grad=False)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randn(128, rand).repeat(5,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def continueous_noise(rand=128):\n",
    "    fixed_z = np.random.randn(128, rand).repeat(5,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
