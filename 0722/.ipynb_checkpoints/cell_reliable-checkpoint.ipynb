{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19876, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk1/0721_fullfill/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchsize = 10\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=128\n",
    "dis=1\n",
    "dis_category = 10\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (linear): Linear (128 -> 1)\n",
      ")\n",
      "_netD_Q (\n",
      "  (softmax): LogSoftmax ()\n",
      "  (linear2): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)\n",
    "\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    #fan_in /= 2.\n",
    "    #fan_out /= 2.\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        #print(\"3:\",m)\n",
    "    else: # Normalized init (Glorot & Bengio)\n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        #print(\"1:\",m)\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        m.weight.data.copy_(weight,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "ld = 1e-4\n",
    "lg = 1e-4\n",
    "lq_d = 1e-4\n",
    "lq_g = 1e-4\n",
    "\n",
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lg, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ_D = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerQ_G = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "            ], lg, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "        \n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "def exp_lr_scheduler(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \n",
    "    lr = init_lr* (1.0 - iteration / float(lr_decay_iter))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def exp_lr_scheduler_2(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    if iteration<200000:\n",
    "        lr = init_lr + init_lr*(1*(iteration // 20000))\n",
    "    if iteration>=200000:\n",
    "        lr = init_lr*2 - init_lr*(1*(iteration // 20000))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=dis_category)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4217579365 20 109.08253479 2.26347327232\n",
      "18.0198299885 40 126.572555542 2.44957637787\n",
      "18.034168005 60 92.9622497559 2.67163729668\n",
      "18.0835030079 80 93.4988250732 1.79928684235\n",
      "18.0418679714 100 85.113319397 2.7561955452\n",
      "18.219397068 120 62.9561576843 2.78754425049\n",
      "18.0527999401 140 65.1598358154 1.48305857182\n",
      "18.1670889854 160 62.7607421875 1.26280617714\n",
      "18.1049778461 180 64.7311401367 1.94963896275\n",
      "18.2591640949 200 59.5311927795 0.737520098686\n",
      "18.1976029873 220 51.1513900757 3.12996196747\n",
      "18.1315560341 240 34.3710250854 2.17595124245\n",
      "18.2139902115 260 47.2888069153 1.12827908993\n",
      "18.1831839085 280 33.0216026306 0.893040657043\n",
      "18.1091291904 300 42.3010063171 1.44527924061\n",
      "18.177834034 320 37.9499435425 1.02815198898\n",
      "18.1042919159 340 32.2951049805 0.924679279327\n",
      "18.2488121986 360 35.7753601074 0.510048270226\n",
      "18.38656497 380 21.7019481659 2.36335420609\n",
      "18.1439700127 400 23.7754364014 0.942422866821\n",
      "18.1655440331 420 36.874294281 0.596853375435\n",
      "18.1186289787 440 17.3130817413 0.536906599998\n",
      "18.2785010338 460 23.6346702576 0.536442875862\n",
      "18.4313309193 480 23.8342494965 0.248156741261\n",
      "18.2812271118 500 16.1756839752 0.10110130161\n",
      "18.3841240406 520 16.25050354 0.731024622917\n",
      "18.1366701126 540 17.6496047974 0.527193188667\n",
      "18.1483819485 560 24.7947731018 0.869305431843\n",
      "18.1813201904 580 14.6744031906 0.0739932060242\n",
      "18.2512149811 600 20.1540260315 0.517689824104\n",
      "18.2184009552 620 18.2728538513 0.287353843451\n",
      "18.1836590767 640 15.7899560928 0.126534491777\n",
      "18.3081271648 660 16.3119926453 0.0321600921452\n",
      "18.207351923 680 21.511182785 0.0120789054781\n",
      "18.2432541847 700 16.4846973419 0.206502079964\n",
      "18.2423388958 720 17.488822937 0.229295164347\n",
      "18.1312150955 740 10.1157407761 0.00813441257924\n",
      "18.2279729843 760 18.7503738403 0.305819571018\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import time\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "end = time.time()\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize,dis_category=dis_category)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+dis_category*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, dis_category)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ_D.step()\n",
    "            optimizerQ_G.step()\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize,dis_category=dis_category)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+dis_category*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, dis_category)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ_D.step()\n",
    "        optimizerQ_G.step()\n",
    "\n",
    "        if gen_iterations % 20 == 0 :\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_white.txt\",\"a\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_white_10.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_cell_10.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
