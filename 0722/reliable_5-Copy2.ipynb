{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "root_dir = '/disk1/labeled/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = []\n",
    "label = []\n",
    "for n,array in enumerate(npyList):\n",
    "    result.append(np.load(array))\n",
    "    label.append([n]*result[n].shape[0])\n",
    "    \n",
    "result = np.concatenate(result)\n",
    "label = np.concatenate(label)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "\n",
    "X_train = torch.FloatTensor(X)\n",
    "X_label = torch.LongTensor(label)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=128\n",
    "dis=1\n",
    "dis_category = 5\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netG.load_state_dict(torch.load('./params/tumor_netG_epoch_162.pth'))\n",
    "netD.load_state_dict(torch.load('./params/tumor_netD_epoch_162.pth'))\n",
    "netD_D.load_state_dict(torch.load('./params/tumor_netD_D_epoch_162.pth'))\n",
    "netD_Q.load_state_dict(torch.load('./params/tumor_netD_Q_epoch_162.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "predict = []\n",
    "netD = netD.cuda()\n",
    "netD_Q = netD_Q.cuda()\n",
    "\n",
    "for iteration in data_iter:\n",
    "    img, img_label = iteration\n",
    "    predict_label = netD_Q(netD(Variable(img.cuda())))\n",
    "    predict.append(predict_label.data.cpu().numpy())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = []\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    predict_label.append(np.argmax(predict[n]))\n",
    "    \n",
    "coherent_array = np.zeros((5,5),dtype=int)\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    coherent_array[label[n],predict_label[n]] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coherent_array must be 5*5\n",
    "def cout_auc(coherent_array):\n",
    "    Accuracy =[]\n",
    "    Position=[]\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            for m in range(0,5):\n",
    "                lis=[0,1,2,3,4]\n",
    "                if i!=j and i!=m and m!=j:\n",
    "                    lis.remove(i)\n",
    "                    lis.remove(j)\n",
    "                    lis.remove(m)\n",
    "                    a=lis[0]\n",
    "                    b=lis[1]\n",
    "                    accuracy = float(coherent_array[0][i]+coherent_array[1][j]+coherent_array[2][m]+coherent_array[3][a]+coherent_array[3][b])/coherent_array.sum()\n",
    "                    position = [i,j,m,a,b]\n",
    "                    Accuracy.append(accuracy)\n",
    "                    Position.append(position)\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            for m in range(0,5):\n",
    "                lis=[0,1,2,3,4]\n",
    "                if i!=j and i!=m and m!=j:\n",
    "                    lis.remove(i)\n",
    "                    lis.remove(j)\n",
    "                    lis.remove(m)\n",
    "                    a=lis[0]\n",
    "                    b=lis[1]\n",
    "                    accuracy = float(coherent_array[0][i]+coherent_array[1][j]+coherent_array[3][m]+coherent_array[2][a]+coherent_array[2][b])/coherent_array.sum()\n",
    "                    position = [i,j,m,a,b]\n",
    "                    Accuracy.append(accuracy)\n",
    "                    Position.append(position)\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            for m in range(0,5):\n",
    "                lis=[0,1,2,3,4]\n",
    "                if i!=j and i!=m and m!=j:\n",
    "                    lis.remove(i)\n",
    "                    lis.remove(j)\n",
    "                    lis.remove(m)\n",
    "                    a=lis[0]\n",
    "                    b=lis[1]\n",
    "                    accuracy = float(coherent_array[0][i]+coherent_array[3][j]+coherent_array[2][m]+coherent_array[1][a]+coherent_array[1][b])/coherent_array.sum()\n",
    "                    position = [i,j,m,a,b]\n",
    "                    Accuracy.append(accuracy)\n",
    "                    Position.append(position)\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            for m in range(0,5):\n",
    "                lis=[0,1,2,3,4]\n",
    "                if i!=j and i!=m and m!=j:\n",
    "                    lis.remove(i)\n",
    "                    lis.remove(j)\n",
    "                    lis.remove(m)\n",
    "                    a=lis[0]\n",
    "                    b=lis[1]\n",
    "                    accuracy = float(coherent_array[0][i]+coherent_array[3][j]+coherent_array[2][m]+coherent_array[0][a]+coherent_array[0][b])/coherent_array.sum()\n",
    "                    position = [i,j,m,a,b]\n",
    "                    Accuracy.append(accuracy)\n",
    "                    Position.append(position)\n",
    "    auc = max(Accuracy)\n",
    "    p=Accuracy.index(auc)\n",
    "    pos = Position[p]\n",
    "    print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cout_auc(coherent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clustering(X,label):\n",
    "    X_train = torch.FloatTensor(X)\n",
    "    X_label = torch.LongTensor(label)\n",
    "    train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "    train_loader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=1)\n",
    "    \n",
    "    rand=128\n",
    "    dis=1\n",
    "    dis_category = 5\n",
    "\n",
    "    class avgpool(nn.Module):\n",
    "        def __init__(self, up_size=0):\n",
    "            super(avgpool, self).__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "            return out_man\n",
    "\n",
    "    class ResidualBlock(nn.Module):\n",
    "\n",
    "        def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "            super(ResidualBlock, self).__init__()\n",
    "\n",
    "            if resample == 'up':\n",
    "                self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "                self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "                self.upsample = torch.nn.Upsample(up_size,2)\n",
    "                self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "                self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "                self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "\n",
    "            elif resample == 'down':\n",
    "                self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "                self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "                self.pool = avgpool()\n",
    "                self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "\n",
    "            elif resample == None:\n",
    "                self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "                self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "\n",
    "            self.resample = resample\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            if self.resample == None:\n",
    "                shortcut = x\n",
    "                output = x\n",
    "\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.conv1(output)\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.conv2(output)\n",
    "\n",
    "            elif self.resample == 'up':\n",
    "                shortcut = x\n",
    "                output = x\n",
    "\n",
    "                shortcut = self.upsample(shortcut) #upsampleconv\n",
    "                shortcut = self.upsample_conv(shortcut)\n",
    "\n",
    "                output = self.bn1(output)\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.conv1(output)\n",
    "\n",
    "                output = self.bn2(output)\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.upsample(output) #upsampleconv\n",
    "                output = self.conv2(output)\n",
    "\n",
    "            elif self.resample == 'down':\n",
    "                shortcut = x\n",
    "                output = x\n",
    "\n",
    "                shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "                shortcut = self.pool(shortcut)\n",
    "\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.conv1(output)\n",
    "\n",
    "                output = nn.functional.relu(output)\n",
    "                output = self.conv2(output)    #convmeanpool\n",
    "                output = self.pool(output)\n",
    "\n",
    "            return output+shortcut\n",
    "\n",
    "    class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "        def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "            super(ResidualBlock_thefirstone, self).__init__()\n",
    "\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            shortcut = x\n",
    "            output = x\n",
    "\n",
    "            shortcut = self.pool(shortcut) #meanpoolconv\n",
    "            shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output) #convmeanpool\n",
    "            output = self.pool(output)\n",
    "\n",
    "            return output+shortcut\n",
    "\n",
    "\n",
    "    class generator(nn.Module):\n",
    "\n",
    "        def __init__(self, rand=128):\n",
    "            super(generator, self).__init__()\n",
    "            self.rand = rand\n",
    "            self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "            self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "            self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "            self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "            self.bn1 = nn.BatchNorm2d(128)\n",
    "            self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(-1,self.rand)\n",
    "            x = self.linear(x)\n",
    "            x = x.view(-1,128,4,4)\n",
    "            x = self.layer_up_1(x)\n",
    "            x = self.layer_up_2(x)\n",
    "            x = self.layer_up_3(x)\n",
    "            x = self.bn1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.conv_last(x)\n",
    "            x = nn.functional.tanh(x)\n",
    "            return x\n",
    "\n",
    "    netG = generator(rand = rand+dis*dis_category)\n",
    "\n",
    "    class discriminator(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(discriminator, self).__init__()\n",
    "            self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "            self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "            self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "            self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "            #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "            #self.linear = nn.Linear(128,1, bias=True)\n",
    "            #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.layer_down_1(x)\n",
    "            x = self.layer_down_2(x)\n",
    "            x = self.layer_none_1(x)\n",
    "            x = self.layer_none_2(x)\n",
    "            #x = self.mean_pool(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = x.mean(2).mean(2)\n",
    "            x = x.view(-1, 128)\n",
    "\n",
    "            #shortcut = x\n",
    "            #output = x\n",
    "\n",
    "            #output = self.linear(output)\n",
    "            #shortcut= self.linear2(shortcut)\n",
    "\n",
    "            #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "            return x\n",
    "\n",
    "    netD = discriminator()\n",
    "\n",
    "#torch.cuda.set_device(1)\n",
    "\n",
    "    class _netD_D(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(_netD_D, self).__init__()\n",
    "            self.linear = nn.Linear(128,1, bias=True)\n",
    "            #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.linear(x)\n",
    "            return x.view(-1,1,1,1)\n",
    "\n",
    "    class _netD_Q(nn.Module):\n",
    "        def __init__(self, nd = 10):\n",
    "            super(_netD_Q, self).__init__()\n",
    "            # input is Z, going into a convolution\n",
    "            #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "            #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "            #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "            self.softmax = nn.LogSoftmax()\n",
    "            #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "            #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "            self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "            self.nd = nd\n",
    "\n",
    "        def forward(self, x):\n",
    "            #x = self.linear1(x)\n",
    "            #x = self.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = self.softmax(x)\n",
    "           # x = x.view(64,10)\n",
    "            return x.view(-1,self.nd,1,1)\n",
    "\n",
    "    netD_D = _netD_D()\n",
    "    netD_Q = _netD_Q(dis_category)\n",
    "    \n",
    "    netG.load_state_dict(torch.load('./params/tumor_netG_epoch_162.pth'))\n",
    "    netD.load_state_dict(torch.load('./params/tumor_netD_epoch_162.pth'))\n",
    "    netD_D.load_state_dict(torch.load('./params/tumor_netD_D_epoch_162.pth'))\n",
    "    netD_Q.load_state_dict(torch.load('./params/tumor_netD_Q_epoch_162.pth'))\n",
    "    \n",
    "    data_iter = iter(train_loader)\n",
    "    predict = []\n",
    "    netD = netD.cuda()\n",
    "    netD_Q = netD_Q.cuda()\n",
    "\n",
    "    for iteration in data_iter:\n",
    "        img, img_label = iteration\n",
    "        predict_label = netD_Q(netD(Variable(img.cuda())))\n",
    "        predict.append(predict_label.data.cpu().numpy())    \n",
    "        \n",
    "    predict_label = []\n",
    "\n",
    "    for n in range(0, len(predict)):\n",
    "        predict_label.append(np.argmax(predict[n]))\n",
    "\n",
    "    coherent_array = np.zeros((5,5),dtype=int)\n",
    "\n",
    "    for n in range(0, len(predict)):\n",
    "        coherent_array[label[n],predict_label[n]] +=1\n",
    "        \n",
    "    cout_auc(coherent_array)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(X,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
