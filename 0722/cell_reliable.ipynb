{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371165, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk2/0724_cell/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "'''\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "'''\n",
    "#result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=74\n",
    "dis=1\n",
    "dis_category = 10\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (linear): Linear (128 -> 1)\n",
      ")\n",
      "_netD_Q (\n",
      "  (softmax): LogSoftmax ()\n",
      "  (linear2): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)\n",
    "\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    #fan_in /= 2.\n",
    "    #fan_out /= 2.\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        #print(\"3:\",m)\n",
    "    else: # Normalized init (Glorot & Bengio)\n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        #print(\"1:\",m)\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        m.weight.data.copy_(weight,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "ld = 1e-4\n",
    "lg = 1e-4\n",
    "lq_d = 1e-4\n",
    "lq_g = 1e-4\n",
    "\n",
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lg, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ_D = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerQ_G = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "            ], lg, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "        \n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "def exp_lr_scheduler(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \n",
    "    lr = init_lr* (1.0 - iteration / float(lr_decay_iter))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def exp_lr_scheduler_2(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    if iteration<200000:\n",
    "        lr = init_lr + init_lr*(1*(iteration // 20000))\n",
    "    if iteration>=200000:\n",
    "        lr = init_lr*2 - init_lr*(1*(iteration // 20000))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(rand=rand, dis_category=dis_category)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.554792166 20 102.39881897 2.39082670212\n",
      "141.252496004 40 63.7488136292 1.83391392231\n",
      "140.933955193 60 59.4947013855 2.06437563896\n",
      "141.106158972 80 59.1440925598 0.494521915913\n",
      "140.798007965 100 62.1015625 0.0710024386644\n",
      "139.888157129 120 49.1749191284 0.159449771047\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import time\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "end = time.time()\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize,dis_category=dis_category)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+dis_category*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, dis_category)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ_D.step()\n",
    "            optimizerQ_G.step()\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize,dis_category=dis_category)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+dis_category*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, dis_category)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ_D.step()\n",
    "        optimizerQ_G.step()\n",
    "\n",
    "        if gen_iterations % 20 == 0 :\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_white.txt\",\"a\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_white_10.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_cell_10.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netD.state_dict(), './params/tumor_netD_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netG.state_dict(), './params/tumor_netG_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_D.state_dict(), './params/tumor_netD_D_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_Q.state_dict(), './params/tumor_netD_Q_10_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/disk1/labeled/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = []\n",
    "label = []\n",
    "for n,array in enumerate(npyList):\n",
    "    result.append(np.load(array))\n",
    "    label.append([n]*result[n].shape[0])\n",
    "    \n",
    "result = np.concatenate(result)\n",
    "label = np.concatenate(label)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "\n",
    "X_train = torch.FloatTensor(X)\n",
    "X_label = torch.LongTensor(label)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=1)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "predict = []\n",
    "netD = netD.cuda()\n",
    "netD_Q = netD_Q.cuda()\n",
    "\n",
    "for iteration in data_iter:\n",
    "    img, img_label = iteration\n",
    "    predict_label = netD_Q(netD(Variable(img.cuda())))\n",
    "    predict.append(predict_label.data.cpu().numpy())    \n",
    "    \n",
    "predict_label = []\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    predict_label.append(np.argmax(predict[n]))\n",
    "    \n",
    "coherent_array = np.zeros((4,10),dtype=int)\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    coherent_array[label[n],predict_label[n]] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for i in range(0,10):\n",
    "    sum += np.max(coherent_array[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
