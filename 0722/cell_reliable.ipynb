{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19876, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk1/0721_fullfill/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "batchsize = 10\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand=128\n",
    "dis=1\n",
    "dis_category = 10\n",
    "\n",
    "class avgpool(nn.Module):\n",
    "    def __init__(self, up_size=0):\n",
    "        super(avgpool, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_man = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4\n",
    "        return out_man\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        if resample == 'up':\n",
    "            self.bn1 = nn.BatchNorm2d(in_dim)\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.upsample = torch.nn.Upsample(up_size,2)\n",
    "            self.upsample_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "            \n",
    "        elif resample == 'down':\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.pool = avgpool()\n",
    "            self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "        elif resample == None:\n",
    "            self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "            \n",
    "        self.resample = resample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.resample == None:\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)\n",
    "            \n",
    "        elif self.resample == 'up':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.upsample(shortcut) #upsampleconv\n",
    "            shortcut = self.upsample_conv(shortcut)\n",
    "            \n",
    "            output = self.bn1(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "\n",
    "            output = self.bn2(output)\n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.upsample(output) #upsampleconv\n",
    "            output = self.conv2(output)\n",
    "                        \n",
    "        elif self.resample == 'down':\n",
    "            shortcut = x\n",
    "            output = x\n",
    "            \n",
    "            shortcut = self.pool_conv(shortcut) #convmeanpool\n",
    "            shortcut = self.pool(shortcut)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv1(output)\n",
    "            \n",
    "            output = nn.functional.relu(output)\n",
    "            output = self.conv2(output)    #convmeanpool\n",
    "            output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "class ResidualBlock_thefirstone(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, resample=None, up_size=0):\n",
    "        super(ResidualBlock_thefirstone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, 3, 1, 1, bias=True)\n",
    "        self.pool = avgpool()\n",
    "        self.pool_conv = nn.Conv2d(in_dim, out_dim, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x\n",
    "        output = x\n",
    "        \n",
    "        shortcut = self.pool(shortcut) #meanpoolconv\n",
    "        shortcut = self.pool_conv(shortcut)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = nn.functional.relu(output)\n",
    "        output = self.conv2(output) #convmeanpool\n",
    "        output = self.pool(output)\n",
    "            \n",
    "        return output+shortcut\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "\n",
    "    def __init__(self, rand=128):\n",
    "        super(generator, self).__init__()\n",
    "        self.rand = rand\n",
    "        self.linear = nn.Linear(rand  ,2048, bias=True)\n",
    "        self.layer_up_1 = ResidualBlock(128, 128, 'up', up_size=8)\n",
    "        self.layer_up_2 = ResidualBlock(128, 128, 'up', up_size=16)\n",
    "        self.layer_up_3 = ResidualBlock(128, 128, 'up', up_size=32)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv_last = nn.Conv2d(128, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.rand)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1,128,4,4)\n",
    "        x = self.layer_up_1(x)\n",
    "        x = self.layer_up_2(x)\n",
    "        x = self.layer_up_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "        return x\n",
    "    \n",
    "netG = generator(rand = rand+dis*dis_category)\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.layer_down_1 = ResidualBlock_thefirstone(3, 128)\n",
    "        self.layer_down_2 = ResidualBlock(128, 128, 'down')\n",
    "        self.layer_none_1 = ResidualBlock(128, 128, None)\n",
    "        self.layer_none_2 = ResidualBlock(128, 128, None)\n",
    "        #self.mean_pool = nn.AvgPool2d(8,1,0)\n",
    "        #self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.linear2 = nn.Linear(128,10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_down_1(x)\n",
    "        x = self.layer_down_2(x)\n",
    "        x = self.layer_none_1(x)\n",
    "        x = self.layer_none_2(x)\n",
    "        #x = self.mean_pool(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.mean(2).mean(2)\n",
    "        x = x.view(-1, 128)\n",
    "\n",
    "        #shortcut = x\n",
    "        #output = x\n",
    "        \n",
    "        #output = self.linear(output)\n",
    "        #shortcut= self.linear2(shortcut)\n",
    "        \n",
    "        #return output.view(-1,1,1,1), shortcut.view(-1,10,1,1)\n",
    "        return x\n",
    "    \n",
    "netD = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (linear): Linear (128 -> 1)\n",
      ")\n",
      "_netD_Q (\n",
      "  (softmax): LogSoftmax ()\n",
      "  (linear2): Linear (128 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.linear = nn.Linear(128,1, bias=True)\n",
    "        #self.conv = nn.Conv2d(4096, 1, 1, 1, 0, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x.view(-1,1,1,1)\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        #self.conv = nn.Conv2d(4096, 128, 1, 1, 0, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        #self.conv2 = nn.Conv2d(128, nd, 1, 1, 0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        #self.linear1 = nn.Linear(4096,128, bias=True)\n",
    "        #self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.linear2 = nn.Linear(128,nd, bias=True)\n",
    "        self.nd = nd\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x.view(-1,self.nd,1,1)\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)\n",
    "\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform(stdev, size):\n",
    "    return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "def initialize_conv(m,he_init=True):\n",
    "    fan_in = m.in_channels * m.kernel_size[0]**2\n",
    "    fan_out = m.out_channels * m.kernel_size[0]**2 / (m.stride[0]**2)\n",
    "\n",
    "    #fan_in /= 2.\n",
    "    #fan_out /= 2.\n",
    "\n",
    "    if m.kernel_size[0]==3:\n",
    "        filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        #print(\"3:\",m)\n",
    "    else: # Normalized init (Glorot & Bengio)\n",
    "        filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "        #print(\"1:\",m)\n",
    "        \n",
    "    filter_values = uniform(\n",
    "                    filters_stdev,\n",
    "                    (m.kernel_size[0], m.kernel_size[0], m.in_channels, m.out_channels)\n",
    "                )\n",
    "    \n",
    "    return filter_values\n",
    "\n",
    "def initialize_linear(m):\n",
    "    weight_values = uniform(\n",
    "                np.sqrt(2./(m.in_features+m.out_features)),\n",
    "                (m.in_features, m.out_features)\n",
    "            )\n",
    "    return weight_values\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight = torch.from_numpy(initialize_conv(m))\n",
    "        m.weight.data.copy_(weight,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_values = torch.from_numpy(initialize_linear(m))\n",
    "        m.weight.data.copy_(weight_values,broadcast=False)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)\n",
    "\n",
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "ld = 1e-4\n",
    "lg = 1e-4\n",
    "lq_d = 1e-4\n",
    "lq_g = 1e-4\n",
    "\n",
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lg, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ_D = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], ld, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerQ_G = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "            ], lg, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+10*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=True)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "        \n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "def exp_lr_scheduler(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \n",
    "    lr = init_lr* (1.0 - iteration / float(lr_decay_iter))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def exp_lr_scheduler_2(optimizer, iteration, init_lr=0.001, lr_decay_iter=1):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    if iteration<200000:\n",
    "        lr = init_lr + init_lr*(1*(iteration // 20000))\n",
    "    if iteration>=200000:\n",
    "        lr = init_lr*2 - init_lr*(1*(iteration // 20000))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=dis_category)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4217579365 20 109.08253479 2.26347327232\n",
      "18.0198299885 40 126.572555542 2.44957637787\n",
      "18.034168005 60 92.9622497559 2.67163729668\n",
      "18.0835030079 80 93.4988250732 1.79928684235\n",
      "18.0418679714 100 85.113319397 2.7561955452\n",
      "18.219397068 120 62.9561576843 2.78754425049\n",
      "18.0527999401 140 65.1598358154 1.48305857182\n",
      "18.1670889854 160 62.7607421875 1.26280617714\n",
      "18.1049778461 180 64.7311401367 1.94963896275\n",
      "18.2591640949 200 59.5311927795 0.737520098686\n",
      "18.1976029873 220 51.1513900757 3.12996196747\n",
      "18.1315560341 240 34.3710250854 2.17595124245\n",
      "18.2139902115 260 47.2888069153 1.12827908993\n",
      "18.1831839085 280 33.0216026306 0.893040657043\n",
      "18.1091291904 300 42.3010063171 1.44527924061\n",
      "18.177834034 320 37.9499435425 1.02815198898\n",
      "18.1042919159 340 32.2951049805 0.924679279327\n",
      "18.2488121986 360 35.7753601074 0.510048270226\n",
      "18.38656497 380 21.7019481659 2.36335420609\n",
      "18.1439700127 400 23.7754364014 0.942422866821\n",
      "18.1655440331 420 36.874294281 0.596853375435\n",
      "18.1186289787 440 17.3130817413 0.536906599998\n",
      "18.2785010338 460 23.6346702576 0.536442875862\n",
      "18.4313309193 480 23.8342494965 0.248156741261\n",
      "18.2812271118 500 16.1756839752 0.10110130161\n",
      "18.3841240406 520 16.25050354 0.731024622917\n",
      "18.1366701126 540 17.6496047974 0.527193188667\n",
      "18.1483819485 560 24.7947731018 0.869305431843\n",
      "18.1813201904 580 14.6744031906 0.0739932060242\n",
      "18.2512149811 600 20.1540260315 0.517689824104\n",
      "18.2184009552 620 18.2728538513 0.287353843451\n",
      "18.1836590767 640 15.7899560928 0.126534491777\n",
      "18.3081271648 660 16.3119926453 0.0321600921452\n",
      "18.207351923 680 21.511182785 0.0120789054781\n",
      "18.2432541847 700 16.4846973419 0.206502079964\n",
      "18.2423388958 720 17.488822937 0.229295164347\n",
      "18.1312150955 740 10.1157407761 0.00813441257924\n",
      "18.2279729843 760 18.7503738403 0.305819571018\n",
      "18.2553770542 780 16.993227005 0.496169388294\n",
      "17.8649799824 800 14.3353919983 0.438088417053\n",
      "18.2945928574 820 19.125743866 0.539961218834\n",
      "18.0481681824 840 9.71614265442 0.0303488727659\n",
      "18.2586939335 860 14.8866472244 0.0194512363523\n",
      "18.1483678818 880 13.5810718536 0.532250106335\n",
      "18.1278550625 900 12.8935585022 0.141920849681\n",
      "18.0277149677 920 16.598690033 0.0139739988372\n",
      "18.2921068668 940 13.187412262 0.204861760139\n",
      "18.0765800476 960 16.4846076965 0.00356059079058\n",
      "18.2050850391 980 8.71144962311 0.233572393656\n",
      "18.2080500126 1000 10.0502672195 0.413410246372\n",
      "18.091479063 1020 16.7258377075 2.23550653458\n",
      "18.189496994 1040 10.0959720612 0.0986899882555\n",
      "18.1675131321 1060 14.1585798264 0.0341074466705\n",
      "18.0244269371 1080 8.31686115265 0.0211500171572\n",
      "18.1530258656 1100 10.5153522491 0.0153793338686\n",
      "18.0603091717 1120 11.1803340912 0.0107648847625\n",
      "18.2733459473 1140 9.65865516663 0.016927767545\n",
      "18.2710080147 1160 8.95542430878 0.00340890884399\n",
      "18.0167090893 1180 11.6010227203 0.014993429184\n",
      "18.0111789703 1200 10.9149637222 0.0005202293396\n",
      "18.2277560234 1220 6.94520902634 0.658255875111\n",
      "18.2487490177 1240 11.0000982285 0.408673107624\n",
      "18.4566609859 1260 9.78459835052 0.481162458658\n",
      "18.2300610542 1280 12.6881580353 0.388982832432\n",
      "18.1085569859 1300 8.95094680786 0.00226230616681\n",
      "18.0869450569 1320 9.07789993286 1.32754778862\n",
      "18.3086600304 1340 10.2310342789 0.0128015521914\n",
      "18.2528748512 1360 10.7026052475 0.0313546173275\n",
      "18.0834100246 1380 9.67442703247 0.000349903100869\n",
      "18.0770249367 1400 8.48641777039 0.0493629686534\n",
      "18.1266319752 1420 7.71737670898 0.431158930063\n",
      "18.125756979 1440 6.18998384476 0.00686483364552\n",
      "18.0712709427 1460 11.7287187576 0.00516605377197\n",
      "18.2195191383 1480 11.7175312042 0.0286138057709\n",
      "18.0832130909 1500 9.36777305603 0.333390951157\n",
      "18.1601979733 1520 8.45028781891 0.0018722533714\n",
      "18.1899991035 1540 7.19236040115 0.756103694439\n",
      "18.0650141239 1560 11.44506073 0.194067433476\n",
      "18.1320919991 1580 10.7183694839 0.110029414296\n",
      "17.9014549255 1600 11.2744617462 0.323722660542\n",
      "18.2025201321 1620 6.31408977509 0.00926117878407\n",
      "18.32791996 1640 8.55696296692 0.0225670337677\n",
      "18.0308530331 1660 5.94837284088 0.000156688693096\n",
      "18.2378921509 1680 9.72085189819 0.00141077046283\n",
      "18.2186279297 1700 5.65483760834 0.0342486388981\n",
      "18.149712801 1720 7.70560932159 0.000690650951583\n",
      "18.2503039837 1740 14.1396064758 3.51707983017\n",
      "18.1528830528 1760 10.3314771652 0.177487805486\n",
      "18.1819479465 1780 12.6224689484 0.00401706714183\n",
      "18.2114760876 1800 9.42175102234 0.0054980753921\n",
      "18.3445999622 1820 7.71731853485 0.000115776063467\n",
      "18.1728649139 1840 9.65629482269 0.0130884647369\n",
      "18.1594059467 1860 9.7488155365 0.000283241271973\n",
      "18.265037775 1880 7.8613986969 0.00467662792653\n",
      "18.2288999557 1900 10.2848339081 0.277573108673\n",
      "18.226418972 1920 7.90852737427 0.0466729179025\n",
      "18.09674716 1940 9.53558349609 0.000402450561523\n",
      "18.2425889969 1960 9.25186538696 0.0132025722414\n",
      "18.3083431721 1980 11.6524457932 0.648427605629\n",
      "17.9264609814 2000 4.70208024979 0.739752113819\n",
      "18.2043161392 2020 8.31896781921 0.0240132324398\n",
      "18.0681319237 2040 7.44302177429 0.675769984722\n",
      "18.1441919804 2060 7.93156719208 0.077246569097\n",
      "18.1762759686 2080 5.62561321259 0.579947769642\n",
      "18.1403229237 2100 9.66234874725 0.0325818061829\n",
      "18.1543269157 2120 6.63794231415 0.00220451364294\n",
      "18.1835911274 2140 5.33559989929 0.0148138999939\n",
      "18.1181490421 2160 6.89297866821 0.000827598560136\n",
      "18.2927651405 2180 9.12178134918 0.000457191461464\n",
      "18.1194179058 2200 8.28719139099 0.0416822917759\n",
      "20.6643810272 2220 12.3283996582 1.34468082251e-05\n",
      "21.9873669147 2240 6.60060167313 0.00654458999634\n",
      "19.0791940689 2260 4.03850889206 0.0535861961544\n",
      "18.8264129162 2280 2.84522533417 0.00142641062848\n",
      "18.753068924 2300 4.29252815247 0.727684140205\n",
      "18.8354940414 2320 6.56904649734 0.000141906741192\n",
      "18.9561290741 2340 3.27460622787 0.27618265152\n",
      "19.1590530872 2360 7.58919668198 0.00552129745483\n",
      "18.8204131126 2380 7.76772975922 0.44454947114\n",
      "18.7163259983 2400 6.64291763306 0.000799369823653\n",
      "19.3150689602 2420 3.70108127594 0.000331974035362\n",
      "19.1340789795 2440 8.50093364716 0.000567388546187\n",
      "19.2411599159 2460 6.50656414032 0.0029088021256\n",
      "18.8432781696 2480 7.9827299118 0.0114177223295\n",
      "18.4594209194 2500 7.15239667892 4.00543212891e-05\n",
      "18.2964298725 2520 3.67328071594 0.00227422709577\n",
      "18.278493166 2540 10.0188627243 9.37461882131e-05\n",
      "18.4299929142 2560 7.90602302551 0.079100087285\n",
      "18.2972309589 2580 4.75872707367 0.000523948692717\n",
      "18.3698208332 2600 8.19612598419 0.0335871689022\n",
      "18.4772849083 2620 7.6775188446 0.000161933901836\n",
      "18.347053051 2640 5.08107376099 0.0182832721621\n",
      "18.3297991753 2660 4.38940429688 0.000154876703164\n",
      "18.4234690666 2680 3.93411374092 8.06808457128e-05\n",
      "18.3682918549 2700 11.1051425934 0.000144386285683\n",
      "18.3035311699 2720 8.92743301392 0.00527372350916\n",
      "18.485558033 2740 3.88631892204 0.0251897573471\n",
      "18.2528209686 2760 6.67777061462 0.000252532947343\n",
      "18.3305399418 2780 7.95385742188 0.0416393056512\n",
      "18.1246209145 2800 7.19355392456 1.2298053503\n",
      "18.4787209034 2820 8.65031528473 0.00421733874828\n",
      "18.3998551369 2840 10.8641681671 0.127451777458\n",
      "18.4014718533 2860 7.29529762268 8.76426711329e-05\n",
      "18.4213240147 2880 6.67779779434 0.0285682193935\n",
      "18.3200700283 2900 10.5079936981 0.000238752370933\n",
      "18.3916089535 2920 7.11709308624 0.00844860076904\n",
      "18.2618620396 2940 8.77003765106 0.00651931762695\n",
      "18.3314950466 2960 5.29967737198 0.00125722889788\n",
      "18.1797339916 2980 5.78545475006 0.000294876110274\n",
      "18.3699932098 3000 5.65907764435 0.00126738543622\n",
      "18.2590479851 3020 9.49892807007 0.484975099564\n",
      "18.2036700249 3040 5.40173864365 0.200992196798\n",
      "18.4180910587 3060 6.74978685379 0.200757220387\n",
      "18.3640720844 3080 8.41428661346 2.5512650013\n",
      "18.5546710491 3100 7.53410291672 0.0101495739073\n",
      "18.3661880493 3120 3.77728748322 3.52859501618e-06\n",
      "18.3090980053 3140 8.83046340942 0.0022927285172\n",
      "18.3378138542 3160 4.15994739532 0.000387382518966\n",
      "18.5801792145 3180 7.95666980743 0.00566411018372\n",
      "18.2197699547 3200 5.97366714478 0.00244197854772\n",
      "18.378166914 3220 4.12834358215 0.000456237787148\n",
      "18.2420740128 3240 6.45574951172 6.13212614553e-05\n",
      "18.4129090309 3260 3.66839075089 1.53844833374\n",
      "18.4232709408 3280 8.48619747162 0.00110864639282\n",
      "18.6293389797 3300 3.11651992798 0.0295543186367\n",
      "18.3425350189 3320 6.11470985413 0.0112975118682\n",
      "18.4471228123 3340 8.94792556763 3.8528443838e-05\n",
      "18.2762448788 3360 7.82001590729 0.62696903944\n",
      "18.2158679962 3380 12.0453643799 0.0593095310032\n",
      "18.3806569576 3400 6.62047529221 0.000438785558799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5372979641 3420 8.56398010254 0.0474051460624\n",
      "18.64260602 3440 6.42098379135 0.140657424927\n",
      "18.2477450371 3460 13.8748626709 0.0035887719132\n",
      "18.2165091038 3480 10.8203191757 0.00516281137243\n",
      "18.3277571201 3500 5.33584451675 0.00139188766479\n",
      "18.8082399368 3520 6.76447868347 0.00049600598868\n",
      "19.0875301361 3540 5.63954782486 0.640059351921\n",
      "18.9790930748 3560 7.07707643509 0.00860424060374\n",
      "18.9958240986 3580 7.53206157684 0.0509499534965\n",
      "18.9581580162 3600 8.27973747253 0.000226974487305\n",
      "19.1097619534 3620 8.41868495941 0.0850028023124\n",
      "19.1234810352 3640 4.54488563538 0.14109711349\n",
      "19.2898421288 3660 6.73201751709 0.000348281871993\n",
      "18.9580349922 3680 5.49537658691 0.000239753717324\n",
      "19.188806057 3700 6.96888160706 2.47955313171e-06\n",
      "18.5743861198 3720 5.99566459656 0.000281715387246\n",
      "18.4378268719 3740 8.37094783783 3.90052809962e-05\n",
      "18.3761408329 3760 3.50823378563 0.00311021809466\n",
      "18.4435060024 3780 4.61509227753 0.0288603790104\n",
      "18.3243598938 3800 9.37185001373 0.219151347876\n",
      "18.2461411953 3820 6.68040513992 0.00597429275513\n",
      "18.3584899902 3840 7.29787349701 0.000121593475342\n",
      "18.2618720531 3860 3.15094041824 0.0117301940918\n",
      "18.3185319901 3880 6.38857698441 0.032007932663\n",
      "18.4663460255 3900 7.53067731857 0.00223951344378\n",
      "18.4080510139 3920 5.11655473709 1.10537183285\n",
      "18.3606078625 3940 5.48258256912 0.00475268345326\n",
      "18.1874761581 3960 8.409324646 0.000362682330888\n",
      "18.1246190071 3980 3.47780227661 0.121950767934\n",
      "18.5758769512 4000 7.86703252792 0.000191402432392\n",
      "18.3055930138 4020 8.33248901367 0.292722553015\n",
      "18.4439499378 4040 4.04027462006 0.0121028898284\n",
      "18.5336771011 4060 7.58438873291 0.00642747897655\n",
      "18.8748381138 4080 7.19814825058 5.53131121706e-06\n",
      "19.1123900414 4100 5.07319784164 0.00126285548322\n",
      "18.9699881077 4120 7.8070397377 0.000111007691885\n",
      "19.0575230122 4140 6.25964403152 0.50047403574\n",
      "19.1911849976 4160 8.87011814117 0.00463480968028\n",
      "19.3335399628 4180 6.56200695038 0.00038824082003\n",
      "19.0044338703 4200 2.82259750366 0.0943355560303\n",
      "19.1966500282 4220 5.7870221138 0.0129488948733\n",
      "19.2137761116 4240 7.34021186829 0.000343990337569\n",
      "19.0196590424 4260 4.68729114532 1.18255611596e-05\n",
      "19.1496789455 4280 6.66319274902 0.0209760665894\n",
      "19.0233108997 4300 6.56931877136 0.000300598156173\n",
      "19.1581091881 4320 8.48951244354 0.00353603367694\n",
      "19.0664248466 4340 5.25677394867 1.12066054344\n",
      "19.0176179409 4360 4.24226522446 9.62257399806e-05\n",
      "19.1520221233 4380 6.41555595398 0.00124015810434\n",
      "19.201335907 4400 9.23949813843 3.14712519867e-06\n",
      "19.1999599934 4420 5.3723988533 0.000640487647615\n",
      "19.0195388794 4440 7.04863452911 0.322082042694\n",
      "18.7129731178 4460 4.18649482727 0.0033919333946\n",
      "18.5638849735 4480 2.5559887886 4.85420241603e-05\n",
      "18.5442230701 4500 7.53269481659 0.00253229145892\n",
      "18.4392471313 4520 3.58875989914 2.17437736865e-05\n",
      "18.6279139519 4540 8.78249645233 0.0378931984305\n",
      "18.4269630909 4560 8.4452047348 0.000128078463604\n",
      "18.3425691128 4580 7.1814904213 1.39236453833e-05\n",
      "18.6928930283 4600 6.07066345215 0.000153636938194\n",
      "18.3295910358 4620 6.68366527557 0.000299072271446\n",
      "18.395015955 4640 10.6194801331 2.11715705518e-05\n",
      "18.3855540752 4660 6.28039598465 0.00481195468456\n",
      "18.5775930882 4680 6.66412258148 0.00160820479505\n",
      "18.4022159576 4700 8.01717662811 8.67843664309e-06\n",
      "18.5969469547 4720 4.53723049164 9.53674330617e-08\n",
      "18.5901060104 4740 8.30405426025 0.00756120681763\n",
      "18.3925230503 4760 5.5607252121 0.0166613571346\n",
      "18.3492949009 4780 7.48668336868 0.0001626968442\n",
      "18.5180490017 4800 5.92517232895 1.38282775879e-05\n",
      "18.4726810455 4820 8.55339431763 0.0256324764341\n",
      "18.5542390347 4840 4.69222927094 0.263843774796\n",
      "18.4965000153 4860 4.06698322296 7.15255737305e-05\n",
      "18.4820950031 4880 7.24665880203 3.92913825635e-05\n",
      "18.4828169346 4900 6.03882408142 5.07354743604e-05\n",
      "18.5837118626 4920 5.26830863953 6.10351571595e-06\n",
      "18.6076221466 4940 8.46981525421 0.0256009586155\n",
      "18.1617350578 4960 11.0391073227 0.00177102093585\n",
      "18.2466239929 4980 5.69863605499 2.76565551758e-05\n",
      "18.3986480236 5000 8.48725128174 0.00247540464625\n",
      "18.3664300442 5020 4.74199390411 0.0207175258547\n",
      "18.4878730774 5040 6.2277598381 0.00144176487811\n",
      "18.4188401699 5060 5.42240333557 0.000428724277299\n",
      "18.4521410465 5080 8.29034519196 0.246886640787\n",
      "18.3864889145 5100 4.58193588257 1.41588568687\n",
      "18.5366501808 5120 6.31524848938 0.146599769592\n",
      "18.321313858 5140 6.9702296257 0.000379991543014\n",
      "18.7171499729 5160 10.006193161 0.000707483268343\n",
      "18.3732330799 5180 4.03672647476 0.000278902065475\n",
      "18.4824650288 5200 6.2262430191 0.00352439889684\n",
      "18.5643570423 5220 9.8625535965 0.57973241806\n",
      "18.4622821808 5240 6.7153429985 0.000471591949463\n",
      "18.7453119755 5260 3.9489839077 0.0013067960972\n",
      "18.6268391609 5280 7.58063745499 0.133520722389\n",
      "18.485656023 5300 7.35005283356 1.15462017059\n",
      "18.3316559792 5320 6.97669219971 0.000172948843101\n",
      "18.3155579567 5340 2.30385613441 0.124117612839\n",
      "18.5872108936 5360 4.44657230377 0.250257581472\n",
      "18.4073059559 5380 8.37760543823 0.000592374824919\n",
      "18.3227450848 5400 8.87293052673 0.000221490859985\n",
      "18.3997490406 5420 6.12193965912 0.0350202545524\n",
      "18.4221959114 5440 7.42239189148 0.000134849542519\n",
      "18.4128119946 5460 8.33852767944 0.00323450565338\n",
      "18.4710919857 5480 6.16518688202 0.0018320560921\n",
      "18.6102099419 5500 6.92144012451 1.54495246534e-05\n",
      "18.4814231396 5520 5.56214046478 5.48362731934e-06\n",
      "18.6374850273 5540 5.50838947296 0.000482654577354\n",
      "18.5269479752 5560 7.2978515625 0.00606646528468\n",
      "18.4045779705 5580 7.30456638336 0.000699758529663\n",
      "18.4818360806 5600 0.360612511635 0.284682422876\n",
      "18.9107460976 5620 7.90834617615 0.0133400438353\n",
      "18.5608379841 5640 6.35303735733 0.000690460205078\n",
      "18.5499389172 5660 5.64770460129 0.0456571578979\n",
      "18.4392669201 5680 9.69126796722 0.166577100754\n",
      "18.5218019485 5700 2.53948879242 0.00164974923246\n",
      "18.4879879951 5720 5.62301921844 0.00175559520721\n",
      "18.5329408646 5740 9.36936378479 0.482095867395\n",
      "18.9157688618 5760 7.71576404572 0.00115242006723\n",
      "18.7015721798 5780 5.09080886841 0.072708889842\n",
      "18.5145158768 5800 6.03036022186 0.0951646789908\n",
      "18.4448828697 5820 4.70675563812 0.747799634933\n",
      "18.741145134 5840 4.78738594055 2.86102306291e-07\n",
      "18.6081490517 5860 5.19431257248 0.000118017196655\n",
      "18.3750669956 5880 8.12319946289 0.000659751880448\n",
      "18.3876960278 5900 6.83884572983 0.163723945618\n",
      "18.4732291698 5920 6.36663722992 0.000594425189774\n",
      "18.3412258625 5940 4.52594184875 0.000179982191185\n",
      "18.4490170479 5960 1.02234232426 0.120630934834\n",
      "18.387912035 5980 5.90390396118 0.000228548044106\n",
      "18.4697790146 6000 6.72770500183 0.00977959670126\n",
      "18.6571099758 6020 9.71755313873 0.00786468945444\n",
      "18.6502788067 6040 6.32909727097 0.0796708911657\n",
      "18.6715409756 6060 8.52384185791 7.24792471374e-06\n",
      "18.3313989639 6080 4.92600297928 0.00306777958758\n",
      "18.7514371872 6100 7.49865198135 1.12021756172\n",
      "18.5407600403 6120 4.06412506104 0.0424825325608\n",
      "18.4415080547 6140 4.41945075989 0.000779724097811\n",
      "18.5715930462 6160 3.07692551613 0.000649309135042\n",
      "18.4778380394 6180 7.21184253693 9.12666291697e-05\n",
      "18.4054529667 6200 4.17737913132 0.0153154376894\n",
      "18.5219619274 6220 5.23990249634 1.4829635802e-05\n",
      "18.4005920887 6240 6.59392166138 6.96182269166e-06\n",
      "18.8094391823 6260 8.90206813812 1.58786770044e-05\n",
      "18.3882770538 6280 10.4193916321 1.87257575989\n",
      "18.833758831 6300 8.61769676208 5.2165985835e-05\n",
      "18.4075229168 6320 8.00874614716 1.08718868432e-05\n",
      "18.5704631805 6340 11.3007326126 0.00147795677185\n",
      "18.5789320469 6360 3.19104766846 0.000104737278889\n",
      "18.3090429306 6380 10.1042881012 0.000190639490029\n",
      "18.4565761089 6400 1.72185719013 0.402503579855\n",
      "18.5096759796 6420 6.70947360992 0.00206713681109\n",
      "18.6484911442 6440 8.16992473602 2.47955313171e-06\n",
      "18.2634720802 6460 3.07405757904 1.54667389393\n",
      "18.454087019 6480 2.90992712975 1.52587892899e-06\n",
      "18.4183690548 6500 6.91217756271 0.00332445790991\n",
      "18.2368328571 6520 0.195880889893 0.231135800481\n",
      "18.4758470058 6540 9.55512332916 0.0400649085641\n",
      "18.6129670143 6560 4.06379747391 0.000395917886635\n",
      "18.4705679417 6580 3.29763770103 1.71661376953e-05\n",
      "18.3890361786 6600 6.19805526733 0.0728691071272\n",
      "18.3811388016 6620 7.83904457092 0.00117163662799\n",
      "18.5400400162 6640 5.334815979 0.00100116734393\n",
      "18.2223389149 6660 4.05274534225 5.24520874023e-05\n",
      "18.5672240257 6680 2.90392971039 0.00253651151434\n",
      "18.5591211319 6700 6.16909790039 3.32355484716e-05\n",
      "18.4001319408 6720 4.32835578918 1.52349475684e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5293979645 6740 6.76357555389 2.28881845032e-06\n",
      "18.4609379768 6760 6.15156602859 2.28881845032e-06\n",
      "18.053538084 6780 5.73388290405 0.0264237876981\n",
      "18.406512022 6800 6.90924739838 0.000384998333175\n",
      "18.6786279678 6820 7.02500343323 0.0282954461873\n",
      "18.2522318363 6840 3.07752633095 0.102994181216\n",
      "18.5114369392 6860 6.94363021851 0.00220161071047\n",
      "18.3168110847 6880 4.73621416092 1.42574308484e-05\n",
      "18.3699960709 6900 6.00617790222 1.1607863903\n",
      "18.3177509308 6920 6.13443946838 0.213359743357\n",
      "18.6566860676 6940 5.21149635315 0.00036416054354\n",
      "18.3701159954 6960 4.74484348297 0.00268552312627\n",
      "18.6881420612 6980 11.7130708694 0.000125932696392\n",
      "18.3999681473 7000 6.2416434288 0.00014967918105\n",
      "18.3745229244 7020 4.9784321785 2.95639028991e-06\n",
      "18.3188090324 7040 7.03553819656 0.0101381065324\n",
      "18.492978096 7060 1.52975571156 1.01993465424\n",
      "18.529900074 7080 10.1744308472 1.09672544113e-06\n",
      "18.3153140545 7100 8.20021533966 0.00287227635272\n",
      "18.6445760727 7120 3.66063928604 3.16619880323e-05\n",
      "18.4333319664 7140 5.31180524826 4.52518470411e-05\n",
      "18.3626601696 7160 6.26528930664 0.000122594836284\n",
      "18.2670559883 7180 3.37580776215 1.39952898026\n",
      "18.4499289989 7200 5.93041229248 0.000241279602051\n",
      "18.388406992 7220 1.44416499138 5.08308403369e-05\n",
      "18.5026350021 7240 5.60434055328 0.0208832137287\n",
      "18.3978660107 7260 6.86318778992 0.0\n",
      "18.403455019 7280 7.6867685318 0.00104628200643\n",
      "18.4456300735 7300 3.35361862183 2.67813539505\n",
      "18.5667600632 7320 6.15664196014 0.884830653667\n",
      "18.472192049 7340 9.8569726944 0.00313642015681\n",
      "18.3049731255 7360 5.6129989624 4.44889083155e-05\n",
      "18.2938768864 7380 7.46358251572 0.00183268787805\n",
      "18.2012970448 7400 2.44903922081 0.000304269779008\n",
      "18.5875511169 7420 4.36807250977 0.510624349117\n",
      "18.4212729931 7440 5.65990114212 6.00814837526e-06\n",
      "18.5078318119 7460 4.02112150192 5.11169419042e-05\n",
      "18.4953210354 7480 7.87576770782 6.2990191509e-05\n",
      "18.4750218391 7500 2.9808549881 0.26250782609\n",
      "18.7056381702 7520 8.96409797668 0.000905132270418\n",
      "18.201086998 7540 4.55432510376 0.498331934214\n",
      "18.506690979 7560 6.94747447968 2.67028804046e-06\n",
      "18.6678161621 7580 5.54050064087 0.136110037565\n",
      "18.3822760582 7600 5.23959302902 3.55720512744e-05\n",
      "18.506469965 7620 6.22853469849 3.52859501618e-06\n",
      "18.3501000404 7640 5.38840103149 0.000905156135559\n",
      "18.4495780468 7660 3.29381608963 7.62939464494e-07\n",
      "18.430106163 7680 2.0749797821 0.675891339779\n",
      "18.4002528191 7700 6.29715919495 0.40266212821\n",
      "18.3790810108 7720 3.32556295395 0.234086304903\n",
      "18.417427063 7740 5.81586885452 0.000742959964555\n",
      "18.5590159893 7760 5.09811353683 0.000195240980247\n",
      "18.315611124 7780 11.437994957 0.000675773597322\n",
      "18.3868410587 7800 9.41103458405 0.0355116613209\n",
      "18.3963799477 7820 5.78586387634 1.81198117843e-06\n",
      "18.4494950771 7840 6.21968746185 3.29017643708e-06\n",
      "18.3559710979 7860 6.82985162735 1.28269193738e-05\n",
      "18.440418005 7880 6.56997537613 0.00421561021358\n",
      "18.5674960613 7900 3.45633411407 1.90734863281e-06\n",
      "18.4511339664 7920 4.95134162903 6.81877145325e-06\n",
      "18.3786649704 7940 8.34246730804 0.00079843995627\n",
      "17.926295042 7960 7.22912502289 0.000157260888955\n",
      "18.4823460579 7980 3.94446110725 0.129650384188\n",
      "18.4387159348 8000 1.10803866386 4.48226928711e-05\n",
      "18.486027956 8020 7.07923460007 0.0217020995915\n",
      "18.4871008396 8040 8.33379459381 0.00378308305517\n",
      "18.5244867802 8060 8.89747810364 0.0141959665343\n",
      "18.3629069328 8080 4.72925329208 0.000119113923574\n",
      "18.1505880356 8100 6.23474788666 0.000119876858662\n",
      "18.348968029 8120 5.23800945282 0.338453382254\n",
      "18.3137249947 8140 6.29063892365 7.15255737305e-07\n",
      "18.4507272243 8160 4.62378358841 0.219621226192\n",
      "18.2855188847 8180 6.70347261429 0.000160396099091\n",
      "18.4618589878 8200 5.04829597473 0.0579880475998\n",
      "18.3882198334 8220 3.35634422302 1.90734866123e-07\n",
      "18.3817539215 8240 6.52807855606 1.19209289551e-06\n",
      "18.4375638962 8260 4.61977005005 0.533241510391\n",
      "18.3696129322 8280 9.32723712921 4.97579567309e-05\n",
      "18.4852499962 8300 7.52309036255 0.0110925082117\n",
      "18.5083670616 8320 2.78049349785 0.265096455812\n",
      "18.2947211266 8340 6.39298629761 4.22120101575e-05\n",
      "18.0588109493 8360 2.43064403534 0.000912666320801\n",
      "18.4861199856 8380 5.62174844742 4.00781646022e-05\n",
      "18.4403090477 8400 0.654241085052 0.00358554976992\n",
      "18.367980957 8420 1.5061570406 0.0269320961088\n",
      "18.3854689598 8440 5.94741249084 0.000384521496017\n",
      "18.4499897957 8460 3.87156438828 6.60419482301e-06\n",
      "18.2553369999 8480 2.3466155529 0.128281697631\n",
      "18.6316099167 8500 7.58322429657 0.00019780099683\n",
      "18.4556920528 8520 0.876709342003 3.28302376147e-05\n",
      "18.3482279778 8540 5.95081377029 0.000203961128136\n",
      "18.4483649731 8560 6.82157850266 4.41074377022e-07\n",
      "18.4947369099 8580 5.51326370239 7.09295272827e-06\n",
      "18.4172971249 8600 6.32647943497 0.084709212184\n",
      "18.4631700516 8620 6.09499549866 3.43799583788e-05\n",
      "18.5394430161 8640 1.9362385273 0.000530910503585\n",
      "18.4214220047 8660 4.85194635391 0.00499577540904\n",
      "18.270950079 8680 6.03789710999 3.1375886465e-05\n",
      "18.5073621273 8700 5.76548910141 0.00648343423381\n",
      "18.2478280067 8720 6.07351732254 0.00028009415837\n",
      "18.4066240788 8740 5.57909679413 0.00344464788213\n",
      "18.5015990734 8760 6.19048261642 0.000222507122089\n",
      "18.2841749191 8780 4.46041679382 0.120100937784\n",
      "18.416135788 8800 6.56553030014 2.67028804046e-06\n",
      "18.3272848129 8820 4.35381698608 0.000654602074064\n",
      "18.255863905 8840 3.84369373322 0.00607595453039\n",
      "18.5936980247 8860 5.56566429138 1.97410590772e-05\n",
      "18.4597530365 8880 8.4878911972 0.0312358494848\n",
      "18.4782021046 8900 4.15164422989 0.000372242939193\n",
      "18.5545010567 8920 5.21454429626 0.741664290428\n",
      "18.5746319294 8940 5.09904956818 0.000674796116073\n",
      "18.5784249306 8960 5.39856481552 0.6810105443\n",
      "18.5121600628 8980 4.15642118454 1.23381614685e-05\n",
      "18.5651290417 9000 6.7859544754 0.000280535226921\n",
      "18.3327040672 9020 1.93350744247 0.000198602676392\n",
      "18.2827839851 9040 3.72099590302 0.0944051519036\n",
      "18.2777411938 9060 6.19507455826 0.000508284545504\n",
      "18.4783358574 9080 7.85661792755 0.000212430953979\n",
      "18.4902060032 9100 4.66343212128 1.19209289551e-06\n",
      "18.4254040718 9120 6.24307584763 0.000347757333657\n",
      "18.3118150234 9140 2.52496337891 0.306509643793\n",
      "18.2357950211 9160 3.9327044487 0.75302118063\n",
      "18.620221138 9180 4.74368143082 1.64747234521e-05\n",
      "18.3887560368 9200 4.92346096039 9.53674316406e-07\n",
      "18.3921868801 9220 6.33814334869 0.0726815462112\n",
      "18.4138507843 9240 4.23871183395 3.81469732247e-07\n",
      "18.3676540852 9260 4.32062149048 0.000374269497115\n",
      "18.3123240471 9280 6.3642168045 0.0256843082607\n",
      "18.3648819923 9300 5.61054563522 6.13927841187e-06\n",
      "18.3091750145 9320 6.03168773651 8.10623191683e-07\n",
      "18.4618141651 9340 7.74528360367 5.24520885392e-07\n",
      "18.4855618477 9360 1.82361137867 0.0121282394975\n",
      "18.3779518604 9380 7.33103466034 7.96318090579e-06\n",
      "18.1917309761 9400 5.76285648346 0.0713705569506\n",
      "18.4442248344 9420 7.1798582077 3.53097930201e-05\n",
      "18.3885529041 9440 6.05330324173 2.40319371223\n",
      "18.2929821014 9460 5.76875400543 0.000219326844672\n",
      "18.5701649189 9480 2.89408063889 0.000173997876118\n",
      "18.4860019684 9500 5.86900186539 8.94069671631e-07\n",
      "18.4408500195 9520 7.11452627182 8.22544109269e-07\n",
      "18.3799331188 9540 6.4677324295 1.39989340305\n",
      "18.1981809139 9560 6.10612344742 0.163259655237\n",
      "18.3757529259 9580 4.89331531525 0.000525641429704\n",
      "18.2864711285 9600 5.85761547089 1.59382816491e-05\n",
      "18.6604979038 9620 6.03947162628 1.70692801476e-05\n",
      "18.7431340218 9640 4.77364492416 3.95774850404e-06\n",
      "18.2271289825 9660 4.51928520203 0.000100064280559\n",
      "18.3063569069 9680 4.53485107422 3.55958945875e-05\n",
      "18.2584660053 9700 5.82540369034 7.61628180044e-05\n",
      "18.3223838806 9720 6.1610956192 0.231959864497\n",
      "18.3693339825 9740 5.61918830872 0.223326355219\n",
      "18.3630621433 9760 8.16657924652 0.00096819398459\n",
      "18.2148318291 9780 3.91977572441 0.000124835962197\n",
      "18.2793958187 9800 5.2603559494 3.33786005058e-07\n",
      "18.4836330414 9820 4.79359674454 0.0107193710282\n",
      "18.1475410461 9840 6.38130331039 1.32322315949e-06\n",
      "18.3028450012 9860 4.33091974258 0.0\n",
      "18.5520429611 9880 0.229728519917 6.32762894384e-05\n",
      "18.4290320873 9900 1.27822864056 0.989014923573\n",
      "18.5510771275 9920 6.40383052826 9.89437126009e-07\n",
      "18.3236320019 9940 0.154810488224 0.000673428177834\n",
      "18.1761779785 9960 3.43479156494 0.375427901745\n",
      "18.5574641228 9980 5.48542737961 0.00936858635396\n",
      "18.4697880745 10000 2.52110600471 0.601961016655\n",
      "18.3140490055 10020 4.12687301636 0.00011322498176\n",
      "18.252810955 10040 8.59262752533 1.66893005371e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.4755330086 10060 7.76396465302 2.95639028991e-06\n",
      "18.4790120125 10080 4.65099525452 4.50611105407e-06\n",
      "18.3063080311 10100 5.89053010941 1.03473666968e-05\n",
      "18.4459860325 10120 9.70737171173 0.781453073025\n",
      "18.4556119442 10140 6.39243602753 0.000135755544761\n",
      "18.4183411598 10160 5.64511203766 0.04384662956\n",
      "18.4018249512 10180 5.74596977234 1.02996827991e-05\n",
      "18.2303359509 10200 4.15478181839 0.000411760818679\n",
      "18.5627799034 10220 6.46558237076 1.58548355103e-05\n",
      "19.0057520866 10240 2.13637018204 1.18017192108e-06\n",
      "19.0396709442 10260 7.19015407562 3.38733188983e-05\n",
      "18.9450070858 10280 2.16632080078 0.00456026801839\n",
      "19.0154871941 10300 4.90879631042 0.0140264630318\n",
      "19.0053918362 10320 7.91359567642 0.572117984295\n",
      "19.0863108635 10340 6.76793146133 1.87754631042e-05\n",
      "19.0933101177 10360 3.51287984848 0.00279864971526\n",
      "19.139234066 10380 7.10030841827 0.00489879865199\n",
      "19.1004490852 10400 3.05322909355 0.00483458023518\n",
      "19.1550688744 10420 8.2781085968 0.000301373016555\n",
      "18.8624899387 10440 6.98332214355 1.26957893372e-05\n",
      "19.0871489048 10460 5.47618579865 0.00711546372622\n",
      "19.1651899815 10480 6.36365938187 0.00210669031367\n",
      "19.0886690617 10500 5.76686429977 7.15255765726e-08\n",
      "19.2226960659 10520 6.14536476135 0.00193848612253\n",
      "19.0129361153 10540 8.21757221222 0.0\n",
      "19.1477570534 10560 10.2909126282 0.00163385865744\n",
      "19.104156971 10580 2.77362728119 3.9815904529e-06\n",
      "19.0074939728 10600 6.41657066345 1.90734866123e-07\n",
      "19.0960268974 10620 5.06884431839 7.62939464494e-07\n",
      "18.507256031 10640 4.46049976349 0.0146475676447\n",
      "18.4288420677 10660 4.02358579636 9.59634780884e-06\n",
      "18.1551151276 10680 8.03450298309 7.29560861146e-06\n",
      "18.3022429943 10700 5.45353555679 7.39455208532e-05\n",
      "18.538533926 10720 3.10463857651 0.00378755340353\n",
      "18.2112858295 10740 1.04529857635 9.29832481233e-07\n",
      "18.2556967735 10760 5.9251537323 4.19139869337e-05\n",
      "18.3152930737 10780 5.83118438721 3.29017643708e-06\n",
      "18.1593270302 10800 6.60528993607 0.00219143624417\n",
      "18.3767950535 10820 2.91464471817 0.111391328275\n",
      "18.3057048321 10840 5.57748413086 0.309063106775\n",
      "18.2230989933 10860 6.37590026855 1.38282780426e-06\n",
      "18.4954779148 10880 7.3543639183 0.000136709219078\n",
      "18.3429481983 10900 4.11573696136 0.000354421150405\n",
      "18.3926889896 10920 3.62648916245 0.000949835754\n",
      "18.3957960606 10940 4.41102981567 1.66893002529e-07\n",
      "18.4696321487 10960 6.80349636078 2.54392616625e-05\n",
      "18.2662730217 10980 4.61792755127 0.00791479367763\n",
      "18.5347189903 11000 5.3397064209 0.0566396936774\n",
      "18.3699691296 11020 5.62480211258 0.0018869995838\n",
      "18.3531918526 11040 7.41819953918 5.54323196411e-06\n",
      "18.2697389126 11060 0.649971365929 0.000221848487854\n",
      "18.6252989769 11080 8.25276565552 0.0010467171669\n",
      "18.3941919804 11100 -0.158743143082 4.88758087158e-05\n",
      "18.6252739429 11120 5.65792274475 0.000470256811241\n",
      "18.3216149807 11140 3.67657518387 0.589136004448\n",
      "18.3258860111 11160 5.79706859589 0.494506031275\n",
      "18.2809050083 11180 4.4527258873 5.12719161634e-05\n",
      "18.4857079983 11200 4.17367267609 3.57627882863e-08\n",
      "18.2681131363 11220 8.92521381378 0.00439300527796\n",
      "18.5600888729 11240 10.2579193115 0.000376760959625\n",
      "18.3266949654 11260 7.58331108093 0.0844493284822\n",
      "18.1880049706 11280 6.35513067245 1.57356259933e-06\n",
      "18.3645641804 11300 11.5007810593 3.45706951066e-07\n",
      "18.3161728382 11320 3.41818213463 4.20808783019e-06\n",
      "18.462528944 11340 2.91180634499 0.278654873371\n",
      "18.4927620888 11360 5.57715511322 4.02927389587e-06\n",
      "18.2529158592 11380 7.85729074478 0.000284290319541\n",
      "18.4499440193 11400 5.53433036804 8.68439674377e-05\n",
      "18.4069280624 11420 4.91900205612 8.83340817381e-06\n",
      "18.3705818653 11440 5.57942295074 7.44223580114e-05\n",
      "18.3866841793 11460 5.61821365356 1.31352519989\n",
      "18.4114968777 11480 4.44340515137 0.00141488318332\n",
      "18.3407728672 11500 2.31172084808 5.56111335754e-05\n",
      "18.2368621826 11520 10.8279409409 1.14440922516e-06\n",
      "18.3144929409 11540 6.7008357048 3.08990493068e-05\n",
      "18.2001149654 11560 3.71657586098 2.33650212067e-06\n",
      "18.4872009754 11580 6.92762231827 0.000547707080841\n",
      "18.5220789909 11600 5.75154781342 0.00548505783081\n",
      "18.5102541447 11620 4.92563724518 1.78813934326e-07\n",
      "18.3323919773 11640 6.21560144424 0.00054308178369\n",
      "18.3034350872 11660 4.50353479385 2.80380245385e-05\n",
      "18.4736990929 11680 11.0980148315 0.000355410564225\n",
      "18.2347249985 11700 3.69993591309 0.000889766204637\n",
      "18.4634439945 11720 3.82325553894 1.19209289551e-05\n",
      "18.5732951164 11740 6.20664644241 0.0574066415429\n",
      "18.4615249634 11760 5.63194608688 0.123405955732\n",
      "18.2827010155 11780 5.17005348206 5.97834587097e-05\n",
      "18.4346659184 11800 6.76603507996 0.544036388397\n",
      "18.4947049618 11820 3.52078604698 0.0349536165595\n",
      "18.3161919117 11840 5.78019618988 1.27553937546e-06\n",
      "18.4236910343 11860 6.18314361572 0.000101649762655\n",
      "18.4000110626 11880 4.06225728989 2.45255851746\n",
      "18.3039181232 11900 6.09729576111 0.790737330914\n",
      "18.1238529682 11920 5.99836301804 1.05619428723e-05\n",
      "17.7632110119 11940 6.86117649078 0.43967333436\n",
      "18.6968510151 11960 4.45242261887 3.93509872083e-05\n",
      "18.4517688751 11980 5.0995054245 0.0253859460354\n",
      "18.2974808216 12000 3.77801203728 1.84774398804e-06\n",
      "18.3480439186 12020 7.22322654724 4.64916229248e-06\n",
      "18.3667991161 12040 -0.149958610535 1.98245052161e-05\n",
      "18.6087920666 12060 4.11959218979 0.00674660224468\n",
      "18.2360160351 12080 7.40043973923 4.76837165309e-08\n",
      "18.3875000477 12100 4.06527805328 3.32355484716e-05\n",
      "18.2003760338 12120 5.21236467361 1.80602073669e-05\n",
      "18.4151520729 12140 6.91519021988 0.794078230858\n",
      "18.3825848103 12160 5.58313798904 5.00678993376e-07\n",
      "18.2219018936 12180 5.4604344368 1.91092494788e-05\n",
      "18.3666760921 12200 7.12131404877 0.157053142786\n",
      "18.403824091 12220 3.3474521637 3.55720512744e-05\n",
      "18.5494320393 12240 6.47780132294 9.7393985925e-06\n",
      "18.1828091145 12260 3.00945520401 0.000382637983421\n",
      "18.4170598984 12280 9.18200111389 5.45382499695e-05\n",
      "18.2880451679 12300 5.76342439651 1.15633008591e-06\n",
      "18.5261929035 12320 6.14827251434 2.50339496688e-07\n",
      "18.2625620365 12340 9.01085567474 0.00107702019159\n",
      "18.494603157 12360 3.50827026367 2.05039987122e-06\n",
      "18.3249619007 12380 3.82382464409 0.0016407489311\n",
      "18.5830998421 12400 6.51645612717 1.85966496247e-06\n",
      "18.5446770191 12420 5.77090358734 1.77621836883e-06\n",
      "18.3673429489 12440 8.78545761108 0.0605501905084\n",
      "18.4438729286 12460 7.42886066437 0.000961852085311\n",
      "18.369093895 12480 1.41155195236 2.1433830625e-05\n",
      "18.2457799911 12500 5.69002628326 6.55651092529e-07\n",
      "18.4522278309 12520 5.36815166473 0.124843142927\n",
      "18.3650920391 12540 5.11032438278 2.63690944848e-05\n",
      "18.2670471668 12560 6.40526294708 3.32593913299e-06\n",
      "18.4299430847 12580 8.92443847656 0.0408501625061\n",
      "18.3600878716 12600 6.32227516174 0.0040739774704\n",
      "18.3832719326 12620 2.39596486092 0.565244495869\n",
      "18.5390620232 12640 4.16863107681 1.25169754028e-06\n",
      "18.3098170757 12660 3.13997173309 0.00311956414953\n",
      "18.3792870045 12680 3.62160229683 0.00059475901071\n",
      "18.3708469868 12700 4.81390476227 0.00282757286914\n",
      "18.4444441795 12720 4.74732971191 0.0781800970435\n",
      "18.2060890198 12740 4.81461429596 0.415356725454\n",
      "18.4061479568 12760 8.47420215607 1.66893002529e-07\n",
      "18.374723196 12780 9.30998516083 5.96046447754e-08\n",
      "18.237582922 12800 6.42692232132 3.8743019104e-06\n",
      "18.2289421558 12820 7.55794334412 1.19686128528e-05\n",
      "18.2882089615 12840 5.06468486786 0.00665793428198\n",
      "18.3304121494 12860 6.05634784698 2.38418582654e-08\n",
      "18.4631369114 12880 5.917948246 0.000434267509263\n",
      "18.419519186 12900 4.62646913528 2.38418582654e-08\n",
      "18.3169999123 12920 7.30828285217 2.38418579102e-07\n",
      "18.2284250259 12940 7.24567890167 0.0047678113915\n",
      "18.3227989674 12960 4.75293254852 0.000627720379271\n",
      "18.5317399502 12980 2.36558818817 0.000432896602433\n",
      "18.9479420185 13000 6.9106297493 0.0306979175657\n",
      "19.1800458431 13020 5.19023895264 0.252865940332\n",
      "18.9994859695 13040 5.39870977402 2.21729283112e-06\n",
      "19.0974059105 13060 8.76392555237 0.0\n",
      "18.9410400391 13080 8.49999141693 0.16169013083\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import time\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "end = time.time()\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize,dis_category=dis_category)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+dis_category*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, dis_category)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ_D.step()\n",
    "            optimizerQ_G.step()\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize,dis_category=dis_category)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+dis_category*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "        \n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, dis_category)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ_D.step()\n",
    "        optimizerQ_G.step()\n",
    "\n",
    "        if gen_iterations % 20 == 0 :\n",
    "            \n",
    "            batch_time = time.time() - end\n",
    "            end = time.time()\n",
    "            \n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_white.txt\",\"a\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(batch_time, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_white_10.png',nrow=10,normalize=True)\n",
    "            vutils.save_image(image_, 'real_cell_10.png',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(netD.state_dict(), './params/tumor_netD_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netG.state_dict(), './params/tumor_netG_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_D.state_dict(), './params/tumor_netD_D_10_epoch_%d.pth' % (epoch))\n",
    "torch.save(netD_Q.state_dict(), './params/tumor_netD_Q_10_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/disk1/labeled/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = []\n",
    "label = []\n",
    "for n,array in enumerate(npyList):\n",
    "    result.append(np.load(array))\n",
    "    label.append([n]*result[n].shape[0])\n",
    "    \n",
    "result = np.concatenate(result)\n",
    "label = np.concatenate(label)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0\n",
    "\n",
    "X_train = torch.FloatTensor(X)\n",
    "X_label = torch.LongTensor(label)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=False, batch_size=1)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "predict = []\n",
    "netD = netD.cuda()\n",
    "netD_Q = netD_Q.cuda()\n",
    "\n",
    "for iteration in data_iter:\n",
    "    img, img_label = iteration\n",
    "    predict_label = netD_Q(netD(Variable(img.cuda())))\n",
    "    predict.append(predict_label.data.cpu().numpy())    \n",
    "    \n",
    "predict_label = []\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    predict_label.append(np.argmax(predict[n]))\n",
    "    \n",
    "coherent_array = np.zeros((4,10),dtype=int)\n",
    "\n",
    "for n in range(0, len(predict)):\n",
    "    coherent_array[label[n],predict_label[n]] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8  34 106  12  13  15  36 332  71 115]\n",
      " [ 52 232  12   2  11  50   2  21  13  82]\n",
      " [  3   2   4   0   8   1   0   2   6   8]\n",
      " [ 33  77  65  38 134  66  71  26 163  45]]\n"
     ]
    }
   ],
   "source": [
    "print(coherent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for i in range(0,10):\n",
    "    sum += np.max(coherent_array[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
