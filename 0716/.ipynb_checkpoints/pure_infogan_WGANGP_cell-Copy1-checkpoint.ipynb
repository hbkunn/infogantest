{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26336, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk1/cell_segment_save/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.133-256.convt): ConvTranspose2d(133, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "dis_category = 5\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*dis_category)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*dis_category)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n",
      "_netD_Q (\n",
      "  (conv): Conv2d(256, 5, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, nd, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+dis_category*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=False)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=5, row=30)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 65.4439849854 7.24994182587\n",
      "0 40 52.2268218994 3.39745879173\n",
      "0 60 43.3786087036 10.4275531769\n",
      "0 80 41.259098053 7.25566101074\n",
      "1 100 34.3507843018 3.46522116661\n",
      "1 120 28.349981308 3.21229195595\n",
      "1 140 22.9673690796 3.20602059364\n",
      "1 160 20.7154312134 3.22600293159\n",
      "2 180 18.5752944946 5.10106372833\n",
      "2 200 15.7414093018 3.21333813667\n",
      "2 220 13.3603057861 3.21197247505\n",
      "2 240 9.57431411743 3.22932505608\n",
      "3 260 8.54016971588 3.24980998039\n",
      "3 280 6.94105005264 3.40680527687\n",
      "3 300 6.55596160889 3.20490098\n",
      "3 320 4.95667695999 3.31693720818\n",
      "4 340 3.35878038406 3.23420834541\n",
      "4 360 4.1241850853 3.20387005806\n",
      "4 380 3.77235770226 3.31712412834\n",
      "4 400 3.40494179726 4.51130867004\n",
      "5 420 3.66855621338 3.30653190613\n",
      "5 440 3.314016819 3.62943673134\n",
      "5 460 2.63563919067 3.2455329895\n",
      "5 480 2.84397244453 3.31944298744\n",
      "6 500 3.92193126678 3.20354580879\n",
      "6 520 3.11459469795 3.20123457909\n",
      "6 540 3.32099795341 3.20243930817\n",
      "6 560 2.57514333725 3.25409007072\n",
      "6 580 4.06815910339 3.20024323463\n",
      "7 600 2.50961828232 5.02500104904\n",
      "7 620 2.63529729843 3.25077772141\n",
      "7 640 2.45607686043 3.57735610008\n",
      "7 660 2.14172506332 3.22613215446\n",
      "8 680 2.32289934158 3.20000720024\n",
      "8 700 2.32119202614 3.20000481606\n",
      "8 720 2.08997583389 3.4143614769\n",
      "8 740 2.1598072052 3.20021700859\n",
      "9 760 2.20565104485 3.20005536079\n",
      "9 780 2.23340678215 8.66763973236\n",
      "9 800 2.18021678925 3.5725376606\n",
      "9 820 2.04045701027 3.20075011253\n",
      "10 840 2.00640892982 3.2000143528\n",
      "10 860 1.85682499409 4.51632595062\n",
      "10 880 1.58676970005 3.20011305809\n",
      "10 900 1.92208015919 3.2000105381\n",
      "11 920 1.7261660099 3.20611596107\n",
      "11 940 1.7740688324 3.20036697388\n",
      "11 960 2.13059830666 3.27483892441\n",
      "11 980 2.39742660522 3.20045495033\n",
      "12 1000 1.90438556671 3.20299172401\n",
      "12 1020 1.94206535816 3.20078349113\n",
      "12 1040 2.05832648277 3.20000863075\n",
      "12 1060 1.89358222485 3.20001339912\n",
      "13 1080 2.08344769478 3.20181679726\n",
      "13 1100 2.19615244865 3.20031476021\n",
      "13 1120 2.12304806709 3.20034337044\n",
      "13 1140 1.90387749672 3.20439529419\n",
      "13 1160 1.97428941727 3.20010590553\n",
      "14 1180 1.14074981213 3.21925663948\n",
      "14 1200 1.91140329838 3.20221757889\n",
      "14 1220 1.39313685894 3.20077753067\n",
      "14 1240 1.64812278748 3.20475101471\n",
      "15 1260 1.55557739735 3.20066523552\n",
      "15 1280 1.83362197876 3.20090174675\n",
      "15 1300 1.62546610832 3.28301501274\n",
      "15 1320 1.30074489117 3.20118403435\n",
      "16 1340 1.56076717377 3.20769262314\n",
      "16 1360 1.26843619347 8.42591094971\n",
      "16 1380 1.86811685562 3.20030093193\n",
      "16 1400 1.5046402216 3.20062851906\n",
      "17 1420 1.86696445942 3.20403695107\n",
      "17 1440 1.39641869068 3.20005249977\n",
      "17 1460 2.22171258926 11.0754184723\n",
      "17 1480 1.59522914886 3.2111530304\n",
      "18 1500 1.53578352928 3.20033097267\n",
      "18 1520 2.0899014473 3.20003628731\n",
      "18 1540 2.175106287 3.20001959801\n",
      "18 1560 1.69824302197 3.20010590553\n",
      "19 1580 1.86108207703 3.21070289612\n",
      "19 1600 1.75143742561 3.20034241676\n",
      "19 1620 1.56440448761 3.20000290871\n",
      "19 1640 1.76199197769 3.20622229576\n",
      "19 1660 1.80267643929 3.21926426888\n",
      "20 1680 1.74826288223 3.2432975769\n",
      "20 1700 1.90671873093 3.20001101494\n",
      "20 1720 1.70271706581 3.20008111\n",
      "20 1740 1.81436467171 3.21524834633\n",
      "21 1760 2.24324512482 3.20047831535\n",
      "21 1780 1.53433310986 3.20018982887\n",
      "21 1800 1.73327851295 3.20321846008\n",
      "21 1820 1.72624838352 3.24636387825\n",
      "22 1840 2.30599355698 3.20000290871\n",
      "22 1860 1.50258529186 3.21525526047\n",
      "22 1880 1.47385430336 3.20005297661\n",
      "22 1900 1.84814727306 3.20308327675\n",
      "23 1920 1.49586451054 10.8826856613\n",
      "23 1940 1.64663541317 3.20003294945\n",
      "23 1960 1.6982421875 3.20009303093\n",
      "23 1980 1.92578554153 3.20018696785\n",
      "24 2000 1.50543749332 3.20077133179\n",
      "24 2020 1.39348995686 5.29567432404\n",
      "24 2040 1.79115593433 3.20054984093\n",
      "24 2060 1.46426212788 3.20002245903\n",
      "25 2080 1.922113657 3.20000004768\n",
      "25 2100 1.88578379154 3.20000100136\n",
      "25 2120 1.74686551094 3.20004105568\n",
      "25 2140 1.6067211628 3.21935892105\n",
      "26 2160 1.7015966177 3.20160031319\n",
      "26 2180 1.31613481045 3.20009851456\n",
      "26 2200 1.76967799664 3.2107591629\n",
      "26 2220 1.63610506058 3.20007658005\n",
      "26 2240 2.19844651222 3.20215606689\n",
      "27 2260 1.27635240555 3.20051956177\n",
      "27 2280 1.18330860138 3.20044946671\n",
      "27 2300 1.80991744995 3.2000041008\n",
      "27 2320 1.44549179077 3.20011520386\n",
      "28 2340 1.69389104843 3.20002174377\n",
      "28 2360 1.47871112823 3.20005679131\n",
      "28 2380 1.53856515884 3.97307920456\n",
      "28 2400 1.4787094593 3.2000310421\n",
      "29 2420 1.26989352703 3.20029997826\n",
      "29 2440 1.53742003441 23.9529418945\n",
      "29 2460 1.89716494083 3.20235586166\n",
      "29 2480 0.938460528851 3.20003056526\n",
      "30 2500 1.49670624733 3.20147585869\n",
      "30 2520 1.7828347683 3.20128631592\n",
      "30 2540 1.62663090229 3.20278573036\n",
      "30 2560 1.47805798054 3.2007894516\n",
      "31 2580 1.68030977249 8.46841716766\n",
      "31 2600 1.17032301426 3.20021057129\n",
      "31 2620 1.10952043533 6.90636348724\n",
      "31 2640 1.89032757282 3.2959485054\n",
      "32 2660 1.44597375393 3.20155119896\n",
      "32 2680 1.45128917694 3.36812496185\n",
      "32 2700 1.07817959785 3.2001619339\n",
      "32 2720 1.05345273018 3.20429801941\n",
      "33 2740 1.58367097378 3.20241832733\n",
      "33 2760 1.70965671539 3.20000457764\n",
      "33 2780 1.45495891571 3.20191645622\n",
      "33 2800 1.49339103699 3.21979045868\n",
      "33 2820 1.50106966496 3.20029115677\n",
      "34 2840 1.15362119675 3.20231485367\n",
      "34 2860 1.38349282742 3.20001316071\n",
      "34 2880 1.5097130537 3.20000386238\n",
      "34 2900 1.6224489212 3.20003318787\n",
      "35 2920 1.31287813187 3.20006895065\n",
      "35 2940 1.46123635769 3.20000267029\n",
      "35 2960 1.29133164883 3.20360803604\n",
      "35 2980 1.2948486805 3.20314121246\n",
      "36 3000 1.38783979416 3.20036268234\n",
      "36 3020 1.21706473827 3.20005083084\n",
      "36 3040 1.3177292347 3.21358728409\n",
      "36 3060 1.30596721172 3.20000052452\n",
      "37 3080 2.16397666931 3.20002698898\n",
      "37 3100 1.72444355488 3.20035624504\n",
      "37 3120 1.44836521149 3.20000219345\n",
      "37 3140 1.10126650333 3.20000171661\n",
      "38 3160 1.99400138855 4.82486104965\n",
      "38 3180 1.18243312836 3.20520925522\n",
      "38 3200 1.25353419781 3.29369163513\n",
      "38 3220 1.42956805229 3.2073738575\n",
      "39 3240 1.30523979664 3.20041298866\n",
      "39 3260 1.35434257984 3.2000412941\n",
      "39 3280 1.3201789856 3.20093488693\n",
      "39 3300 1.74883246422 3.20016789436\n",
      "39 3320 1.79788005352 3.20296955109\n",
      "40 3340 0.980550944805 3.20013546944\n",
      "40 3360 1.03297841549 3.20001983643\n",
      "40 3380 1.60533607006 3.21668624878\n",
      "40 3400 1.79037451744 3.20000362396\n",
      "41 3420 1.7950117588 3.20004081726\n",
      "41 3440 1.21293854713 3.30221056938\n",
      "41 3460 1.09699845314 5.6513338089\n",
      "41 3480 1.1856803894 3.20000267029\n",
      "42 3500 1.44026219845 3.20004820824\n",
      "42 3520 1.36458659172 3.23316526413\n",
      "42 3540 1.69240188599 3.28221654892\n",
      "42 3560 1.42293763161 3.2000067234\n",
      "43 3580 1.62610125542 3.21887636185\n",
      "43 3600 1.46134078503 3.2111325264\n",
      "43 3620 1.63300251961 3.21888780594\n",
      "43 3640 1.19907152653 3.20002031326\n",
      "44 3660 1.34314525127 3.20002961159\n",
      "44 3680 1.42794942856 3.21478652954\n",
      "44 3700 1.24354922771 3.20000314713\n",
      "44 3720 1.46094787121 3.20001292229\n",
      "45 3740 1.26156675816 3.20011997223\n",
      "45 3760 1.78544557095 3.20000362396\n",
      "45 3780 0.991400718689 3.20015764236\n",
      "45 3800 1.36991715431 3.23276782036\n",
      "46 3820 1.19719934464 3.21947526932\n",
      "46 3840 1.15665316582 3.20056867599\n",
      "46 3860 1.30027365685 3.20011663437\n",
      "46 3880 0.855872392654 3.20174455643\n",
      "46 3900 0.750111341476 3.4308218956\n",
      "47 3920 1.34819340706 3.20003128052\n",
      "47 3940 1.58730781078 3.20511770248\n",
      "47 3960 1.35538065434 3.2048895359\n",
      "47 3980 1.4442460537 3.2000041008\n",
      "48 4000 1.39978945255 3.20737838745\n",
      "48 4020 1.6124856472 3.20041799545\n",
      "48 4040 1.54057955742 3.22284173965\n",
      "48 4060 0.997877717018 3.20057010651\n",
      "49 4080 1.20189666748 3.20004749298\n",
      "49 4100 1.19756269455 3.22617149353\n",
      "49 4120 1.37929582596 3.20007252693\n",
      "49 4140 1.25801765919 3.20002031326\n",
      "50 4160 1.59227573872 3.20038032532\n",
      "50 4180 1.04216635227 3.20104598999\n",
      "50 4200 1.11709022522 3.20763373375\n",
      "50 4220 1.32233309746 3.20070004463\n",
      "51 4240 1.29520773888 3.20000100136\n",
      "51 4260 1.35384082794 3.20008587837\n",
      "51 4280 1.21907496452 3.20171546936\n",
      "51 4300 1.06376671791 3.20063924789\n",
      "52 4320 0.85691177845 6.17537117004\n",
      "52 4340 1.0541267395 3.20188856125\n",
      "52 4360 1.49308395386 3.20007443428\n",
      "52 4380 1.05260431767 3.20000720024\n",
      "53 4400 1.04314339161 3.20013785362\n",
      "53 4420 1.11671674252 3.20005130768\n",
      "53 4440 1.45703876019 3.20000004768\n",
      "53 4460 1.19960618019 3.20008993149\n",
      "53 4480 1.23002266884 3.20000529289\n",
      "54 4500 1.4061498642 3.20010304451\n",
      "54 4520 1.11809945107 4.69123458862\n",
      "54 4540 1.38534152508 3.2004365921\n",
      "54 4560 1.36324501038 3.20000886917\n",
      "55 4580 1.20651423931 4.51190328598\n",
      "55 4600 1.30666029453 3.20053076744\n",
      "55 4620 1.44621098042 3.20032739639\n",
      "55 4640 1.05412614346 3.25308179855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 4660 1.56210649014 3.20021438599\n",
      "56 4680 1.20399689674 3.21005868912\n",
      "56 4700 0.958921253681 3.20218420029\n",
      "56 4720 1.07291460037 3.20137095451\n",
      "57 4740 1.22780025005 3.2063202858\n",
      "57 4760 1.45863986015 3.20623612404\n",
      "57 4780 1.35540318489 3.20000433922\n",
      "57 4800 1.5948060751 3.20000433922\n",
      "58 4820 1.02167105675 3.20008468628\n",
      "58 4840 1.10582411289 3.20039844513\n",
      "58 4860 0.968405067921 3.20026731491\n",
      "58 4880 1.25031018257 3.20013213158\n",
      "59 4900 1.48752987385 3.20004701614\n",
      "59 4920 1.21341502666 3.20010137558\n",
      "59 4940 1.4299274683 3.20020008087\n",
      "59 4960 1.4248598814 3.20018267632\n",
      "59 4980 1.15742170811 3.20003342628\n",
      "60 5000 0.909351944923 3.20000123978\n",
      "60 5020 1.07333731651 3.24005889893\n",
      "60 5040 1.16092503071 3.60380148888\n",
      "60 5060 1.16445803642 3.22538471222\n",
      "61 5080 1.15579652786 3.20000004768\n",
      "61 5100 1.22188723087 3.22069120407\n",
      "61 5120 1.0725774765 8.1193523407\n",
      "61 5140 1.11497557163 3.20000123978\n",
      "62 5160 1.17383658886 3.32047986984\n",
      "62 5180 1.3897010088 3.20004415512\n",
      "62 5200 1.09797251225 3.20044636726\n",
      "62 5220 1.40685462952 3.20001745224\n",
      "63 5240 1.10188698769 3.20000290871\n",
      "63 5260 1.53918778896 3.20000696182\n",
      "63 5280 1.04403162003 3.2021279335\n",
      "63 5300 0.981768965721 3.20749592781\n",
      "64 5320 0.880404949188 6.00838470459\n",
      "64 5340 0.894061088562 3.20000505447\n",
      "64 5360 1.2494354248 3.20021080971\n",
      "64 5380 1.06134927273 3.20000600815\n",
      "65 5400 1.27742946148 3.2000002861\n",
      "65 5420 0.831867575645 3.2000014782\n",
      "65 5440 1.30648720264 3.20000696182\n",
      "65 5460 1.28616476059 3.20042848587\n",
      "66 5480 1.48354411125 3.20012617111\n",
      "66 5500 1.23626112938 3.2000336647\n",
      "66 5520 1.21505212784 3.2000477314\n",
      "66 5540 1.28708410263 3.20010781288\n",
      "66 5560 0.853583574295 3.20000982285\n",
      "67 5580 0.936863780022 3.20000100136\n",
      "67 5600 0.912288248539 3.61095476151\n",
      "67 5620 1.39598679543 3.2000014782\n",
      "67 5640 0.855685472488 3.20036840439\n",
      "68 5660 1.09932804108 3.20005774498\n",
      "68 5680 1.43106842041 3.20000314713\n",
      "68 5700 1.32925724983 3.26888847351\n",
      "68 5720 1.17353320122 3.20009803772\n",
      "69 5740 1.29971444607 3.20470428467\n",
      "69 5760 1.24367368221 3.20000767708\n",
      "69 5780 1.34741425514 3.20001173019\n",
      "69 5800 1.0176949501 3.20009708405\n",
      "70 5820 1.2684649229 3.20127534866\n",
      "70 5840 1.28566777706 3.20055246353\n",
      "70 5860 1.50975704193 3.2000002861\n",
      "70 5880 1.06159853935 3.20011854172\n",
      "71 5900 0.976928472519 3.20431041718\n",
      "71 5920 1.16047024727 3.20001006126\n",
      "71 5940 1.19818854332 3.20000696182\n",
      "71 5960 1.3869395256 3.20005011559\n",
      "72 5980 0.849817276001 3.20000004768\n",
      "72 6000 1.21863043308 3.20017385483\n",
      "72 6020 1.3844666481 3.20003962517\n",
      "72 6040 0.704568862915 3.20102357864\n",
      "73 6060 1.76648533344 3.20000004768\n",
      "73 6080 0.991092324257 3.20006895065\n",
      "73 6100 1.20374667645 3.20002746582\n",
      "73 6120 1.12622642517 3.20000529289\n",
      "73 6140 1.18049860001 3.20006108284\n",
      "74 6160 1.15769791603 3.3890004158\n",
      "74 6180 1.26104581356 3.20036697388\n",
      "74 6200 1.26709425449 3.2000002861\n",
      "74 6220 1.28699123859 3.2005739212\n",
      "75 6240 1.41275846958 3.20138382912\n",
      "75 6260 1.52465009689 3.27793836594\n",
      "75 6280 1.27061080933 3.2000067234\n",
      "75 6300 1.20174193382 3.20002698898\n",
      "76 6320 1.4522562027 3.21164894104\n",
      "76 6340 1.20170426369 3.20203876495\n",
      "76 6360 0.892909765244 3.2000002861\n",
      "76 6380 1.34349215031 3.20081067085\n",
      "77 6400 1.30394768715 3.22756719589\n",
      "77 6420 1.36121571064 3.20002841949\n",
      "77 6440 1.3133995533 3.20000052452\n",
      "77 6460 1.12609243393 3.20006775856\n",
      "78 6480 1.27255547047 3.20000076294\n",
      "78 6500 1.19340395927 3.20000958443\n",
      "78 6520 1.22624015808 3.20003890991\n",
      "78 6540 1.07519471645 3.2012629509\n",
      "79 6560 1.15099334717 3.20357346535\n",
      "79 6580 1.33167684078 3.20036220551\n",
      "79 6600 1.36962771416 3.20000004768\n",
      "79 6620 1.04244613647 3.20074415207\n",
      "79 6640 0.977629601955 3.20003390312\n",
      "80 6660 1.15538680553 4.15847444534\n",
      "80 6680 1.55349433422 3.20002245903\n",
      "80 6700 1.11663675308 3.20003604889\n",
      "80 6720 1.13448536396 3.20023727417\n",
      "81 6740 1.28213012218 3.2000041008\n",
      "81 6760 0.951467752457 3.20000338554\n",
      "81 6780 1.20693910122 3.20001173019\n",
      "81 6800 1.12936353683 3.20799398422\n",
      "82 6820 1.47819757462 3.28819417953\n",
      "82 6840 1.03456151485 9.11887359619\n",
      "82 6860 1.10573077202 3.20002889633\n",
      "82 6880 0.812571763992 3.20084571838\n",
      "83 6900 1.17275464535 3.21024441719\n",
      "83 6920 1.32138288021 3.20013976097\n",
      "83 6940 1.15757083893 3.20012784004\n",
      "83 6960 1.17126750946 3.20010781288\n",
      "84 6980 1.36088144779 3.20002579689\n",
      "84 7000 1.31639313698 3.20008301735\n",
      "84 7020 1.11497080326 3.20000863075\n",
      "84 7040 1.17421340942 3.20000100136\n",
      "85 7060 1.12277138233 3.20002436638\n",
      "85 7080 1.03782725334 3.20000100136\n",
      "85 7100 1.07081627846 3.2107155323\n",
      "85 7120 1.22333729267 3.20000100136\n",
      "86 7140 1.09423458576 3.20122528076\n",
      "86 7160 1.43684792519 3.20041894913\n",
      "86 7180 1.26685857773 3.2000451088\n",
      "86 7200 1.27712166309 3.2199420929\n",
      "86 7220 1.12642896175 3.20000123978\n",
      "87 7240 1.14336597919 3.2104370594\n",
      "87 7260 0.999091148376 3.20000839233\n",
      "87 7280 1.06995201111 3.20002889633\n",
      "87 7300 0.863231480122 3.20304107666\n",
      "88 7320 1.27712547779 3.20002174377\n",
      "88 7340 0.770660102367 3.20000362396\n",
      "88 7360 1.20014691353 9.28793048859\n",
      "88 7380 1.09091639519 3.20347738266\n",
      "89 7400 1.14098954201 3.20000576973\n",
      "89 7420 0.73710745573 3.20000576973\n",
      "89 7440 1.10810804367 3.20000195503\n",
      "89 7460 1.1943782568 3.2000014782\n",
      "90 7480 1.33669745922 3.20058846474\n",
      "90 7500 0.728148877621 4.68186950684\n",
      "90 7520 1.34422361851 3.20000386238\n",
      "90 7540 1.35773432255 3.2000041008\n",
      "91 7560 1.12784969807 3.20001363754\n",
      "91 7580 1.20217347145 3.20015501976\n",
      "91 7600 1.31078135967 3.20308542252\n",
      "91 7620 0.90873414278 3.2002530098\n",
      "92 7640 0.933641433716 3.20000004768\n",
      "92 7660 1.56217765808 3.20003271103\n",
      "92 7680 0.993786752224 3.2000772953\n",
      "92 7700 1.37808406353 3.20009064674\n",
      "93 7720 1.57916784286 3.20023727417\n",
      "93 7740 0.804390490055 3.2000169754\n",
      "93 7760 1.12652862072 3.20001244545\n",
      "93 7780 1.0811522007 3.20008945465\n",
      "93 7800 1.06977272034 3.20024561882\n",
      "94 7820 1.0696824789 3.20018601418\n",
      "94 7840 1.26249396801 3.20000767708\n",
      "94 7860 1.28825211525 3.20000052452\n",
      "94 7880 0.930442214012 3.20000076294\n",
      "95 7900 1.32153046131 3.20158362389\n",
      "95 7920 1.0268021822 3.20136928558\n",
      "95 7940 1.00422430038 3.20270037651\n",
      "95 7960 0.914618194103 3.20000886917\n",
      "96 7980 1.2399494648 3.20000195503\n",
      "96 8000 0.881528437138 4.39405345917\n",
      "96 8020 1.42168724537 3.20000004768\n",
      "96 8040 1.2971765995 3.20000076294\n",
      "97 8060 1.02689564228 3.2183971405\n",
      "97 8080 1.13682961464 3.20000004768\n",
      "97 8100 1.20854282379 3.21470379829\n",
      "97 8120 1.01137542725 3.20000100136\n",
      "98 8140 0.965899884701 3.20017266273\n",
      "98 8160 1.30485534668 3.20000338554\n",
      "98 8180 1.08849394321 3.20000362396\n",
      "98 8200 1.30284881592 3.20021724701\n",
      "99 8220 0.978112339973 3.20000815392\n",
      "99 8240 1.1249152422 3.20038795471\n",
      "99 8260 1.30713617802 3.2000014782\n",
      "99 8280 0.670561015606 3.20002055168\n",
      "99 8300 0.727125763893 3.20029139519\n",
      "100 8320 0.973894953728 3.20000243187\n",
      "100 8340 1.07483422756 3.20000004768\n",
      "100 8360 1.17209291458 3.2000143528\n",
      "100 8380 1.06503677368 3.20000171661\n",
      "101 8400 1.13231384754 3.20002126694\n",
      "101 8420 1.33365345001 3.20103192329\n",
      "101 8440 1.17089271545 3.2594537735\n",
      "101 8460 0.746905326843 3.20000195503\n",
      "102 8480 1.25091075897 3.20129799843\n",
      "102 8500 1.07663404942 3.20025610924\n",
      "102 8520 1.05940282345 3.20000505447\n",
      "102 8540 0.863321363926 3.2003891468\n",
      "103 8560 0.853689730167 3.25142884254\n",
      "103 8580 0.769913494587 3.20000815392\n",
      "103 8600 1.4123044014 3.2000002861\n",
      "103 8620 1.22815310955 3.20030641556\n",
      "104 8640 1.23429596424 3.20120358467\n",
      "104 8660 0.871964871883 3.20000004768\n",
      "104 8680 1.33428740501 3.20921206474\n",
      "104 8700 1.79454696178 3.20000195503\n",
      "105 8720 1.16119158268 3.2000169754\n",
      "105 8740 0.945443153381 3.20000553131\n",
      "105 8760 1.22120881081 3.20001149178\n",
      "105 8780 1.01317083836 3.20000052452\n",
      "106 8800 1.22471892834 3.20000100136\n",
      "106 8820 0.682042121887 3.20003223419\n",
      "106 8840 0.754835128784 3.20000290871\n",
      "106 8860 1.00977480412 3.20001220703\n",
      "106 8880 0.999681770802 3.2000837326\n",
      "107 8900 1.18834269047 3.20000004768\n",
      "107 8920 0.90104663372 3.20002102852\n",
      "107 8940 1.05681180954 3.30537724495\n",
      "107 8960 1.09484708309 3.20003652573\n",
      "108 8980 0.712130427361 3.20000100136\n",
      "108 9000 1.2109259367 3.20025134087\n",
      "108 9020 0.900137901306 3.20000839233\n",
      "108 9040 0.806563019753 4.88559532166\n",
      "109 9060 1.27623999119 3.20000767708\n",
      "109 9080 0.986774384975 3.20029497147\n",
      "109 9100 0.825530707836 3.2000079155\n",
      "109 9120 1.0707950592 3.20000720024\n",
      "110 9140 1.13555574417 3.20000123978\n",
      "110 9160 1.56272494793 3.20000362396\n",
      "110 9180 0.954526305199 3.20000171661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 9200 1.09458827972 3.20000004768\n",
      "111 9220 1.19415307045 3.20000910759\n",
      "111 9240 1.29559850693 3.20187115669\n",
      "111 9260 0.547906637192 3.20854210854\n",
      "111 9280 0.672657191753 3.20000195503\n",
      "112 9300 1.07311201096 3.20000243187\n",
      "112 9320 1.11339449883 3.20000004768\n",
      "112 9340 0.741924881935 3.20016121864\n",
      "112 9360 1.15370118618 3.20000052452\n",
      "113 9380 1.05205905437 3.20000004768\n",
      "113 9400 1.29414522648 3.20032000542\n",
      "113 9420 1.33315384388 3.20015954971\n",
      "113 9440 1.08064770699 3.20001626015\n",
      "113 9460 1.01889765263 3.20005536079\n",
      "114 9480 1.49855208397 3.34735751152\n",
      "114 9500 1.228328228 3.20000100136\n",
      "114 9520 1.02544188499 3.20100140572\n",
      "114 9540 0.998567461967 3.2002723217\n",
      "115 9560 1.00564754009 3.20000004768\n",
      "115 9580 1.39423525333 3.20000243187\n",
      "115 9600 1.11284923553 3.20009231567\n",
      "115 9620 0.987979888916 3.20000934601\n",
      "116 9640 0.600915968418 3.20764970779\n",
      "116 9660 0.987036764622 3.20000004768\n",
      "116 9680 1.1396125555 3.2000579834\n",
      "116 9700 1.06485772133 3.20001292229\n",
      "117 9720 1.2642083168 3.20013618469\n",
      "124 10300 0.981961131096 3.20000004768\n",
      "124 10320 0.918593049049 3.20070409775\n",
      "124 10340 1.14027488232 3.20000004768\n",
      "124 10360 1.06109714508 3.20007252693\n",
      "125 10380 1.02999579906 3.21449065208\n",
      "125 10400 0.896709501743 3.20000004768\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize,dis_category=5)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+dis_category*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, dis_category)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss+3.2\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize,dis_category=5)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+dis_category*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, dis_category)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss+3.2\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell_2.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_seg_3.png',nrow=dis_category,normalize=True)\n",
    "            vutils.save_image(image_, 'real_samples_seg_3.png',normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
