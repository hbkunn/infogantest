{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26336, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-6:\n",
      "Process PoolWorker-3:\n",
      "Process PoolWorker-8:\n",
      "Process PoolWorker-12:\n",
      "Process PoolWorker-11:\n",
      "Process PoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-1:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-10:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process PoolWorker-7:\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-4:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "    self.run()\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "    task = get()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "    task = get()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    task = get()\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    task = get()\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "    task = get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "pool = ThreadPool(12) \n",
    "root_dir = '/disk1/cell_segment_save/'\n",
    "npyList = os.listdir(root_dir)\n",
    "npyList = [root_dir+n for n in npyList]\n",
    "result = pool.map(np.load, npyList)\n",
    "result = np.concatenate(result)\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "result_list = [result] \n",
    "seq = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq2= iaa.Sequential([\n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.Flipud(1), \n",
    "    iaa.Fliplr(1), \n",
    "    #iaa.ContrastNormalization((1.40,1.60)),\n",
    "])\n",
    "\n",
    "result_list.append(seq.augment_images(result))\n",
    "result_list.append(seq2.augment_images(result))\n",
    "result_list.append(seq3.augment_images(result))\n",
    "result = np.concatenate(result_list,axis=0)\n",
    "print(result.shape)\n",
    "\n",
    "X = np.asarray([x.transpose((2,0,1)) for x in result])\n",
    "X = X.astype(np.float32)/(255.0/2) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import functions\n",
    "batchsize = 64\n",
    "\n",
    "#X_train = bonemarrow_cell()\n",
    "#X_train = functions.bonemarrow_cell()\n",
    "X_train = X\n",
    "X_label = torch.LongTensor(np.zeros((X_train.shape[0]),dtype=int))\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "train = torch.utils.data.TensorDataset(X_train,X_label)\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batchsize, num_workers=4)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "test,test_label = functions.get_test()\n",
    "test = test[np.argsort(test_label)]\n",
    "test_label = test_label[np.argsort(test_label)]\n",
    "\n",
    "Test_label = torch.LongTensor(test_label.astype('int'))\n",
    "Test = torch.FloatTensor(test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Test,Test_label), shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (initial.133-256.convt): ConvTranspose2d(133, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (initial.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (initial.256.relu): ReLU (inplace)\n",
      "    (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.128.relu): ReLU (inplace)\n",
      "    (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid.64.relu): ReLU (inplace)\n",
      "    (final.64-3.convt): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final.3.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (initial.conv.3-64): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rand = 128\n",
    "dis = 1\n",
    "dis_category = 5\n",
    "\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ngf = 64, n_extra_layers=0):\n",
    "        super(_netG, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "netG = _netG(nz = rand+dis*dis_category)\n",
    "print (netG)\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR  ======================== \"\"\"\n",
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, isize = 32, nz = 149, nc = 3, ndf = 64, n_extra_layers=0):\n",
    "        super(_netD, self).__init__()\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            #main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            #nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            #main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            #nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "       # main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "       #                 nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = _netD(nz = rand+dis*dis_category)\n",
    "print (netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_D (\n",
      "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n",
      "_netD_Q (\n",
      "  (conv): Conv2d(256, 5, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (softmax): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _netD_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD_D, self).__init__()\n",
    "        self.conv = nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class _netD_Q(nn.Module):\n",
    "    def __init__(self, nd = 10):\n",
    "        super(_netD_Q, self).__init__()\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv = nn.Conv2d(256, nd, 4, 1, 0, bias=False)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "       # x = x.view(64,10)\n",
    "        return x\n",
    "\n",
    "netD_D = _netD_D()\n",
    "netD_Q = _netD_Q(dis_category)\n",
    "print(netD_D)\n",
    "print(netD_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD_D (\n",
       "  (conv): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD, netG, netD_D, netD_Q = netD.cuda(), netG.cuda(), netD_D.cuda(), netD_Q.cuda()\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "netD_Q.apply(weights_init)\n",
    "netD_D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam([\n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_D.parameters()}\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), 2e-4, betas=(0.5, 0.9))\n",
    " \n",
    "optimizerQ = optim.Adam([\n",
    "                {'params': netG.parameters()},            \n",
    "                {'params': netD.parameters()},\n",
    "                {'params': netD_Q.parameters()},\n",
    "            ], 2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "input = torch.FloatTensor(batchsize, 3, 32, 32)\n",
    "noise = torch.FloatTensor(batchsize, rand+dis_category*dis,1 ,1 )\n",
    "\n",
    "fixed_noise = torch.FloatTensor(np.random.multinomial(batchsize, 10*[0.1], size=1))\n",
    "c = torch.randn(batchsize, 10)\n",
    "z = torch.randn(batchsize, rand)\n",
    "\n",
    "label = torch.FloatTensor(1)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_logli = nn.NLLLoss(size_average=False)\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "criterion, criterion_logli, criterion_mse = criterion.cuda(), criterion_logli.cuda(), criterion_mse.cuda()\n",
    "input, label = input.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "z, c = z.cuda(), c.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_c(batchsize, dis_category=10):\n",
    "    rand_c = np.zeros((batchsize,dis_category),dtype='float32')\n",
    "    for i in range(0,batchsize):\n",
    "        rand = np.random.multinomial(1, dis_category*[1/float(dis_category)], size=1)\n",
    "        rand_c[i] = rand\n",
    "\n",
    "    label_c = np.argmax(rand_c,axis=1)\n",
    "    label_c = torch.LongTensor(label_c.astype('int'))\n",
    "    rand_c = torch.from_numpy(rand_c.astype('float32'))\n",
    "    return rand_c,label_c\n",
    "\n",
    "def zero_grad():\n",
    "    netD.zero_grad()\n",
    "    netD_Q.zero_grad()\n",
    "    netD_D.zero_grad()\n",
    "    netG.zero_grad()\n",
    "\n",
    "def weight_clamp():\n",
    "    for p in netD.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    for p in netD_D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_2.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "    #for p in netD_Q_3.parameters():\n",
    "            #p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "def fix_noise(dis=1, rand=128, dis_category=10, row=10):\n",
    "    \n",
    "    fixed_z = np.random.randn(row, rand).repeat(dis_category,axis=0)\n",
    "    changing_dis = np.zeros((row*dis_category,dis_category),dtype = np.float32)\n",
    "    list = [n for n in range(0,dis_category)]*row\n",
    "    for i in range(0,row*dis_category):\n",
    "        changing_dis[i,list[i]] = 1\n",
    "    map1 = np.concatenate((changing_dis,fixed_z),axis=1)\n",
    "    lst = [map1.astype(np.float32)]\n",
    "    return lst[0].reshape(row*dis_category,rand+dis*dis_category,1,1)\n",
    "\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "one = one.cuda()\n",
    "mone = mone.cuda()\n",
    "fixed_noise = torch.from_numpy(fix_noise(dis_category=5, row=30)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef state_one():\\ndef state_two():\\ndef state_three():\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_gradient_penalty(netD_D, netD, real_data, fake_data,lamda,batch_size):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    #print (real_data.size())\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda()\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    interpolates = interpolates.cuda()\n",
    "    interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD_D(netD(interpolates))#.view(batch_size,-1)\n",
    "\n",
    "    gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              create_graph=True)\n",
    "    \n",
    "    #gradients, = autograd.grad(outputs=disc_interpolates.sum(), inputs=interpolates,\n",
    "                              #grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
    "                              #create_graph=True, retain_graph=True, only_inputs=True)\n",
    "    #gradients*gradients\n",
    "    \n",
    "    gradient_penalty = ((gradients.view(batch_size,-1).norm(2, dim=1) - 1) ** 2).mean()* lamda\n",
    "    return gradient_penalty\n",
    "\n",
    "'''\n",
    "def state_one():\n",
    "def state_two():\n",
    "def state_three():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20 1.27425694466 0.000107169151306\n",
      "0 40 1.51756715775 9.02414321899e-05\n",
      "0 60 1.60492181778 4.86373901367e-05\n",
      "0 80 1.52202057838 0.0195163488388\n",
      "1 100 1.37649929523 2.51342582703\n",
      "1 120 0.939636588097 6.12735748291e-05\n",
      "1 140 1.47229456902 0.000457167625427\n",
      "1 160 1.56773674488 0.0489934682846\n",
      "2 180 1.4648514986 6.83069229126e-05\n",
      "2 200 1.40035867691 2.16960906982e-05\n",
      "2 220 1.57432639599 3.45706939697e-06\n",
      "2 240 1.48653030396 4.76837158203e-06\n",
      "3 260 1.60800886154 0.000683069229126\n",
      "3 280 1.14482927322 0.00102746486664\n",
      "3 300 1.14656507969 4.29153442383e-06\n",
      "3 320 1.52410030365 0.000246286392212\n",
      "4 340 1.81710672379 0.000125408172607\n",
      "4 360 1.62865960598 0.138378977776\n",
      "4 380 1.29099178314 0.00851309299469\n",
      "4 400 1.75954985619 8.42809677124e-05\n",
      "5 420 1.7249546051 0.000489234924316\n",
      "5 440 1.17118668556 9.53674316406e-07\n",
      "5 460 1.16953015327 0.204828619957\n",
      "5 480 1.38005578518 0.000370979309082\n",
      "6 500 1.70884823799 0.122345805168\n",
      "6 520 1.2184548378 0.000129818916321\n",
      "6 540 1.59911489487 0.000134944915771\n",
      "6 560 1.50445199013 1.28746032715e-05\n",
      "6 580 1.48562300205 0.00632005929947\n",
      "7 600 1.14632952213 0.000143885612488\n",
      "7 620 1.14463305473 0.0145223140717\n",
      "7 640 1.82081544399 0.000371098518372\n",
      "7 660 1.2908103466 0.0117644071579\n",
      "8 680 1.53584349155 0.00410151481628\n",
      "8 700 1.34034824371 0.0847988128662\n",
      "8 720 1.51553714275 0.0124441981316\n",
      "8 740 1.27174699306 1.37754607201\n",
      "9 760 1.43609619141 0.000234603881836\n",
      "9 780 1.42166101933 0.00884222984314\n",
      "9 800 1.05232954025 0.134656369686\n",
      "9 820 1.73401725292 4.1127204895e-05\n",
      "10 840 1.4850307703 3.19480895996e-05\n",
      "10 860 1.90686702728 0.000481247901917\n",
      "10 880 2.03791475296 2.99215316772e-05\n",
      "10 900 1.64078938961 0.00469088554382\n",
      "11 920 1.00641834736 0.000289678573608\n",
      "11 940 0.841955423355 0.404877901077\n",
      "11 960 1.22675061226 0.000399112701416\n",
      "11 980 1.53588521481 0.0010678768158\n",
      "12 1000 1.3072681427 0.0186786651611\n",
      "12 1020 1.70479333401 0.107489928603\n",
      "12 1040 1.3495361805 0.000456213951111\n",
      "12 1060 1.23430407047 2.13384628296e-05\n",
      "13 1080 1.62295198441 4.0055103302\n",
      "13 1100 1.2367566824 0.00089406222105\n",
      "13 1120 1.25242471695 0.000384926795959\n",
      "13 1140 1.2460488081 1.20401382446e-05\n",
      "13 1160 1.07901573181 0.000250339508057\n",
      "14 1180 1.4569003582 0.743310332298\n",
      "14 1200 1.65703976154 0.0518474578857\n",
      "14 1220 1.2393835783 0.00191956758499\n",
      "14 1240 0.785271883011 0.0378463864326\n",
      "15 1260 1.34479951859 0.000661253929138\n",
      "15 1280 1.38218021393 0.0271887779236\n",
      "15 1300 1.58615219593 3.79323959351\n",
      "15 1320 0.929474592209 3.54781413078\n",
      "16 1340 1.26961910725 3.09944152832e-06\n",
      "16 1360 1.29898476601 5.3882598877e-05\n",
      "16 1380 0.720021784306 0.0272390842438\n",
      "16 1400 0.962599277496 0.00130217522383\n",
      "17 1420 1.33431243896 0.000608801841736\n",
      "17 1440 1.40348529816 1.44243240356e-05\n",
      "17 1460 1.38735020161 0.155408382416\n",
      "17 1480 1.0059119463 4.29153442383e-06\n",
      "18 1500 1.35283780098 5.76436805725\n",
      "18 1520 1.53103423119 4.76837158203e-07\n",
      "18 1540 1.32111263275 3.72524023056\n",
      "18 1560 1.28481268883 0.0022234916687\n",
      "19 1580 1.03400695324 0.00101685523987\n",
      "19 1600 1.49249470234 0.372334182262\n",
      "19 1620 1.37407612801 0.00351524353027\n",
      "19 1640 1.06803834438 0.00591313838959\n",
      "19 1660 0.354297935963 0.000197768211365\n",
      "20 1680 1.25594890118 0.892546355724\n",
      "20 1700 1.1668804884 5.91278076172e-05\n",
      "20 1720 1.31939744949 0.000210642814636\n",
      "20 1740 1.3156632185 3.2901763916e-05\n",
      "21 1760 1.19057869911 4.13656234741e-05\n",
      "21 1780 1.17456519604 0.0102699697018\n",
      "21 1800 1.28560101986 0.0202194452286\n",
      "21 1820 1.18874311447 0.000111937522888\n",
      "22 1840 1.16155993938 4.86373901367e-05\n",
      "22 1860 1.32456803322 2.59876251221e-05\n",
      "22 1880 1.27637410164 6.66161251068\n",
      "22 1900 1.38528513908 1.83123886585\n",
      "23 1920 1.32997310162 8.34465026855e-06\n",
      "23 1940 1.37014698982 0.000375032424927\n",
      "23 1960 1.29468417168 0.000188589096069\n",
      "23 1980 1.1443580389 2.62260437012e-06\n",
      "24 2000 0.893454730511 0.323788553476\n",
      "24 2020 0.921740174294 0.000172957777977\n",
      "24 2040 0.994036316872 1.31130218506e-06\n",
      "24 2060 1.28312933445 0.0135703086853\n",
      "25 2080 1.13363611698 1.49011611938e-05\n",
      "25 2100 1.25266933441 5.9858174324\n",
      "25 2120 1.2640529871 0.000186920166016\n",
      "25 2140 1.26826941967 2.11000442505e-05\n",
      "26 2160 1.6687104702 0.000481724739075\n",
      "26 2180 1.22980749607 2.85695362091\n",
      "26 2200 1.43507409096 9.53674316406e-07\n",
      "26 2220 1.26741492748 0.0101435184479\n",
      "26 2240 1.22368550301 0.0819170475006\n",
      "27 2260 1.69056868553 7.15255737305e-07\n",
      "27 2280 1.35969531536 0.0127395391464\n",
      "27 2300 1.41144227982 3.21865081787e-06\n",
      "27 2320 1.31539952755 3.09944152832e-06\n",
      "28 2340 1.28820049763 7.95125961304e-05\n",
      "28 2360 1.27835524082 1.37090682983e-05\n",
      "28 2380 1.36037266254 0.0256844162941\n",
      "28 2400 1.15211606026 2.74181365967e-06\n",
      "29 2420 1.50011456013 0.000232934951782\n",
      "29 2440 1.22888863087 0.000211358070374\n",
      "29 2460 1.43875622749 0.0017706155777\n",
      "29 2480 0.843302607536 0.000212430953979\n",
      "30 2500 1.68952727318 0.000162839889526\n",
      "30 2520 1.25349807739 1.57356262207e-05\n",
      "30 2540 0.910030722618 0.00060772895813\n",
      "30 2560 1.4237023592 1.85966491699e-05\n",
      "31 2580 0.871515750885 0.0130729675293\n",
      "31 2600 1.47277212143 0.000256538391113\n",
      "31 2620 1.53572225571 0.00145280361176\n",
      "31 2640 1.17058074474 9.25064086914e-05\n",
      "32 2660 1.15375506878 0.000125408172607\n",
      "32 2680 0.946923613548 0.00219035148621\n",
      "32 2700 1.76279830933 1.68085098267e-05\n",
      "32 2720 0.897034406662 0.000148057937622\n",
      "33 2740 1.08320879936 0.000267148017883\n",
      "33 2760 1.55327689648 5.32865524292e-05\n",
      "33 2780 1.32190775871 0.00734758377075\n",
      "33 2800 1.15402460098 0.000151395797729\n",
      "33 2820 1.10919928551 0.00165605545044\n",
      "34 2840 1.35838890076 1.90734863281e-06\n",
      "34 2860 1.34454417229 5.12261867523\n",
      "34 2880 2.30822467804 0.0569381713867\n",
      "34 2900 1.1907800436 0.0183613300323\n",
      "35 2920 1.07316923141 0.0128283500671\n",
      "35 2940 0.950053095818 0.090744972229\n",
      "35 2960 1.21873760223 0.000116944313049\n",
      "35 2980 1.13810968399 1.47819519043e-05\n",
      "36 3000 1.27190947533 0.000102639198303\n",
      "36 3020 1.22263741493 2.38418579102e-07\n",
      "36 3040 1.26940345764 4.76837158203e-06\n",
      "36 3060 0.871905446053 0.000135183334351\n",
      "37 3080 1.30077803135 0.0\n",
      "37 3100 1.50623297691 3.09944152832e-06\n",
      "37 3120 1.19239020348 2.74181365967e-06\n",
      "37 3140 1.22088885307 0.000432968139648\n",
      "38 3160 1.44456136227 0.00215888023376\n",
      "38 3180 1.41820168495 0.0201395750046\n",
      "38 3200 1.25612139702 2.86102294922e-06\n",
      "38 3220 1.44431316853 1.32322311401e-05\n",
      "39 3240 0.988447248936 0.000515460968018\n",
      "39 3260 1.30518782139 7.21454048157\n",
      "39 3280 1.3325138092 0.000200986862183\n",
      "39 3300 1.28996050358 5.96046447754e-06\n",
      "39 3320 0.893730759621 0.0929876565933\n",
      "40 3340 1.4130641222 4.05311584473e-06\n",
      "40 3360 1.18887209892 2.21729278564e-05\n",
      "40 3380 1.49709868431 2.00271606445e-05\n",
      "40 3400 1.24189543724 2.25305557251e-05\n",
      "41 3420 1.39242732525 0.000865459442139\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "gen_iterations = 0\n",
    "lamda = 10\n",
    "\n",
    "for epoch in range(100000):\n",
    "\n",
    "    dataiter = iter(train_loader)\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(train_loader):\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        for iter_d in range(0,5):\n",
    "            if i >=len(train_loader):\n",
    "                continue\n",
    "                                \n",
    "            image_, _ = dataiter.next()\n",
    "            _batchsize = image_.size(0)\n",
    "            image_ = image_.cuda()\n",
    "            i +=1\n",
    "            input.resize_as_(image_).copy_(image_)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            #train with real\n",
    "            errD_real = netD_D(netD(inputv)).mean()\n",
    "            errD_real.backward(mone)\n",
    "            \n",
    "            # train with fake\n",
    "            rand_c,label_c = sample_c(_batchsize,dis_category=5)\n",
    "            rand_c = rand_c.cuda()\n",
    "            c.resize_as_(rand_c).copy_(rand_c)\n",
    "            z.resize_(_batchsize, rand, 1, 1).normal_(0, 1)\n",
    "            noise = torch.cat([c,z],1)\n",
    "            noise_resize = noise.view(_batchsize,rand+dis_category*dis,1,1)\n",
    "            noisev = Variable(noise_resize, volatile = True)\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD_D(netD(inputv)).mean()\n",
    "            errD_fake.backward(one)\n",
    "            \n",
    "            # train with gradient penalty\n",
    "            gradient_penalty = calc_gradient_penalty(netD_D,netD, input, fake.data,lamda,_batchsize)\n",
    "            gradient_penalty.backward()\n",
    "            \n",
    "            D_cost = -errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # update Q\n",
    "            zero_grad()\n",
    "            noisev = Variable(noise_resize)\n",
    "            G_sample = netG(noisev)\n",
    "            Q_c_given_x = netD_Q(netD(G_sample)).view(_batchsize, dis_category)\n",
    "            crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "            mi_loss = 1*crossent_loss\n",
    "            mi_loss.backward()\n",
    "            optimizerQ.step()\n",
    "\n",
    "    # update G  \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = False \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = False \n",
    "\n",
    "        zero_grad()\n",
    "        rand_c,label_c = sample_c(batchsize,dis_category=5)\n",
    "        rand_c = rand_c.cuda()\n",
    "        c.resize_as_(rand_c).copy_(rand_c)\n",
    "        z.resize_(batchsize, rand, 1, 1).normal_(0, 1)\n",
    "        noise = torch.cat([c,z],1)\n",
    "        noise_resize = noise.view(batchsize,rand+dis_category*dis,1,1)\n",
    "        noisev = Variable(noise_resize)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD_D(netD(fake)).mean()\n",
    "        errG.backward(mone)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        \n",
    "        \n",
    "        # update Q\n",
    "        \n",
    "        for p in netD.parameters(): \n",
    "            p.requires_grad = True \n",
    "        for p in netD_D.parameters(): \n",
    "            p.requires_grad = True \n",
    "\n",
    "        zero_grad()\n",
    "        noisev = Variable(noise_resize)\n",
    "        G_sample = netG(noisev)\n",
    "        Q_c_given_x = netD_Q(netD(G_sample)).view(batchsize, dis_category)\n",
    "        crossent_loss = criterion_logli(Q_c_given_x ,Variable(label_c.cuda()))\n",
    "            \n",
    "        mi_loss = 1*crossent_loss\n",
    "        mi_loss.backward()\n",
    "        optimizerQ.step()\n",
    "\n",
    "        \n",
    "       # print (Q_c_given_x)\n",
    "\n",
    "        # ent_loss = torch.mean(-torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "       # ent_loss_2 = torch.mean(-torch.sum(c2 * torch.log(c2 + 1e-8), dim=1))\n",
    "       # ent_loss_3 = torch.mean(-torch.sum(c3 * torch.log(c3 + 1e-8), dim=1))\n",
    "        \n",
    "        if gen_iterations % 20 == 0 :\n",
    "            #errD = D_real - D_fake\n",
    "            with open(\"output_cell.txt\",\"w\") as f:\n",
    "                f.write('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]) + '\\n')\n",
    "            print ('{0} {1} {2} {3}'.format(epoch, gen_iterations , -D_cost.data[0] , mi_loss.data[0]))\n",
    "            G_sample = netG(Variable(fixed_noise))\n",
    "            vutils.save_image(G_sample.data, 'fake_cell_seg_2.png',nrow=dis_category,normalize=True)\n",
    "            vutils.save_image(image_, 'real_samples_seg_2.png',normalize=True)\n",
    "            \n",
    "            #vutils.save_image(G_sample.data, '{0}fake_samples_{1}.png'.format(-errD.data[0], gen_iterations))\n",
    "            #vutils.save_image(G_sample.data, 'fake_samples.png',normalize = True)\n",
    "            \n",
    "            #for t in range(0,5):\n",
    "                #fixed_noise = generate_fix_noise(dis, cont, rand)[t].reshape(100,rand+dis*10+cont,1,1)\n",
    "                #G_sample = netG(Variable(torch.FloatTensor(fixed_noise).cuda()))\n",
    "                #vutils.save_image(G_sample.data, 'map_%d_cell.png'% (t),nrow=10,normalize=True)\n",
    "\n",
    "            #torch.save(netG.state_dict(), './params/0517/tumor_netG_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD.state_dict(), './params/0517/tumor_netD_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_D.state_dict(), './params/0517/tumor_netD_D_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q.state_dict(), './params/0517/tumor_netD_Q_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_2.state_dict(), './params/0517/tumor_netD_Q_2_epoch_%d.pth' % (epoch))\n",
    "            #torch.save(netD_Q_3.state_dict(), './params/0517/tumor_netD_Q_3_epoch_%d.pth' % (epoch))\n",
    "        \n",
    "        #storage = np.zeros((100,3,64,64),dtype=np.float32)\n",
    "        #z_fix = Variable(torch.randn(1,128,1,1).cuda().normal_(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
